{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric_temporal.nn.recurrent import AGCRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try batch size=4/7 for 25 years of historical data; for a seq length of 5(5/3 batches per epoch)\n",
    "#input features will be input sectors (8) + 3 additional features for now\n",
    "#T will be every 5 years (tbd)\n",
    "#assume data will be read from csv \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class ModelDataset(Dataset):\n",
    "    def __init__(self, csv_file, T=5):\n",
    "        \"\"\"\n",
    "        X shape: [num_samples, T, num_nodes, in_channels]\n",
    "        Y shape: [num_samples, num_nodes, num_sectors]\n",
    "        \"\"\"\n",
    "        df=pd.read_csv(csv_file)\n",
    "        data = df.values \n",
    "        self.T = T\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        \n",
    "        # Build sequences of length 'T'\n",
    "        # For each index i, we take rows[i : i+T] as inputs \n",
    "        # and row[i+T] (or some slice) as the target.\n",
    "        for i in range(len(data) - T):\n",
    "            # For example, let's say columns [0:13] are features, column 13 is target\n",
    "            x_seq = data[i : i + T, :13] \n",
    "            y_val = data[i + T, 13]\n",
    "\n",
    "            self.X.append(x_seq)\n",
    "            self.y.append(y_val)\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Dictionary\n",
    "\n",
    "0. Singapore\n",
    "1. China\n",
    "2. Malaysia\n",
    "3. United States\n",
    "4. Hong Kong, China\n",
    "5. Indonesia\n",
    "6. Korea, Rep.\n",
    "7. Japan\n",
    "8. Thailand\n",
    "9. Australia\n",
    "10. Vietnam\n",
    "11. India\n",
    "12. United Arab Emirates\n",
    "13. Philippines\n",
    "14. Germany\n",
    "15. France\n",
    "16. Switzerland\n",
    "17. Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector Dictionary\n",
    "0. Category 1 (Agri)\n",
    "1. Category 2 (Mining)\n",
    "2. Category 3 (Construction)\n",
    "3. Category 4 (Textile)\n",
    "4. Category 5 (Transport Svcs)\n",
    "5. Category 6 (ICT)\n",
    "6. Category 7 (Health, pharm, sports etc)\n",
    "7. Category 8 (Govt, Millitary, Misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Model structure\n",
    "        self.num_nodes = 18  # Example: 100 country-pairs\n",
    "        self.input_dim = 11    # e.g. sectorial export volume + sentiment score + 2 indexes\n",
    "        self.rnn_units = 32\n",
    "        self.output_dim = 8   # e.g., predict only the sectorial export volume\n",
    "        self.horizon = 2      # forecast 2 steps ahead\n",
    "        self.num_layers = 2\n",
    "        self.cheb_order = 2\n",
    "        self.embed_dim = 10\n",
    "        self.default_graph = True  \n",
    "        self.log_dir = './logs/'\n",
    "        self.debug = False\n",
    "        self.model='AGCRN'\n",
    "        self.normaliser = 'std'\n",
    "        self.device='cpu'\n",
    "        \n",
    "        # Training\n",
    "        self.seed=10\n",
    "        self.loss_func= 'mse'\n",
    "        self.epochs = 10\n",
    "        self.lr_init = 0.003\n",
    "        self.lr_decay = False\n",
    "        self.lr_decay_steps = 5,20,40,70\n",
    "        self.lr_decay_rate = 0.3\n",
    "        self.early_stop = True\n",
    "        self.early_stop_patience = 15\n",
    "        self.teacher_forcing = False\n",
    "        self.tf_decay_steps = 2000\n",
    "        self.real_value = True\n",
    "        self.grad_norm = True\n",
    "        self.max_grad_norm = 5\n",
    "\n",
    "        # Testing\n",
    "        self.mae_thresh=None\n",
    "        self.mape_thresh=0.\n",
    "\n",
    "        #Logging\n",
    "        self.log_step = 20\n",
    "        self.plot=False\n",
    "\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the dataset and data loader\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mModelDataset\u001b[49m(csv_path, T\u001b[38;5;241m=\u001b[39mT)\n\u001b[0;32m      3\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ModelDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the dataset and data loader\n",
    "dataset = ModelDataset(csv_path, T=T)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from model.AGCRN.model.BasicTrainer import Trainer\n",
    "from model.AGCRN.model.Run import masked_mae_loss\n",
    "from model.model import AGCRNFinal\n",
    "import os\n",
    "\n",
    "model=AGCRNFinal(args)\n",
    "model=model.to(args.device)\n",
    "for p in model.parameters():\n",
    "    nn.init.xavier_uniform_(p)\n",
    "\n",
    "#load dataset here\n",
    "\n",
    "#init loss function, optimizer\n",
    "loss=torch.nn.MSELoss().to(args.device)\n",
    "optimizer=optim.Adam(model.parameters(),lr=args.lr_init,eps=1.0e-8,weight_decay=0.0,amsgrad=False)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler=None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "\n",
    "#config log path\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "log_dir = os.path.join(current_dir,'logs')\n",
    "args.log_dir = log_dir\n",
    "\n",
    "#start training\n",
    "trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "                  args, lr_scheduler=lr_scheduler)\n",
    "if args.mode == 'train':\n",
    "    trainer.train()\n",
    "# elif args.mode == 'test':\n",
    "#     model.load_state_dict(torch.load('./pre-trained/{}.pth'.format(args.dataset)))\n",
    "#     print(\"Load saved model\")\n",
    "#     trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "# else:\n",
    "#     raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_country_pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# MODEL DEFINITION\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AGCRN(\n\u001b[1;32m----> 3\u001b[0m     number_of_nodes\u001b[38;5;241m=\u001b[39m\u001b[43mnum_country_pair\u001b[49m,\n\u001b[0;32m      4\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39min_channels,\n\u001b[0;32m      5\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39mout_channels,\n\u001b[0;32m      6\u001b[0m     K\u001b[38;5;241m=\u001b[39mK,\n\u001b[0;32m      7\u001b[0m     embedding_dimensions\u001b[38;5;241m=\u001b[39membedding_dims\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 3) Create the node embedding E separately (following your interface).\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#    We'll just do a random init. This is learnable, so we wrap it in nn.Parameter.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m E \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(num_country_pair, embedding_dims), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_country_pair' is not defined"
     ]
    }
   ],
   "source": [
    "# MODEL DEFINITION\n",
    "model = AGCRN(\n",
    "    number_of_nodes=num_country_pair,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    K=K,\n",
    "    embedding_dimensions=embedding_dims\n",
    ")\n",
    "\n",
    "# 3) Create the node embedding E separately (following your interface).\n",
    "#    We'll just do a random init. This is learnable, so we wrap it in nn.Parameter.\n",
    "E = nn.Parameter(torch.zeros(num_country_pair, embedding_dims), requires_grad=True)\n",
    "\n",
    "# 4) \"prediction head\" to map from [out_channels] -> [num_sectors]\n",
    "prediction_head = nn.Linear(out_channels, num_sectors)\n",
    "\n",
    "# 5) Combine everything in a single optimizer. We must include the node embedding (E) as well.\n",
    "optimizer = optim.Adam(\n",
    "    list(model.parameters()) + list(prediction_head.parameters()) + [E],\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 6) Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X_batch, Y_batch in dataloader:\n",
    "        # X_batch: [batch_size, num_nodes, in_channels]\n",
    "        # Y_batch: [batch_size, num_nodes, num_sectors]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        H = None\n",
    "        # Unroll over T time steps\n",
    "        for t in range(T):\n",
    "            X_t = X_batch[:, t, :, :]  # [batch_size, num_nodes, in_channels]\n",
    "            H = model(X_t, E, H)  # H is the hidden state, E is the node embedding\n",
    "            \n",
    "\n",
    "        # Now map from [out_channels] -> 1 dimension\n",
    "        # We'll do this for each node:\n",
    "        Y_pred = prediction_head(H)\n",
    "        print('Y_pred.shape', Y_pred.shape)\n",
    "        # Compute MSE loss with target\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "\n",
    "        # Backprop & update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
