{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric_temporal.nn.recurrent import AGCRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try batch size=4/7 for 25 years of historical data; for a seq length of 5(5/3 batches per epoch)\n",
    "#input features will be input sectors (8) + 3 additional features for now\n",
    "#T will be every 5 years (tbd)\n",
    "#assume data will be read from csv \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class ModelDataset(Dataset):\n",
    "    def __init__(self, csv_file, T=5):\n",
    "        \"\"\"\n",
    "        X shape: [num_samples, T, num_nodes, in_channels]\n",
    "        Y shape: [num_samples, num_nodes, num_sectors]\n",
    "        \"\"\"\n",
    "        df=pd.read_csv(csv_file)\n",
    "        data = df.values \n",
    "        self.T = T\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        \n",
    "        # Build sequences of length 'T'\n",
    "        # For each index i, we take rows[i : i+T] as inputs \n",
    "        # and row[i+T] (or some slice) as the target.\n",
    "        for i in range(len(data) - T):\n",
    "            # For example, let's say columns [0:13] are features, column 13 is target\n",
    "            x_seq = data[i : i + T, :13] \n",
    "            y_val = data[i + T, 13]\n",
    "\n",
    "            self.X.append(x_seq)\n",
    "            self.y.append(y_val)\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Dictionary\n",
    "\n",
    "0. Singapore\n",
    "1. China\n",
    "2. Malaysia\n",
    "3. United States\n",
    "4. Hong Kong, China\n",
    "5. Indonesia\n",
    "6. Korea, Rep.\n",
    "7. Japan\n",
    "8. Thailand\n",
    "9. Australia\n",
    "10. Vietnam\n",
    "11. India\n",
    "12. United Arab Emirates\n",
    "13. Philippines\n",
    "14. Germany\n",
    "15. France\n",
    "16. Switzerland\n",
    "17. Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector Dictionary\n",
    "0. Category 1 (Agri)\n",
    "1. Category 2 (Mining)\n",
    "2. Category 3 (Construction)\n",
    "3. Category 4 (Textile)\n",
    "4. Category 5 (Transport Svcs)\n",
    "5. Category 6 (ICT)\n",
    "6. Category 7 (Health, pharm, sports etc)\n",
    "7. Category 8 (Govt, Millitary, Misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_countries=18\n",
    "num_country_pairs=18*(17-1) \n",
    "num_sectors=8 # 8 sectors\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Model structure\n",
    "        self.num_nodes = num_country_pairs  \n",
    "        self.input_dim = num_sectors+3    # e.g. sectorial export volume + sentiment score + 2 indexes\n",
    "        self.rnn_units = 32\n",
    "        self.output_dim = num_sectors   # e.g., predict only the sectorial export volume\n",
    "        self.horizon = 2      # forecast 2 steps ahead\n",
    "        self.num_layers = 2\n",
    "        self.cheb_k = 2\n",
    "        self.embed_dim = 10\n",
    "        self.default_graph = True  \n",
    "        self.log_dir = './logs/'\n",
    "        self.debug = False\n",
    "        self.model='AGCRN'\n",
    "        self.normaliser = 'std'\n",
    "        self.device='cpu'\n",
    "        self.batch_size=4 # 4/7 depending on results\n",
    "        \n",
    "        # Training\n",
    "        self.seed=10\n",
    "        self.loss_func= 'mse'\n",
    "        self.epochs = 10\n",
    "        self.lr_init = 0.003\n",
    "        self.lr_decay = False\n",
    "        self.lr_decay_steps = 5,20,40,70\n",
    "        self.lr_decay_rate = 0.3\n",
    "        self.early_stop = True\n",
    "        self.early_stop_patience = 15\n",
    "        self.teacher_forcing = False\n",
    "        self.tf_decay_steps = 2000\n",
    "        self.real_value = True\n",
    "        self.grad_norm = True\n",
    "        self.max_grad_norm = 5\n",
    "\n",
    "        # Testing\n",
    "        self.mae_thresh=None\n",
    "        self.mape_thresh=0.\n",
    "\n",
    "        #Logging\n",
    "        self.log_step = 20\n",
    "        self.plot=False\n",
    "\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the dataset and data loader\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m val_dataset\u001b[38;5;241m=\u001b[39m \u001b[43mModelDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_dataset\u001b[38;5;241m=\u001b[39m ModelDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, T\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mhorizon)\n\u001b[0;32m      4\u001b[0m test_dataset\u001b[38;5;241m=\u001b[39m ModelDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, T\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mhorizon)\n",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m, in \u001b[0;36mModelDataset.__init__\u001b[1;34m(self, csv_file, T)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    X shape: [num_samples, T, num_nodes, in_channels]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    Y shape: [num_samples, num_nodes, num_sectors]\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mvalues \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m=\u001b[39m T\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "# Create the dataset and data loader\n",
    "train_dataset= ModelDataset('data.csv', T=args.horizon)\n",
    "val_dataset= ModelDataset('data.csv', T=args.horizon)\n",
    "test_dataset= ModelDataset('data.csv', T=args.horizon)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not module",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m current_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\n\u001b[1;32m---> 28\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m args\u001b[38;5;241m.\u001b[39mlog_dir \u001b[38;5;241m=\u001b[39m log_dir\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#start training\u001b[39;00m\n",
      "File \u001b[1;32m<frozen ntpath>:108\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not module"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from AGCRN.model.BasicTrainer import Trainer\n",
    "from agcrn_model import AGCRNFinal\n",
    "\n",
    "model=AGCRNFinal(args)\n",
    "model=model.to(args.device)\n",
    "# for p in model.parameters():\n",
    "#     nn.init.xavier_uniform_(p)\n",
    "\n",
    "#load dataset here\n",
    "\n",
    "#init loss function, optimizer\n",
    "loss=torch.nn.MSELoss().to(args.device)\n",
    "optimizer=optim.Adam(model.parameters(),lr=args.lr_init,eps=1.0e-8,weight_decay=0.0,amsgrad=False)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler=None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "\n",
    "#config log path\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "current_dir = os.getcwd()\n",
    "log_dir = os.path.join(current_dir,'logs')\n",
    "args.log_dir = log_dir\n",
    "\n",
    "#start training\n",
    "trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler, #need to get these \n",
    "                  args, lr_scheduler=lr_scheduler)\n",
    "if args.mode == 'train':\n",
    "    trainer.train()\n",
    "# elif args.mode == 'test':\n",
    "#     model.load_state_dict(torch.load('./pre-trained/{}.pth'.format(args.dataset)))\n",
    "#     print(\"Load saved model\")\n",
    "#     trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "# else:\n",
    "#     raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_country_pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# MODEL DEFINITION\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AGCRN(\n\u001b[1;32m----> 3\u001b[0m     number_of_nodes\u001b[38;5;241m=\u001b[39m\u001b[43mnum_country_pair\u001b[49m,\n\u001b[0;32m      4\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39min_channels,\n\u001b[0;32m      5\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39mout_channels,\n\u001b[0;32m      6\u001b[0m     K\u001b[38;5;241m=\u001b[39mK,\n\u001b[0;32m      7\u001b[0m     embedding_dimensions\u001b[38;5;241m=\u001b[39membedding_dims\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 3) Create the node embedding E separately (following your interface).\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#    We'll just do a random init. This is learnable, so we wrap it in nn.Parameter.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m E \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(num_country_pair, embedding_dims), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_country_pair' is not defined"
     ]
    }
   ],
   "source": [
    "# # MODEL DEFINITION\n",
    "# model = AGCRN(\n",
    "#     number_of_nodes=num_country_pairs,\n",
    "#     in_channels=in_channels,\n",
    "#     out_channels=out_channels,\n",
    "#     K=K,\n",
    "#     embedding_dimensions=embedding_dims\n",
    "# )\n",
    "\n",
    "# # 3) Create the node embedding E separately (following your interface).\n",
    "# #    We'll just do a random init. This is learnable, so we wrap it in nn.Parameter.\n",
    "# E = nn.Parameter(torch.zeros(num_country_pairs, embedding_dims), requires_grad=True)\n",
    "\n",
    "# # 4) \"prediction head\" to map from [out_channels] -> [num_sectors]\n",
    "# prediction_head = nn.Linear(out_channels, num_sectors)\n",
    "\n",
    "# # 5) Combine everything in a single optimizer. We must include the node embedding (E) as well.\n",
    "# optimizer = optim.Adam(\n",
    "#     list(model.parameters()) + list(prediction_head.parameters()) + [E],\n",
    "#     lr=lr\n",
    "# )\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # 6) Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for X_batch, Y_batch in dataloader:\n",
    "#         # X_batch: [batch_size, num_nodes, in_channels]\n",
    "#         # Y_batch: [batch_size, num_nodes, num_sectors]\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         H = None\n",
    "#         # Unroll over T time steps\n",
    "#         for t in range(T):\n",
    "#             X_t = X_batch[:, t, :, :]  # [batch_size, num_nodes, in_channels]\n",
    "#             H = model(X_t, E, H)  # H is the hidden state, E is the node embedding\n",
    "            \n",
    "\n",
    "#         # Now map from [out_channels] -> 1 dimension\n",
    "#         # We'll do this for each node:\n",
    "#         Y_pred = prediction_head(H)\n",
    "#         print('Y_pred.shape', Y_pred.shape)\n",
    "#         # Compute MSE loss with target\n",
    "#         loss = criterion(Y_pred, Y_batch)\n",
    "\n",
    "#         # Backprop & update\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "#     avg_loss = total_loss / len(dataset)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
