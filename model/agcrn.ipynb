{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector Dictionary\n",
    "0. Category 1 (Agri)\n",
    "1. Category 2 (Mining)\n",
    "2. Category 3 (Construction)\n",
    "3. Category 4 (Textile)\n",
    "4. Category 5 (Transport Svcs)\n",
    "5. Category 6 (ICT)\n",
    "6. Category 7 (Health, pharm, sports etc)\n",
    "7. Category 8 (Govt, Millitary, Misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country_a</th>\n",
       "      <th>country_b</th>\n",
       "      <th>bec_1</th>\n",
       "      <th>bec_2</th>\n",
       "      <th>bec_3</th>\n",
       "      <th>bec_4</th>\n",
       "      <th>bec_5</th>\n",
       "      <th>bec_6</th>\n",
       "      <th>bec_7</th>\n",
       "      <th>bec_8</th>\n",
       "      <th>D</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ARE</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.060739e+09</td>\n",
       "      <td>0.620151</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ARE</td>\n",
       "      <td>CHE</td>\n",
       "      <td>3.797882e+06</td>\n",
       "      <td>1.355991e+06</td>\n",
       "      <td>6.939408e+06</td>\n",
       "      <td>3.759398e+08</td>\n",
       "      <td>2.690339e+06</td>\n",
       "      <td>9.364289e+05</td>\n",
       "      <td>4.422947e+07</td>\n",
       "      <td>9.824469e+07</td>\n",
       "      <td>0.586050</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ARE</td>\n",
       "      <td>CHN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.953953e+09</td>\n",
       "      <td>0.635445</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ARE</td>\n",
       "      <td>DEU</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.159614e+08</td>\n",
       "      <td>0.566027</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ARE</td>\n",
       "      <td>FRA</td>\n",
       "      <td>4.972880e+07</td>\n",
       "      <td>4.680296e+07</td>\n",
       "      <td>9.306163e+07</td>\n",
       "      <td>7.307789e+07</td>\n",
       "      <td>9.781168e+07</td>\n",
       "      <td>1.751175e+07</td>\n",
       "      <td>1.162716e+08</td>\n",
       "      <td>2.462986e+08</td>\n",
       "      <td>0.620551</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 country_a country_b         bec_1         bec_2         bec_3  \\\n",
       "0           0       ARE       AUS  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "1           1       ARE       CHE  3.797882e+06  1.355991e+06  6.939408e+06   \n",
       "2           2       ARE       CHN  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "3           3       ARE       DEU  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "4           4       ARE       FRA  4.972880e+07  4.680296e+07  9.306163e+07   \n",
       "\n",
       "          bec_4         bec_5         bec_6         bec_7         bec_8  \\\n",
       "0  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.060739e+09   \n",
       "1  3.759398e+08  2.690339e+06  9.364289e+05  4.422947e+07  9.824469e+07   \n",
       "2  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  2.953953e+09   \n",
       "3  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  7.159614e+08   \n",
       "4  7.307789e+07  9.781168e+07  1.751175e+07  1.162716e+08  2.462986e+08   \n",
       "\n",
       "          D  year  \n",
       "0  0.620151  2006  \n",
       "1  0.586050  2006  \n",
       "2  0.635445  2006  \n",
       "3  0.566027  2006  \n",
       "4  0.620551  2006  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp=pd.read_csv('../data/final/final_training_model_data.csv',header=0)\n",
    "# temp.head()\n",
    "# temp.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# temp=temp[(temp['country_a']!='ARE')& (temp['country_b']!='ARE')]\n",
    "# temp.to_csv('../data/final/without_ARE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_countries=17\n",
    "num_country_pairs=num_countries*(num_countries-1) \n",
    "num_sectors=8 # 8 sectors\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Model structure\n",
    "        self.num_nodes = num_country_pairs  \n",
    "        self.input_dim = num_sectors+1    # e.g. sectorial export volume + composite\n",
    "        self.rnn_units = 64\n",
    "        self.output_dim = num_sectors   # e.g., predict only the sectorial export volume\n",
    "        self.horizon = 3      # forecast 3 steps ahead\n",
    "        self.num_layers = 2\n",
    "        self.cheb_k = 2\n",
    "        self.embed_dim = 10\n",
    "        self.default_graph = True  \n",
    "        self.log_dir = './logs/'\n",
    "        self.debug = False\n",
    "        self.model='AGCRN'\n",
    "        self.normaliser = 'max11'\n",
    "        self.device='cpu'\n",
    "        self.batch_size=4 # 4/7 depending on results\n",
    "        self.mode='train'\n",
    "        # Training\n",
    "        self.seed=10\n",
    "        self.loss_func= 'mse'\n",
    "        self.epochs = 50\n",
    "        self.lr_init = 0.008\n",
    "        self.lr_decay = True\n",
    "        self.lr_decay_step = '5,20,40,70'\n",
    "        self.lr_decay_rate = 0.3\n",
    "        self.early_stop = True\n",
    "        self.early_stop_patience = 5\n",
    "        self.teacher_forcing = True\n",
    "        self.tf_decay_steps = 20\n",
    "        self.real_value = False\n",
    "        self.grad_norm = True\n",
    "        self.max_grad_norm = 5\n",
    "\n",
    "        # Testing\n",
    "        self.mae_thresh=None\n",
    "        self.mape_thresh=0.\n",
    "\n",
    "        #Logging\n",
    "        self.log_step = 20\n",
    "        self.plot=True\n",
    "\n",
    "    def set_args(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Update attributes of Args dynamically based on the kwargs.\n",
    "        If a key doesn't match an existing attribute, a warning is printed.\n",
    "        \"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                print(f\"Warning: '{key}' is not a recognized attribute of the Args class.\")\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=pd.read_csv('../data/final/without_ARE.csv',header=0) #i will only have 2007 to 2023 now\n",
    "# temp_only_d=temp[['D','country_a','country_b','year']]\n",
    "# temp.drop(columns=['Unnamed: 0','D'],inplace=True)\n",
    "# temp.sort_values(by=['country_a','country_b','year'],inplace=True)\n",
    "# bec_columns = [f'bec_{i}' for i in range(1, 9)]\n",
    "# for col in bec_columns:\n",
    "#     # Create a new column to store the percentage change.\n",
    "#     temp[f'pct_{col}'] = temp.groupby(['country_a', 'country_b'])[col].pct_change() * 100\n",
    "# temp.dropna(inplace=True)\n",
    "# temp.reset_index(drop=True,inplace=True)\n",
    "# temp=temp.merge(temp_only_d, on=['country_a','country_b','year'], how='left')\n",
    "# temp.drop(columns=bec_columns,inplace=True)\n",
    "# temp.rename(columns={'pct_bec_1':'bec_1','pct_bec_2':'bec_2','pct_bec_3':'bec_3','pct_bec_4':'bec_4','pct_bec_5':'bec_5','pct_bec_6':'bec_6','pct_bec_7':'bec_7','pct_bec_8':'bec_8'},inplace=True)\n",
    "# temp.to_csv('../data/final/without_ARE_pct.csv',index=False)\n",
    "# fbic_data=fbic_data.rename(columns={'iso3a':'country_a','iso3b':'country_b'})\n",
    "# training_data=pd.read_csv('../data/final/training_model_data.csv',header=0)\n",
    "# training_data=training_data[['country_a','country_b','bec_1','bec_2','bec_3','bec_4','bec_5','bec_6','bec_7','bec_8','sentiment_index','tradeagreementindex','year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=pd.read_csv('../data/final/compiled_model_data.csv',header=0)\n",
    "# temp=temp.reset_index(drop=True)\n",
    "# temp=temp[['country_a','country_b','bec_1','bec_2','bec_3','bec_4','bec_5','bec_6','bec_7','bec_8','D','year']]\n",
    "# temp=temp[(temp['year']>=2006) & (temp['year']<=2020)]\n",
    "# temp.to_csv('../data/final/final_training_model_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation to pipeline data into model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_tensor(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with columns:\n",
    "      country1, country2, sector1, sector2, ..., sector8, sentiment, year\n",
    "    and returns a tensor of shape (T, N, D), where:\n",
    "      T = number of years,\n",
    "      N = number of unique country pairs,\n",
    "      D = num of sectors + features.\n",
    "    Also returns the sorted list of years and country pair nodes.\n",
    "    \"\"\"\n",
    "    # Read the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Ensure the 'year' column is integer (if needed)\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    \n",
    "    # Get a sorted list of unique years\n",
    "    years = sorted(df['year'].unique())\n",
    "    T = len(years)\n",
    "    \n",
    "    # Get all unique country pairs\n",
    "    pairs_df = df[['country_a', 'country_b']].drop_duplicates()\n",
    "    # Create a sorted list of tuples (country1, country2) for consistent node ordering\n",
    "    country_pairs = sorted([tuple(x) for x in pairs_df.values])\n",
    "    N = len(country_pairs)\n",
    "    \n",
    "    # Number of features (8 sectors + 1 sentiment)\n",
    "    D = 9\n",
    "\n",
    "    # Initialize an empty numpy array for the tensor data\n",
    "    tensor_data = np.empty((T, N, D), dtype=float)\n",
    "    \n",
    "    # Loop over each year and each country pair to fill in the tensor\n",
    "    for t, year in enumerate(years):\n",
    "        # Get data for the current year\n",
    "        df_year = df[df['year'] == year]\n",
    "        for n, (c1, c2) in enumerate(country_pairs):\n",
    "            # Filter rows for the current country pair\n",
    "            row = df_year[(df_year['country_a'] == c1) & (df_year['country_b'] == c2)]\n",
    "            if not row.empty:\n",
    "                # Extract the 8 sector columns and the sentiment column.\n",
    "                # Assumes these columns are named exactly as shown.\n",
    "                features = row.iloc[0][['bec_1', 'bec_2', 'bec_3', 'bec_4', \n",
    "                                         'bec_5', 'bec_6', 'bec_7', 'bec_8', 'D']].values\n",
    "                tensor_data[t, n, :] = features.astype(float)\n",
    "            else:\n",
    "                # If a record is missing for a given year/country pair, fill with zeros (or choose another strategy)\n",
    "                tensor_data[t, n, :] = np.zeros(D)\n",
    "                \n",
    "    return tensor_data, years, country_pairs\n",
    "\n",
    "def group_into_windows(tensor_data, window_size):\n",
    "    \"\"\"\n",
    "    Given a tensor of shape (T, N, D), group the data into overlapping windows.\n",
    "    Each window is of length window_size\n",
    "    Returns a numpy array of shape (num_samples, window_size, N, D).\n",
    "    \"\"\"\n",
    "    T, N, D = tensor_data.shape\n",
    "    num_samples = T - window_size + 1  # sliding window with stride 1\n",
    "    windows = []\n",
    "    for i in range(num_samples):\n",
    "        window = tensor_data[i: i + window_size]  # shape: (window_size, N, D)\n",
    "        windows.append(window)\n",
    "    windows = np.stack(windows)  # shape: (num_samples, window_size, N, D)\n",
    "    return windows\n",
    "\n",
    "def split_input_target_direct(windows, input_len, horizon=3):\n",
    "    \"\"\"\n",
    "    Splits each window into input and a single target that is horizon steps forward.\n",
    "    \n",
    "    windows: numpy array of shape (num_samples, window_size, N, D)\n",
    "              where window_size = input_len + horizon.\n",
    "    input_len: number of time steps used as input.\n",
    "    horizon: steps forward to pick the target (here, horizon=3).\n",
    "    \n",
    "    Returns:\n",
    "      x: inputs of shape (num_samples, input_len, N, D)\n",
    "      y: targets of shape (num_samples, N, 8), which are the first 8 features of the target time step.\n",
    "    \"\"\"\n",
    "    # x: first input_len time steps (e.g., years 2006-2009 if input_len=4)\n",
    "    x = windows[:, :input_len]  \n",
    "    # y_full: the time step exactly horizon steps forward (i.e., index input_len + horizon - 1)\n",
    "    # y_full = windows[:, input_len + horizon-1]  \n",
    "    y_full = windows[:, input_len:input_len + horizon]\n",
    "    # y: only the first 8 features from the predicted time step (ignoring sentiment_index and tradeagreementindex)\n",
    "    y = y_full[..., :8]\n",
    "    return x, y\n",
    "\n",
    "def train_val_split(x, y, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data into train and validation sets by ratio.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    split_index = int(num_samples * (1 - val_ratio))\n",
    "    x_train, y_train = x[:split_index], y[:split_index]\n",
    "    x_val, y_val = x[split_index:], y[split_index:]\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AGCRN.lib.dataloader import normalize_dataset\n",
    "\n",
    "#convert csv to tensor\n",
    "training_data_tensor, years, country_pairs = csv_to_tensor('../data/final/without_ARE_pct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for year 2007:\n",
      "[[288.52271933 279.03809661 204.2020891  ... 174.11212633 196.4292726\n",
      "    0.55853787]\n",
      " [ 38.17646947  31.0278394   36.47753454 ...  63.61518823 -17.67302399\n",
      "    0.56009085]\n",
      " [ -0.29719578   3.56841464  14.62732324 ...  20.83388439 -32.08029114\n",
      "    0.59298115]\n",
      " ...\n",
      " [ -0.33857947  18.18635501  26.43173842 ...  16.49221368  18.26733175\n",
      "    0.3671412 ]\n",
      " [ -1.33240313  -4.83563772  47.63716963 ... -16.34540145  10.97735679\n",
      "    0.46350549]\n",
      " [ 27.02208384 -18.73648827  26.17908911 ...  44.25435862  25.63200196\n",
      "    0.57982181]]\n",
      "Features for ('AUS', 'CHE') in 2007:\n",
      "[2.88522719e+02 2.79038097e+02 2.04202089e+02 1.18079757e+03\n",
      " 1.39229930e+02 6.28642389e+02 1.74112126e+02 1.96429273e+02\n",
      " 5.58537867e-01]\n"
     ]
    }
   ],
   "source": [
    "# Inspect data to check\n",
    "print(\"Data for year {}:\".format(years[0]))\n",
    "print(training_data_tensor[0])  # prints the data for all nodes/features for the first year\n",
    "\n",
    "print(\"Features for {} in {}:\".format(country_pairs[0], years[0]))\n",
    "print(training_data_tensor[0, 0, :]) # prints the features for the first country pair in the first year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Dictionary (ordered by alphabetical order)\n",
    "\n",
    "0. Australia (AUS)\n",
    "1. Switzerland (CHE)\n",
    "2. China (CHN)\n",
    "3. Germany (DEU)\n",
    "4. France (FRA)\n",
    "5. Hong Kong, China (HKG)\n",
    "6. Indonesia (IDN)\n",
    "7. India (IND)\n",
    "8. Japan (JPN)\n",
    "9. Korea, Rep. (KOR)\n",
    "10. Malaysia (MYS)\n",
    "11. Netherlands (NLD)\n",
    "12. Philippines (PHL)\n",
    "13. Singapore (SGP)\n",
    "14. Thailand (THA)\n",
    "15. United States (USA)\n",
    "16. Vietnam (VNM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of country pairs for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AUS', 'CHE'),\n",
       " ('AUS', 'CHN'),\n",
       " ('AUS', 'DEU'),\n",
       " ('AUS', 'FRA'),\n",
       " ('AUS', 'HKG'),\n",
       " ('AUS', 'IDN'),\n",
       " ('AUS', 'IND'),\n",
       " ('AUS', 'JPN'),\n",
       " ('AUS', 'KOR'),\n",
       " ('AUS', 'MYS'),\n",
       " ('AUS', 'NLD'),\n",
       " ('AUS', 'PHL'),\n",
       " ('AUS', 'SGP'),\n",
       " ('AUS', 'THA'),\n",
       " ('AUS', 'USA'),\n",
       " ('AUS', 'VNM'),\n",
       " ('CHE', 'AUS'),\n",
       " ('CHE', 'CHN'),\n",
       " ('CHE', 'DEU'),\n",
       " ('CHE', 'FRA'),\n",
       " ('CHE', 'HKG'),\n",
       " ('CHE', 'IDN'),\n",
       " ('CHE', 'IND'),\n",
       " ('CHE', 'JPN'),\n",
       " ('CHE', 'KOR'),\n",
       " ('CHE', 'MYS'),\n",
       " ('CHE', 'NLD'),\n",
       " ('CHE', 'PHL'),\n",
       " ('CHE', 'SGP'),\n",
       " ('CHE', 'THA'),\n",
       " ('CHE', 'USA'),\n",
       " ('CHE', 'VNM'),\n",
       " ('CHN', 'AUS'),\n",
       " ('CHN', 'CHE'),\n",
       " ('CHN', 'DEU'),\n",
       " ('CHN', 'FRA'),\n",
       " ('CHN', 'HKG'),\n",
       " ('CHN', 'IDN'),\n",
       " ('CHN', 'IND'),\n",
       " ('CHN', 'JPN'),\n",
       " ('CHN', 'KOR'),\n",
       " ('CHN', 'MYS'),\n",
       " ('CHN', 'NLD'),\n",
       " ('CHN', 'PHL'),\n",
       " ('CHN', 'SGP'),\n",
       " ('CHN', 'THA'),\n",
       " ('CHN', 'USA'),\n",
       " ('CHN', 'VNM'),\n",
       " ('DEU', 'AUS'),\n",
       " ('DEU', 'CHE'),\n",
       " ('DEU', 'CHN'),\n",
       " ('DEU', 'FRA'),\n",
       " ('DEU', 'HKG'),\n",
       " ('DEU', 'IDN'),\n",
       " ('DEU', 'IND'),\n",
       " ('DEU', 'JPN'),\n",
       " ('DEU', 'KOR'),\n",
       " ('DEU', 'MYS'),\n",
       " ('DEU', 'NLD'),\n",
       " ('DEU', 'PHL'),\n",
       " ('DEU', 'SGP'),\n",
       " ('DEU', 'THA'),\n",
       " ('DEU', 'USA'),\n",
       " ('DEU', 'VNM'),\n",
       " ('FRA', 'AUS'),\n",
       " ('FRA', 'CHE'),\n",
       " ('FRA', 'CHN'),\n",
       " ('FRA', 'DEU'),\n",
       " ('FRA', 'HKG'),\n",
       " ('FRA', 'IDN'),\n",
       " ('FRA', 'IND'),\n",
       " ('FRA', 'JPN'),\n",
       " ('FRA', 'KOR'),\n",
       " ('FRA', 'MYS'),\n",
       " ('FRA', 'NLD'),\n",
       " ('FRA', 'PHL'),\n",
       " ('FRA', 'SGP'),\n",
       " ('FRA', 'THA'),\n",
       " ('FRA', 'USA'),\n",
       " ('FRA', 'VNM'),\n",
       " ('HKG', 'AUS'),\n",
       " ('HKG', 'CHE'),\n",
       " ('HKG', 'CHN'),\n",
       " ('HKG', 'DEU'),\n",
       " ('HKG', 'FRA'),\n",
       " ('HKG', 'IDN'),\n",
       " ('HKG', 'IND'),\n",
       " ('HKG', 'JPN'),\n",
       " ('HKG', 'KOR'),\n",
       " ('HKG', 'MYS'),\n",
       " ('HKG', 'NLD'),\n",
       " ('HKG', 'PHL'),\n",
       " ('HKG', 'SGP'),\n",
       " ('HKG', 'THA'),\n",
       " ('HKG', 'USA'),\n",
       " ('HKG', 'VNM'),\n",
       " ('IDN', 'AUS'),\n",
       " ('IDN', 'CHE'),\n",
       " ('IDN', 'CHN'),\n",
       " ('IDN', 'DEU'),\n",
       " ('IDN', 'FRA'),\n",
       " ('IDN', 'HKG'),\n",
       " ('IDN', 'IND'),\n",
       " ('IDN', 'JPN'),\n",
       " ('IDN', 'KOR'),\n",
       " ('IDN', 'MYS'),\n",
       " ('IDN', 'NLD'),\n",
       " ('IDN', 'PHL'),\n",
       " ('IDN', 'SGP'),\n",
       " ('IDN', 'THA'),\n",
       " ('IDN', 'USA'),\n",
       " ('IDN', 'VNM'),\n",
       " ('IND', 'AUS'),\n",
       " ('IND', 'CHE'),\n",
       " ('IND', 'CHN'),\n",
       " ('IND', 'DEU'),\n",
       " ('IND', 'FRA'),\n",
       " ('IND', 'HKG'),\n",
       " ('IND', 'IDN'),\n",
       " ('IND', 'JPN'),\n",
       " ('IND', 'KOR'),\n",
       " ('IND', 'MYS'),\n",
       " ('IND', 'NLD'),\n",
       " ('IND', 'PHL'),\n",
       " ('IND', 'SGP'),\n",
       " ('IND', 'THA'),\n",
       " ('IND', 'USA'),\n",
       " ('IND', 'VNM'),\n",
       " ('JPN', 'AUS'),\n",
       " ('JPN', 'CHE'),\n",
       " ('JPN', 'CHN'),\n",
       " ('JPN', 'DEU'),\n",
       " ('JPN', 'FRA'),\n",
       " ('JPN', 'HKG'),\n",
       " ('JPN', 'IDN'),\n",
       " ('JPN', 'IND'),\n",
       " ('JPN', 'KOR'),\n",
       " ('JPN', 'MYS'),\n",
       " ('JPN', 'NLD'),\n",
       " ('JPN', 'PHL'),\n",
       " ('JPN', 'SGP'),\n",
       " ('JPN', 'THA'),\n",
       " ('JPN', 'USA'),\n",
       " ('JPN', 'VNM'),\n",
       " ('KOR', 'AUS'),\n",
       " ('KOR', 'CHE'),\n",
       " ('KOR', 'CHN'),\n",
       " ('KOR', 'DEU'),\n",
       " ('KOR', 'FRA'),\n",
       " ('KOR', 'HKG'),\n",
       " ('KOR', 'IDN'),\n",
       " ('KOR', 'IND'),\n",
       " ('KOR', 'JPN'),\n",
       " ('KOR', 'MYS'),\n",
       " ('KOR', 'NLD'),\n",
       " ('KOR', 'PHL'),\n",
       " ('KOR', 'SGP'),\n",
       " ('KOR', 'THA'),\n",
       " ('KOR', 'USA'),\n",
       " ('KOR', 'VNM'),\n",
       " ('MYS', 'AUS'),\n",
       " ('MYS', 'CHE'),\n",
       " ('MYS', 'CHN'),\n",
       " ('MYS', 'DEU'),\n",
       " ('MYS', 'FRA'),\n",
       " ('MYS', 'HKG'),\n",
       " ('MYS', 'IDN'),\n",
       " ('MYS', 'IND'),\n",
       " ('MYS', 'JPN'),\n",
       " ('MYS', 'KOR'),\n",
       " ('MYS', 'NLD'),\n",
       " ('MYS', 'PHL'),\n",
       " ('MYS', 'SGP'),\n",
       " ('MYS', 'THA'),\n",
       " ('MYS', 'USA'),\n",
       " ('MYS', 'VNM'),\n",
       " ('NLD', 'AUS'),\n",
       " ('NLD', 'CHE'),\n",
       " ('NLD', 'CHN'),\n",
       " ('NLD', 'DEU'),\n",
       " ('NLD', 'FRA'),\n",
       " ('NLD', 'HKG'),\n",
       " ('NLD', 'IDN'),\n",
       " ('NLD', 'IND'),\n",
       " ('NLD', 'JPN'),\n",
       " ('NLD', 'KOR'),\n",
       " ('NLD', 'MYS'),\n",
       " ('NLD', 'PHL'),\n",
       " ('NLD', 'SGP'),\n",
       " ('NLD', 'THA'),\n",
       " ('NLD', 'USA'),\n",
       " ('NLD', 'VNM'),\n",
       " ('PHL', 'AUS'),\n",
       " ('PHL', 'CHE'),\n",
       " ('PHL', 'CHN'),\n",
       " ('PHL', 'DEU'),\n",
       " ('PHL', 'FRA'),\n",
       " ('PHL', 'HKG'),\n",
       " ('PHL', 'IDN'),\n",
       " ('PHL', 'IND'),\n",
       " ('PHL', 'JPN'),\n",
       " ('PHL', 'KOR'),\n",
       " ('PHL', 'MYS'),\n",
       " ('PHL', 'NLD'),\n",
       " ('PHL', 'SGP'),\n",
       " ('PHL', 'THA'),\n",
       " ('PHL', 'USA'),\n",
       " ('PHL', 'VNM'),\n",
       " ('SGP', 'AUS'),\n",
       " ('SGP', 'CHE'),\n",
       " ('SGP', 'CHN'),\n",
       " ('SGP', 'DEU'),\n",
       " ('SGP', 'FRA'),\n",
       " ('SGP', 'HKG'),\n",
       " ('SGP', 'IDN'),\n",
       " ('SGP', 'IND'),\n",
       " ('SGP', 'JPN'),\n",
       " ('SGP', 'KOR'),\n",
       " ('SGP', 'MYS'),\n",
       " ('SGP', 'NLD'),\n",
       " ('SGP', 'PHL'),\n",
       " ('SGP', 'THA'),\n",
       " ('SGP', 'USA'),\n",
       " ('SGP', 'VNM'),\n",
       " ('THA', 'AUS'),\n",
       " ('THA', 'CHE'),\n",
       " ('THA', 'CHN'),\n",
       " ('THA', 'DEU'),\n",
       " ('THA', 'FRA'),\n",
       " ('THA', 'HKG'),\n",
       " ('THA', 'IDN'),\n",
       " ('THA', 'IND'),\n",
       " ('THA', 'JPN'),\n",
       " ('THA', 'KOR'),\n",
       " ('THA', 'MYS'),\n",
       " ('THA', 'NLD'),\n",
       " ('THA', 'PHL'),\n",
       " ('THA', 'SGP'),\n",
       " ('THA', 'USA'),\n",
       " ('THA', 'VNM'),\n",
       " ('USA', 'AUS'),\n",
       " ('USA', 'CHE'),\n",
       " ('USA', 'CHN'),\n",
       " ('USA', 'DEU'),\n",
       " ('USA', 'FRA'),\n",
       " ('USA', 'HKG'),\n",
       " ('USA', 'IDN'),\n",
       " ('USA', 'IND'),\n",
       " ('USA', 'JPN'),\n",
       " ('USA', 'KOR'),\n",
       " ('USA', 'MYS'),\n",
       " ('USA', 'NLD'),\n",
       " ('USA', 'PHL'),\n",
       " ('USA', 'SGP'),\n",
       " ('USA', 'THA'),\n",
       " ('USA', 'VNM'),\n",
       " ('VNM', 'AUS'),\n",
       " ('VNM', 'CHE'),\n",
       " ('VNM', 'CHN'),\n",
       " ('VNM', 'DEU'),\n",
       " ('VNM', 'FRA'),\n",
       " ('VNM', 'HKG'),\n",
       " ('VNM', 'IDN'),\n",
       " ('VNM', 'IND'),\n",
       " ('VNM', 'JPN'),\n",
       " ('VNM', 'KOR'),\n",
       " ('VNM', 'MYS'),\n",
       " ('VNM', 'NLD'),\n",
       " ('VNM', 'PHL'),\n",
       " ('VNM', 'SGP'),\n",
       " ('VNM', 'THA'),\n",
       " ('VNM', 'USA')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "country_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector Dictionary\n",
    "0. Category 1\n",
    "</br> Description: Agriculture, forestry, fishing, food, beverages, tobacco\n",
    "</br> HS Goods: 972\n",
    "\n",
    "1. Category 2\n",
    "</br>Description: Mining, quarrying, refinery, fuels, chemicals, electricity, water, waste treatment\n",
    "</br>HS Goods: 983\n",
    "\n",
    "2. Category 3\n",
    "</br>Description: Construction, wood, glass, stone, basic metals, housing, electrical appliances, furniture\n",
    "</br>HS Goods: 1313\n",
    "\n",
    "3. Category 4\n",
    "</br>Description: Textile, apparel, shoes, jewelry, leather\n",
    "</br>HS Goods: 895\n",
    "\n",
    "4. Category 5\n",
    "</br>Description: Transport equipment and services, travel, postal services\n",
    "</br>HS Goods: 180\n",
    "\n",
    "5. Category 6\n",
    "</br>Description: ICT, media, computers, business and financial services\n",
    "</br>HS Goods: 441\n",
    "\n",
    "6. Category 7\n",
    "</br>Description: Health, pharmaceuticals, education, cultural, sport\n",
    "</br>HS Goods: 178\n",
    "\n",
    "7. Category 8\n",
    "</br>Description: Government, military and other\n",
    "</br>HS Goods: 139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize the dataset by MinMax11 Normalization\n",
      "Windows shape (num_samples, input_len, N, D): (9, 6, 272, 9)\n",
      "Input shape (num_samples, horizon, N, D): (9, 3, 272, 9)\n",
      "Target shape (num_samples, N, D): (9, 3, 272, 8)\n",
      "Train samples: 7\n",
      "Validation samples: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#do normalisation\n",
    "data_to_normalize = training_data_tensor[:, :, :8]\n",
    "normalized_data, scaler = normalize_dataset(data_to_normalize, normalizer=args.normaliser,column_wise=True)\n",
    "remaining_features = training_data_tensor[:, :, 8:]\n",
    "# Get the shape dimensions\n",
    "T, N, _ = remaining_features.shape\n",
    "\n",
    "# Initialize the scaler with the desired feature range (0, 1)\n",
    "scaler2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape the first column of remaining_features to 2D (T*N, 1)\n",
    "col_data = remaining_features[:, :, 0].reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the column data using the scaler\n",
    "col_scaled = scaler2.fit_transform(col_data)\n",
    "# Concatenate along the last axis\n",
    "normalized_training_data = np.concatenate((normalized_data, remaining_features), axis=-1)\n",
    "\n",
    "# 2. Group data into overlapping windows of 7 time periods (3 input + 3 ahead).\n",
    "windows = group_into_windows(normalized_training_data, window_size=6)\n",
    "print(\"Windows shape (num_samples, input_len, N, D):\", windows.shape)\n",
    "\n",
    "# 3. Split each window into 3 input time periods and a single target (3 steps forward).\n",
    "x, y = split_input_target_direct(windows, input_len=3, horizon=3)\n",
    "print(\"Input shape (num_samples, horizon, N, D):\", x.shape)\n",
    "print(\"Target shape (num_samples, N, D):\", y.shape)\n",
    "\n",
    "# 4. Perform train/validation split.\n",
    "x_train, y_train, x_val, y_val = train_val_split(x, y, val_ratio=0.2)\n",
    "print(\"Train samples:\", x_train.shape[0])\n",
    "print(\"Validation samples:\", x_val.shape[0])\n",
    "\n",
    "x_train_tensor=torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor=torch.tensor(y_train, dtype=torch.float32)\n",
    "train_dataset=TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "x_val_tensor=torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor=torch.tensor(y_val, dtype=torch.float32)\n",
    "val_dataset=TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create the dataset and data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from AGCRN.model.BasicTrainer import Trainer\n",
    "from agcrn_model import AGCRNFinal\n",
    "import os\n",
    "\n",
    "def run():\n",
    "    model=AGCRNFinal(args)\n",
    "    model=model.to(args.device)\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim() >= 2:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        else:\n",
    "            # For biases or 1D parameters, just fill with zeros or some small constant\n",
    "            nn.init.zeros_(p)\n",
    "\n",
    "    #load dataset here\n",
    "\n",
    "    #init loss function, optimizer\n",
    "    loss=torch.nn.MSELoss().to(args.device)\n",
    "    optimizer=optim.Adam(model.parameters(),lr=args.lr_init,eps=1.0e-8,weight_decay=0.0,amsgrad=False)\n",
    "\n",
    "    #learning rate decay\n",
    "    lr_scheduler=None\n",
    "    if args.lr_decay:\n",
    "        print('Applying learning rate decay.')\n",
    "        lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                            milestones=lr_decay_steps,\n",
    "                                                            gamma=args.lr_decay_rate)\n",
    "\n",
    "    #config log path\n",
    "    current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    current_dir = os.getcwd()\n",
    "    log_dir = os.path.join(current_dir,'logs')\n",
    "    args.log_dir = log_dir\n",
    "\n",
    "    #start training\n",
    "    trainer = Trainer(model, loss, optimizer, train_loader, val_loader, scaler=scaler, #need to get these \n",
    "                    args=args, lr_scheduler=lr_scheduler)\n",
    "    if args.mode == 'train':\n",
    "        trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning\n",
    "\n",
    "We test a few possible hyperparameters based on the paper and do a 80-20 split of the data and get the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model is lr_init=0.004, embed=10, lr_decay=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=[5,10,15]\n",
    "lr_init=[0.002,0.003,0.004,0.006,0.008]\n",
    "lr_decay_rate=[0.1,0.2,0.3]\n",
    "\n",
    "for embed in embed_dim:\n",
    "    for lr in lr_init:\n",
    "        for decay_rate in lr_decay_rate:\n",
    "            args.set_args(embed_dim=embed, lr_init=lr,lr_decay_rate=decay_rate)\n",
    "            run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation to pipeline data into model (for 2021 to 2023 % change volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country_a</th>\n",
       "      <th>country_b</th>\n",
       "      <th>bec_1</th>\n",
       "      <th>bec_2</th>\n",
       "      <th>bec_3</th>\n",
       "      <th>bec_4</th>\n",
       "      <th>bec_5</th>\n",
       "      <th>bec_6</th>\n",
       "      <th>bec_7</th>\n",
       "      <th>bec_8</th>\n",
       "      <th>D</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ARE</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.060739e+09</td>\n",
       "      <td>0.620151</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ARE</td>\n",
       "      <td>CHE</td>\n",
       "      <td>3.797882e+06</td>\n",
       "      <td>1.355991e+06</td>\n",
       "      <td>6.939408e+06</td>\n",
       "      <td>3.759398e+08</td>\n",
       "      <td>2.690339e+06</td>\n",
       "      <td>9.364289e+05</td>\n",
       "      <td>4.422947e+07</td>\n",
       "      <td>9.824469e+07</td>\n",
       "      <td>0.586050</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ARE</td>\n",
       "      <td>CHN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.953953e+09</td>\n",
       "      <td>0.635445</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ARE</td>\n",
       "      <td>DEU</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.159614e+08</td>\n",
       "      <td>0.566027</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ARE</td>\n",
       "      <td>FRA</td>\n",
       "      <td>4.972880e+07</td>\n",
       "      <td>4.680296e+07</td>\n",
       "      <td>9.306163e+07</td>\n",
       "      <td>7.307789e+07</td>\n",
       "      <td>9.781168e+07</td>\n",
       "      <td>1.751175e+07</td>\n",
       "      <td>1.162716e+08</td>\n",
       "      <td>2.462986e+08</td>\n",
       "      <td>0.620551</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 country_a country_b         bec_1         bec_2         bec_3  \\\n",
       "0           0       ARE       AUS  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "1           1       ARE       CHE  3.797882e+06  1.355991e+06  6.939408e+06   \n",
       "2           2       ARE       CHN  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "3           3       ARE       DEU  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "4           4       ARE       FRA  4.972880e+07  4.680296e+07  9.306163e+07   \n",
       "\n",
       "          bec_4         bec_5         bec_6         bec_7         bec_8  \\\n",
       "0  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.060739e+09   \n",
       "1  3.759398e+08  2.690339e+06  9.364289e+05  4.422947e+07  9.824469e+07   \n",
       "2  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  2.953953e+09   \n",
       "3  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  7.159614e+08   \n",
       "4  7.307789e+07  9.781168e+07  1.751175e+07  1.162716e+08  2.462986e+08   \n",
       "\n",
       "          D  year  \n",
       "0  0.620151  2006  \n",
       "1  0.586050  2006  \n",
       "2  0.635445  2006  \n",
       "3  0.566027  2006  \n",
       "4  0.620551  2006  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp=pd.read_csv('../data/final/final_modeL_data.csv',header=0)\n",
    "# temp.head()\n",
    "# temp.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# temp=temp[(temp['country_a']!='ARE')& (temp['country_b']!='ARE')]\n",
    "# temp=temp[(temp['year']>=2020) & (temp['year']<=2023)]\n",
    "# temp_only_d=temp[['D','country_a','country_b','year']]\n",
    "# temp.drop(columns=['D'],inplace=True)\n",
    "# temp.sort_values(by=['country_a','country_b','year'],inplace=True)\n",
    "# bec_columns = [f'bec_{i}' for i in range(1, 9)]\n",
    "# for col in bec_columns:\n",
    "#     # Create a new column to store the percentage change.\n",
    "#     temp[f'pct_{col}'] = temp.groupby(['country_a', 'country_b'])[col].pct_change() * 100\n",
    "# temp.dropna(inplace=True)\n",
    "# temp.reset_index(drop=True,inplace=True)\n",
    "# temp=temp.merge(temp_only_d, on=['country_a','country_b','year'], how='left')\n",
    "# temp.drop(columns=bec_columns,inplace=True)\n",
    "# temp.rename(columns={'pct_bec_1':'bec_1','pct_bec_2':'bec_2','pct_bec_3':'bec_3','pct_bec_4':'bec_4','pct_bec_5':'bec_5','pct_bec_6':'bec_6','pct_bec_7':'bec_7','pct_bec_8':'bec_8'},inplace=True)\n",
    "# temp.to_csv('../data/final/without_ARE_pct_2021_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_actual=Args()\n",
    "args_actual.set_args(embed_dim=10, lr_init=0.004,lr_decay_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining the data into the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_tensor_run(csv_file,sentiment_dict,year_nlp=2023):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with columns:\n",
    "      country1, country2, sector1, sector2, ..., sector8, sentiment, year\n",
    "    and returns a tensor of shape (T, N, D), where:\n",
    "      T = number of years,\n",
    "      N = number of unique country pairs,\n",
    "      D = num of sectors + features.\n",
    "    Also returns the sorted list of years and country pair nodes.\n",
    "    \"\"\"\n",
    "    def change_sentiment_index(df1,dict,year):\n",
    "        \"\"\"\n",
    "        Change the sentiment index of the dataframe based on NLP model. (For now, it replaces based on year)\n",
    "        \"\"\"\n",
    "        for country_pair, sentiment in dict.items():\n",
    "            # Extract the country pair from the tuple\n",
    "            country_a, country_b = country_pair.split('-')\n",
    "            # Update the sentiment index for the specific year and country pair\n",
    "            df1.loc[(df1['year'] == year) & (df1['country_a'] == country_a) & (df1['country_b'] == country_b), 'sentiment_index'] = sentiment\n",
    "            df1.loc[(df1['year'] == year) & (df1['country_a'] == country_b) & (df1['country_b'] == country_a), 'sentiment_index'] = sentiment\n",
    "        return df1\n",
    "    # Read the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df=change_sentiment_index(df,sentiment_dict,year_nlp)\n",
    "\n",
    "    # Transform tradeagreementindex and sentiment_index into D\n",
    "    df['D']=1+(-1)*0.5*df['tradeagreementindex']+(-1)*0.5*df['sentiment_index']\n",
    "    df=df.drop(columns=['sentiment_index','tradeagreementindex'],axis=1)   \n",
    "\n",
    "    T = 3 # Number of years to get from\n",
    "    # Get all unique country pairs\n",
    "    pairs_df = df[['country_a', 'country_b']].drop_duplicates()\n",
    "    # Create a sorted list of tuples (country1, country2) for consistent node ordering\n",
    "    country_pairs = sorted([tuple(x) for x in pairs_df.values])\n",
    "    N = len(country_pairs)\n",
    "    \n",
    "    # Number of features (8 sectors + 1 sentiment)\n",
    "    D = 9\n",
    "\n",
    "    # Initialize an empty numpy array for the tensor data\n",
    "    tensor_data = np.empty((T, N, D), dtype=float)\n",
    "    \n",
    "    # Loop over each year and each country pair to fill in the tensor\n",
    "    for t, year in enumerate(years):\n",
    "        # Get data for the current year\n",
    "        df_year = df[df['year'] == year]\n",
    "        for n, (c1, c2) in enumerate(country_pairs):\n",
    "            # Filter rows for the current country pair\n",
    "            row = df_year[(df_year['country_a'] == c1) & (df_year['country_b'] == c2)]\n",
    "            if not row.empty:\n",
    "                # Extract the 8 sector columns and the sentiment column.\n",
    "                # Assumes these columns are named exactly as shown.\n",
    "                features = row.iloc[0][['bec_1', 'bec_2', 'bec_3', 'bec_4', \n",
    "                                         'bec_5', 'bec_6', 'bec_7', 'bec_8', 'D']].values\n",
    "                tensor_data[t, n, :] = features.astype(float)\n",
    "            else:\n",
    "                # If a record is missing for a given year/country pair, fill with zeros (or choose another strategy)\n",
    "                tensor_data[t, n, :] = np.zeros(D)\n",
    "                \n",
    "    return tensor_data, years, country_pairs\n",
    "\n",
    "def group_into_windows(tensor_data, window_size):\n",
    "    \"\"\"\n",
    "    Given a tensor of shape (T, N, D), group the data into overlapping windows.\n",
    "    Each window is of length window_size\n",
    "    Returns a numpy array of shape (num_samples, window_size, N, D).\n",
    "    \"\"\"\n",
    "    T, N, D = tensor_data.shape\n",
    "    num_samples = T - window_size + 1  # sliding window with stride 1\n",
    "    windows = []\n",
    "    for i in range(num_samples):\n",
    "        window = tensor_data[i: i + window_size]  # shape: (window_size, N, D)\n",
    "        windows.append(window)\n",
    "    windows = np.stack(windows)  # shape: (num_samples, window_size, N, D)\n",
    "    return windows\n",
    "\n",
    "def split_input_target_direct(windows, input_len, horizon=3):\n",
    "    \"\"\"\n",
    "    Splits each window into input and a single target that is horizon steps forward.\n",
    "    \n",
    "    windows: numpy array of shape (num_samples, window_size, N, D)\n",
    "              where window_size = input_len + horizon.\n",
    "    input_len: number of time steps used as input.\n",
    "    horizon: steps forward to pick the target (here, horizon=3).\n",
    "    \n",
    "    Returns:\n",
    "      x: inputs of shape (num_samples, input_len, N, D)\n",
    "      y: targets of shape (num_samples, N, 8), which are the first 8 features of the target time step.\n",
    "    \"\"\"\n",
    "    # x: first input_len time steps (e.g., years 2006-2009 if input_len=4)\n",
    "    x = windows[:, :input_len]  \n",
    "    # y_full: the time step exactly horizon steps forward (i.e., index input_len + horizon - 1)\n",
    "    # y_full = windows[:, input_len + horizon-1]  \n",
    "    y_full = windows[:, input_len:input_len + horizon]\n",
    "    # y: only the first 8 features from the predicted time step (ignoring sentiment_index and tradeagreementindex)\n",
    "    y = y_full[..., :8]\n",
    "    return x, y\n",
    "\n",
    "def train_val_split(x, y, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data into train and validation sets by ratio.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    split_index = int(num_samples * (1 - val_ratio))\n",
    "    x_train, y_train = x[:split_index], y[:split_index]\n",
    "    x_val, y_val = x[split_index:], y[split_index:]\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AGCRN.lib.dataloader import normalize_dataset\n",
    "\n",
    "test_data_tensor, years, country_pairs_model = csv_to_tensor_run('../data/final/run_model_data.csv',sentiment_dict=sentiment_dict,year_nlp=year_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect country_pairs to ensure it is the same as initial_nums to perform data transformation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AUS', 'CHE'),\n",
       " ('AUS', 'CHN'),\n",
       " ('AUS', 'DEU'),\n",
       " ('AUS', 'FRA'),\n",
       " ('AUS', 'HKG'),\n",
       " ('AUS', 'IDN'),\n",
       " ('AUS', 'IND'),\n",
       " ('AUS', 'JPN'),\n",
       " ('AUS', 'KOR'),\n",
       " ('AUS', 'MYS'),\n",
       " ('AUS', 'NLD'),\n",
       " ('AUS', 'PHL'),\n",
       " ('AUS', 'SGP'),\n",
       " ('AUS', 'THA'),\n",
       " ('AUS', 'USA'),\n",
       " ('AUS', 'VNM'),\n",
       " ('CHE', 'AUS'),\n",
       " ('CHE', 'CHN'),\n",
       " ('CHE', 'DEU'),\n",
       " ('CHE', 'FRA'),\n",
       " ('CHE', 'HKG'),\n",
       " ('CHE', 'IDN'),\n",
       " ('CHE', 'IND'),\n",
       " ('CHE', 'JPN'),\n",
       " ('CHE', 'KOR'),\n",
       " ('CHE', 'MYS'),\n",
       " ('CHE', 'NLD'),\n",
       " ('CHE', 'PHL'),\n",
       " ('CHE', 'SGP'),\n",
       " ('CHE', 'THA'),\n",
       " ('CHE', 'USA'),\n",
       " ('CHE', 'VNM'),\n",
       " ('CHN', 'AUS'),\n",
       " ('CHN', 'CHE'),\n",
       " ('CHN', 'DEU'),\n",
       " ('CHN', 'FRA'),\n",
       " ('CHN', 'HKG'),\n",
       " ('CHN', 'IDN'),\n",
       " ('CHN', 'IND'),\n",
       " ('CHN', 'JPN'),\n",
       " ('CHN', 'KOR'),\n",
       " ('CHN', 'MYS'),\n",
       " ('CHN', 'NLD'),\n",
       " ('CHN', 'PHL'),\n",
       " ('CHN', 'SGP'),\n",
       " ('CHN', 'THA'),\n",
       " ('CHN', 'USA'),\n",
       " ('CHN', 'VNM'),\n",
       " ('DEU', 'AUS'),\n",
       " ('DEU', 'CHE'),\n",
       " ('DEU', 'CHN'),\n",
       " ('DEU', 'FRA'),\n",
       " ('DEU', 'HKG'),\n",
       " ('DEU', 'IDN'),\n",
       " ('DEU', 'IND'),\n",
       " ('DEU', 'JPN'),\n",
       " ('DEU', 'KOR'),\n",
       " ('DEU', 'MYS'),\n",
       " ('DEU', 'NLD'),\n",
       " ('DEU', 'PHL'),\n",
       " ('DEU', 'SGP'),\n",
       " ('DEU', 'THA'),\n",
       " ('DEU', 'USA'),\n",
       " ('DEU', 'VNM'),\n",
       " ('FRA', 'AUS'),\n",
       " ('FRA', 'CHE'),\n",
       " ('FRA', 'CHN'),\n",
       " ('FRA', 'DEU'),\n",
       " ('FRA', 'HKG'),\n",
       " ('FRA', 'IDN'),\n",
       " ('FRA', 'IND'),\n",
       " ('FRA', 'JPN'),\n",
       " ('FRA', 'KOR'),\n",
       " ('FRA', 'MYS'),\n",
       " ('FRA', 'NLD'),\n",
       " ('FRA', 'PHL'),\n",
       " ('FRA', 'SGP'),\n",
       " ('FRA', 'THA'),\n",
       " ('FRA', 'USA'),\n",
       " ('FRA', 'VNM'),\n",
       " ('HKG', 'AUS'),\n",
       " ('HKG', 'CHE'),\n",
       " ('HKG', 'CHN'),\n",
       " ('HKG', 'DEU'),\n",
       " ('HKG', 'FRA'),\n",
       " ('HKG', 'IDN'),\n",
       " ('HKG', 'IND'),\n",
       " ('HKG', 'JPN'),\n",
       " ('HKG', 'KOR'),\n",
       " ('HKG', 'MYS'),\n",
       " ('HKG', 'NLD'),\n",
       " ('HKG', 'PHL'),\n",
       " ('HKG', 'SGP'),\n",
       " ('HKG', 'THA'),\n",
       " ('HKG', 'USA'),\n",
       " ('HKG', 'VNM'),\n",
       " ('IDN', 'AUS'),\n",
       " ('IDN', 'CHE'),\n",
       " ('IDN', 'CHN'),\n",
       " ('IDN', 'DEU'),\n",
       " ('IDN', 'FRA'),\n",
       " ('IDN', 'HKG'),\n",
       " ('IDN', 'IND'),\n",
       " ('IDN', 'JPN'),\n",
       " ('IDN', 'KOR'),\n",
       " ('IDN', 'MYS'),\n",
       " ('IDN', 'NLD'),\n",
       " ('IDN', 'PHL'),\n",
       " ('IDN', 'SGP'),\n",
       " ('IDN', 'THA'),\n",
       " ('IDN', 'USA'),\n",
       " ('IDN', 'VNM'),\n",
       " ('IND', 'AUS'),\n",
       " ('IND', 'CHE'),\n",
       " ('IND', 'CHN'),\n",
       " ('IND', 'DEU'),\n",
       " ('IND', 'FRA'),\n",
       " ('IND', 'HKG'),\n",
       " ('IND', 'IDN'),\n",
       " ('IND', 'JPN'),\n",
       " ('IND', 'KOR'),\n",
       " ('IND', 'MYS'),\n",
       " ('IND', 'NLD'),\n",
       " ('IND', 'PHL'),\n",
       " ('IND', 'SGP'),\n",
       " ('IND', 'THA'),\n",
       " ('IND', 'USA'),\n",
       " ('IND', 'VNM'),\n",
       " ('JPN', 'AUS'),\n",
       " ('JPN', 'CHE'),\n",
       " ('JPN', 'CHN'),\n",
       " ('JPN', 'DEU'),\n",
       " ('JPN', 'FRA'),\n",
       " ('JPN', 'HKG'),\n",
       " ('JPN', 'IDN'),\n",
       " ('JPN', 'IND'),\n",
       " ('JPN', 'KOR'),\n",
       " ('JPN', 'MYS'),\n",
       " ('JPN', 'NLD'),\n",
       " ('JPN', 'PHL'),\n",
       " ('JPN', 'SGP'),\n",
       " ('JPN', 'THA'),\n",
       " ('JPN', 'USA'),\n",
       " ('JPN', 'VNM'),\n",
       " ('KOR', 'AUS'),\n",
       " ('KOR', 'CHE'),\n",
       " ('KOR', 'CHN'),\n",
       " ('KOR', 'DEU'),\n",
       " ('KOR', 'FRA'),\n",
       " ('KOR', 'HKG'),\n",
       " ('KOR', 'IDN'),\n",
       " ('KOR', 'IND'),\n",
       " ('KOR', 'JPN'),\n",
       " ('KOR', 'MYS'),\n",
       " ('KOR', 'NLD'),\n",
       " ('KOR', 'PHL'),\n",
       " ('KOR', 'SGP'),\n",
       " ('KOR', 'THA'),\n",
       " ('KOR', 'USA'),\n",
       " ('KOR', 'VNM'),\n",
       " ('MYS', 'AUS'),\n",
       " ('MYS', 'CHE'),\n",
       " ('MYS', 'CHN'),\n",
       " ('MYS', 'DEU'),\n",
       " ('MYS', 'FRA'),\n",
       " ('MYS', 'HKG'),\n",
       " ('MYS', 'IDN'),\n",
       " ('MYS', 'IND'),\n",
       " ('MYS', 'JPN'),\n",
       " ('MYS', 'KOR'),\n",
       " ('MYS', 'NLD'),\n",
       " ('MYS', 'PHL'),\n",
       " ('MYS', 'SGP'),\n",
       " ('MYS', 'THA'),\n",
       " ('MYS', 'USA'),\n",
       " ('MYS', 'VNM'),\n",
       " ('NLD', 'AUS'),\n",
       " ('NLD', 'CHE'),\n",
       " ('NLD', 'CHN'),\n",
       " ('NLD', 'DEU'),\n",
       " ('NLD', 'FRA'),\n",
       " ('NLD', 'HKG'),\n",
       " ('NLD', 'IDN'),\n",
       " ('NLD', 'IND'),\n",
       " ('NLD', 'JPN'),\n",
       " ('NLD', 'KOR'),\n",
       " ('NLD', 'MYS'),\n",
       " ('NLD', 'PHL'),\n",
       " ('NLD', 'SGP'),\n",
       " ('NLD', 'THA'),\n",
       " ('NLD', 'USA'),\n",
       " ('NLD', 'VNM'),\n",
       " ('PHL', 'AUS'),\n",
       " ('PHL', 'CHE'),\n",
       " ('PHL', 'CHN'),\n",
       " ('PHL', 'DEU'),\n",
       " ('PHL', 'FRA'),\n",
       " ('PHL', 'HKG'),\n",
       " ('PHL', 'IDN'),\n",
       " ('PHL', 'IND'),\n",
       " ('PHL', 'JPN'),\n",
       " ('PHL', 'KOR'),\n",
       " ('PHL', 'MYS'),\n",
       " ('PHL', 'NLD'),\n",
       " ('PHL', 'SGP'),\n",
       " ('PHL', 'THA'),\n",
       " ('PHL', 'USA'),\n",
       " ('PHL', 'VNM'),\n",
       " ('SGP', 'AUS'),\n",
       " ('SGP', 'CHE'),\n",
       " ('SGP', 'CHN'),\n",
       " ('SGP', 'DEU'),\n",
       " ('SGP', 'FRA'),\n",
       " ('SGP', 'HKG'),\n",
       " ('SGP', 'IDN'),\n",
       " ('SGP', 'IND'),\n",
       " ('SGP', 'JPN'),\n",
       " ('SGP', 'KOR'),\n",
       " ('SGP', 'MYS'),\n",
       " ('SGP', 'NLD'),\n",
       " ('SGP', 'PHL'),\n",
       " ('SGP', 'THA'),\n",
       " ('SGP', 'USA'),\n",
       " ('SGP', 'VNM'),\n",
       " ('THA', 'AUS'),\n",
       " ('THA', 'CHE'),\n",
       " ('THA', 'CHN'),\n",
       " ('THA', 'DEU'),\n",
       " ('THA', 'FRA'),\n",
       " ('THA', 'HKG'),\n",
       " ('THA', 'IDN'),\n",
       " ('THA', 'IND'),\n",
       " ('THA', 'JPN'),\n",
       " ('THA', 'KOR'),\n",
       " ('THA', 'MYS'),\n",
       " ('THA', 'NLD'),\n",
       " ('THA', 'PHL'),\n",
       " ('THA', 'SGP'),\n",
       " ('THA', 'USA'),\n",
       " ('THA', 'VNM'),\n",
       " ('USA', 'AUS'),\n",
       " ('USA', 'CHE'),\n",
       " ('USA', 'CHN'),\n",
       " ('USA', 'DEU'),\n",
       " ('USA', 'FRA'),\n",
       " ('USA', 'HKG'),\n",
       " ('USA', 'IDN'),\n",
       " ('USA', 'IND'),\n",
       " ('USA', 'JPN'),\n",
       " ('USA', 'KOR'),\n",
       " ('USA', 'MYS'),\n",
       " ('USA', 'NLD'),\n",
       " ('USA', 'PHL'),\n",
       " ('USA', 'SGP'),\n",
       " ('USA', 'THA'),\n",
       " ('USA', 'VNM'),\n",
       " ('VNM', 'AUS'),\n",
       " ('VNM', 'CHE'),\n",
       " ('VNM', 'CHN'),\n",
       " ('VNM', 'DEU'),\n",
       " ('VNM', 'FRA'),\n",
       " ('VNM', 'HKG'),\n",
       " ('VNM', 'IDN'),\n",
       " ('VNM', 'IND'),\n",
       " ('VNM', 'JPN'),\n",
       " ('VNM', 'KOR'),\n",
       " ('VNM', 'MYS'),\n",
       " ('VNM', 'NLD'),\n",
       " ('VNM', 'PHL'),\n",
       " ('VNM', 'SGP'),\n",
       " ('VNM', 'THA'),\n",
       " ('VNM', 'USA')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_pairs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for year 2021:\n",
      "[[-23.76676415   5.44830872   8.48811764 ... -16.42896358   0.16023751\n",
      "    0.80989102]\n",
      " [ -4.32133521  28.1916725   27.40833829 ...   4.54033474 -27.55138592\n",
      "    0.86075767]\n",
      " [ 30.04506149  -6.13320462   7.47301508 ... -19.3816128   10.17375255\n",
      "    0.83792047]\n",
      " ...\n",
      " [  4.71237695  53.61440096  27.75656542 ...   2.92823602  29.68103916\n",
      "    0.63215932]\n",
      " [-11.86357687  46.59080175  10.18924034 ...   4.79350723  45.04526876\n",
      "    0.64640775]\n",
      " [ 14.98410578  23.31365631  36.93566773 ...  11.27367646  10.96054528\n",
      "    0.85279134]]\n",
      "Features for ('AUS', 'CHE') in 2021:\n",
      "[-23.76676415   5.44830872   8.48811764 -36.85539035 -14.90805586\n",
      " -14.90717037 -16.42896358   0.16023751   0.80989102]\n"
     ]
    }
   ],
   "source": [
    "# Inspect data to check\n",
    "print(\"Data for year {}:\".format(years[0]))\n",
    "print(test_data_tensor[0])  # prints the data for all nodes/features for the first year\n",
    "\n",
    "\n",
    "print(\"Features for {} in {}:\".format(country_pairs[0], years[0]))\n",
    "print(test_data_tensor[0, 0, :]) # prints the features for the first country pair in the first year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize the dataset by MinMax11 Normalization\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#do normalisation\n",
    "data_to_normalize = test_data_tensor[:, :, :8]\n",
    "normalized_data, scaler = normalize_dataset(data_to_normalize, normalizer=args.normaliser,column_wise=True)\n",
    "remaining_features = test_data_tensor[:, :, 8:]\n",
    "\n",
    "# Get the shape dimensions\n",
    "T, N, _ = remaining_features.shape\n",
    "\n",
    "# Initialize the scaler with the desired feature range (-1, 1)\n",
    "scaler2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape the first column of remaining_features to 2D (T*N, 1)\n",
    "col_data = remaining_features[:, :, 0].reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the column data using the scaler\n",
    "col_scaled = scaler2.fit_transform(col_data)\n",
    "# Concatenate along the last axis\n",
    "normalized_test_data = np.concatenate((normalized_data, remaining_features), axis=-1)\n",
    "test_x_tensor=torch.tensor(normalized_test_data, dtype=torch.float32)\n",
    "test_x_tensor = test_x_tensor.unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single pass of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agcrn_model import AGCRNFinal\n",
    "\n",
    "#load model and previously saved states\n",
    "model=AGCRNFinal(args)\n",
    "model=model.to(args.device)\n",
    "model.load_state_dict(torch.load('./logs/lr_init_0.004_embed_dim_10_lr_decay_0.1/best_model.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_x_tensor = test_x_tensor.to(args.device)\n",
    "    predictions = model(test_x_tensor,None,0)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "\n",
    "#convert back to original scale\n",
    "predictions = scaler.inverse_transform(predictions[0, :, :, :8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert % change predictions into absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_nums=pd.read_csv('../data/final/without_ARE_2021_2023.csv',header=0)\n",
    "\n",
    "# we only need 2023 data to compute forecasted 2024, 2025 and 2026 values\n",
    "initial_nums=initial_nums[initial_nums['year']==2023]\n",
    "initial_nums.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for ordering of country pairs before appending back to dataframe (should have no outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe indices by country pairs\n",
    "for country_pair in country_pairs:\n",
    "    country_a, country_b = country_pair\n",
    "    \n",
    "    # Extract the subset of dataframe where country_a and country_b match\n",
    "    subset = initial_nums[(initial_nums['country_a'] == country_a) & (initial_nums['country_b'] == country_b)]\n",
    "    \n",
    "    #check for any country pair/year missing \n",
    "    if subset.empty:\n",
    "       print(f'country_pair: ({country_a}, {country_b}) is missing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying percentage change to absolute values to get forecasted 24,25,26 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bec_cols=[f'bec_{i}' for i in range(1, 9)]\n",
    "future_years = [2024, 2025, 2026]\n",
    "predicted=[]\n",
    "\n",
    "for pair_idx, row in initial_nums.iterrows():\n",
    "    # convert base values to numpy array and float\n",
    "    base_values = row[bec_cols].values.astype(float)\n",
    "    \n",
    "    # copy just for saving the initial values\n",
    "    current_values = base_values.copy()\n",
    "    \n",
    "    # Apply the percentage changes for each future year\n",
    "    for year_offset, future_year in enumerate(future_years):\n",
    "        # extract the 8 % changes for country pair in forecast_year\n",
    "        pct_change = predictions[year_offset, pair_idx, :]\n",
    "        # convert to factor change\n",
    "        factor = 1 + pct_change / 100.0\n",
    "        \n",
    "        # update current values\n",
    "        current_values = current_values * factor\n",
    "        \n",
    "        # build dictionary to build final dataframe\n",
    "        row_dict = {\n",
    "            'country_a': row['country_a'],\n",
    "            'country_b': row['country_b'],\n",
    "            'year': future_year\n",
    "        }\n",
    "        # add each bec column\n",
    "        for col_idx, col in enumerate(bec_cols):\n",
    "            row_dict[col] = current_values[col_idx]\n",
    "        \n",
    "        # Add the row to our list\n",
    "        predicted.append(row_dict)\n",
    "\n",
    "# Create a new df containing the predicted absolute trade volumes for 2024, 2025, 2026\n",
    "predictions_df = pd.DataFrame(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline data to frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put exports and imports in the same row and transform column names\n",
    "temp=pd.merge(predictions_df,predictions_df,how='outer',left_on=['country_a','country_b','year'],right_on=['country_b','country_a','year'],suffixes=('_export_A_to_b', '_import_A_from_B'))\n",
    "temp.drop(columns=['country_a_import_A_from_B','country_b_import_A_from_B'],inplace=True)\n",
    "temp.rename(columns={'country_a_export_A_to_b':'country_a','country_b_export_A_to_b':'country_b'},inplace=True)\n",
    "temp['country_pair'] = temp.apply(\n",
    "    lambda x: '_'.join(sorted([x['country_a'], x['country_b']])), \n",
    "    axis=1\n",
    ")\n",
    "temp = temp.drop_duplicates(subset=['country_pair', 'year'], keep='first')\n",
    "temp = temp.drop('country_pair', axis=1)\n",
    "\n",
    "#add additional columns for frontend\n",
    "bec_export_cols=[f'bec_{i}_export_A_to_b' for i in range(1, 9)]\n",
    "bec_import_cols=[f'bec_{i}_import_A_from_B' for i in range(1, 9)]\n",
    "temp['total_export_A_to_B']=temp[bec_export_cols].sum(axis=1)\n",
    "temp['total_import_A_from_B']=temp[bec_import_cols].sum(axis=1)\n",
    "temp['trade_volume']=temp['total_export_A_to_B']+temp['total_import_A_from_B']\n",
    "temp['scenario']='forecast'\n",
    "\n",
    "#reorder columns for frontend\n",
    "temp=temp[['country_a','country_b','year','total_export_A_to_B','total_import_A_from_B','trade_volume']+bec_export_cols+bec_import_cols+['scenario']]\n",
    "\n",
    "# #filter only 2026 data and add scenario column for frontend\n",
    "temp=temp[temp['year']==2026] #comment out if need 2024 to 2026 data as well\n",
    "temp['scenario']='postshock'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "temp is the final dataframe that contains the same columns as sample_2026.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country_a', 'country_b', 'year', 'total_export_A_to_B',\n",
       "       'total_import_A_from_B', 'trade_volume', 'bec_1_export_A_to_b',\n",
       "       'bec_2_export_A_to_b', 'bec_3_export_A_to_b', 'bec_4_export_A_to_b',\n",
       "       'bec_5_export_A_to_b', 'bec_6_export_A_to_b', 'bec_7_export_A_to_b',\n",
       "       'bec_8_export_A_to_b', 'bec_1_import_A_from_B', 'bec_2_import_A_from_B',\n",
       "       'bec_3_import_A_from_B', 'bec_4_import_A_from_B',\n",
       "       'bec_5_import_A_from_B', 'bec_6_import_A_from_B',\n",
       "       'bec_7_import_A_from_B', 'bec_8_import_A_from_B', 'scenario'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check temp columns\n",
    "temp.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
