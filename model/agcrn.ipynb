{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from torch_geometric_temporal.nn.recurrent import AGCRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #try batch size=4/7 for 25 years of historical data; for a seq length of 5(5/3 batches per epoch)\n",
    "# #input features will be input sectors (8) + 3 additional features for now\n",
    "# #T will be every 5 years (tbd)\n",
    "# #assume data will be read from csv \n",
    "\n",
    "\n",
    "# class ModelDataset(Dataset):\n",
    "#     def __init__(self, csv_file, T=5):\n",
    "#         \"\"\"\n",
    "#         X shape: [num_samples, T, num_nodes, in_channels]\n",
    "#         Y shape: [num_samples, num_nodes, num_sectors]\n",
    "#         \"\"\"\n",
    "#         df=pd.read_csv(csv_file)\n",
    "#         data = df.values \n",
    "#         self.T = T\n",
    "#         self.X = []\n",
    "#         self.y = []\n",
    "        \n",
    "#         # Build sequences of length 'T'\n",
    "#         # For each index i, we take rows[i : i+T] as inputs \n",
    "#         # and row[i+T] (or some slice) as the target.\n",
    "#         for i in range(len(data) - T):\n",
    "#             # For example, let's say columns [0:13] are features, column 13 is target\n",
    "#             x_seq = data[i : i + T, :13] \n",
    "#             y_val = data[i + T, 13]\n",
    "\n",
    "#             self.X.append(x_seq)\n",
    "#             self.y.append(y_val)\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "#         self.y = torch.tensor(self.y, dtype=torch.float32)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.X.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector Dictionary\n",
    "0. Category 1 (Agri)\n",
    "1. Category 2 (Mining)\n",
    "2. Category 3 (Construction)\n",
    "3. Category 4 (Textile)\n",
    "4. Category 5 (Transport Svcs)\n",
    "5. Category 6 (ICT)\n",
    "6. Category 7 (Health, pharm, sports etc)\n",
    "7. Category 8 (Govt, Millitary, Misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_countries=18\n",
    "num_country_pairs=18*(18-1) \n",
    "num_sectors=8 # 8 sectors\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Model structure\n",
    "        self.num_nodes = num_country_pairs  \n",
    "        self.input_dim = num_sectors+2    # e.g. sectorial export volume + sentiment score + 2 indexes\n",
    "        self.rnn_units = 64\n",
    "        self.output_dim = num_sectors   # e.g., predict only the sectorial export volume\n",
    "        self.horizon = 3      # forecast 3 steps ahead\n",
    "        self.num_layers = 2\n",
    "        self.cheb_k = 2\n",
    "        self.embed_dim = 20\n",
    "        self.default_graph = True  \n",
    "        self.log_dir = './logs/'\n",
    "        self.debug = False\n",
    "        self.model='AGCRN'\n",
    "        self.normaliser = 'std'\n",
    "        self.device='cpu'\n",
    "        self.batch_size=7 # 4/7 depending on results\n",
    "        self.mode='train'\n",
    "        # Training\n",
    "        self.seed=10\n",
    "        self.loss_func= 'mse'\n",
    "        self.epochs = 50\n",
    "        self.lr_init = 0.009\n",
    "        self.lr_decay = False\n",
    "        self.lr_decay_steps = 5,20,40,70\n",
    "        self.lr_decay_rate = 0.3\n",
    "        self.early_stop = True\n",
    "        self.early_stop_patience = 15\n",
    "        self.teacher_forcing = True\n",
    "        self.tf_decay_steps = 20\n",
    "        self.real_value = False\n",
    "        self.grad_norm = True\n",
    "        self.max_grad_norm = 5\n",
    "\n",
    "        # Testing\n",
    "        self.mae_thresh=None\n",
    "        self.mape_thresh=0.\n",
    "\n",
    "        #Logging\n",
    "        self.log_step = 20\n",
    "        self.plot=True\n",
    "\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbic_data=pd.read_csv('../data/cleaned/FBIC_cleaned.csv',header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbic_data=fbic_data.rename(columns={'iso3a':'country_a','iso3b':'country_b'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data=pd.read_csv('../data/final/training_model_data.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data=training_data[['country_a','country_b','bec_1','bec_2','bec_3','bec_4','bec_5','bec_6','bec_7','bec_8','sentiment_index','tradeagreementindex','year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data.to_csv('../data/final/training_model_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_tensor(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with columns:\n",
    "      country1, country2, sector1, sector2, ..., sector8, sentiment, year\n",
    "    and returns a tensor of shape (T, N, D), where:\n",
    "      T = number of years,\n",
    "      N = number of unique country pairs,\n",
    "      D = num of sectors + features.\n",
    "    Also returns the sorted list of years and country pair nodes.\n",
    "    \"\"\"\n",
    "    # Read the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Ensure the 'year' column is integer (if needed)\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    \n",
    "    # Get a sorted list of unique years\n",
    "    years = sorted(df['year'].unique())\n",
    "    T = len(years)\n",
    "    \n",
    "    # Get all unique country pairs\n",
    "    pairs_df = df[['country_a', 'country_b']].drop_duplicates()\n",
    "    # Create a sorted list of tuples (country1, country2) for consistent node ordering\n",
    "    country_pairs = sorted([tuple(x) for x in pairs_df.values])\n",
    "    N = len(country_pairs)\n",
    "    \n",
    "    # Number of features (8 sectors + 1 sentiment)\n",
    "    D = 10\n",
    "\n",
    "    # Initialize an empty numpy array for the tensor data\n",
    "    tensor_data = np.empty((T, N, D), dtype=float)\n",
    "    \n",
    "    # Loop over each year and each country pair to fill in the tensor\n",
    "    for t, year in enumerate(years):\n",
    "        # Get data for the current year\n",
    "        df_year = df[df['year'] == year]\n",
    "        for n, (c1, c2) in enumerate(country_pairs):\n",
    "            # Filter rows for the current country pair\n",
    "            row = df_year[(df_year['country_a'] == c1) & (df_year['country_b'] == c2)]\n",
    "            if not row.empty:\n",
    "                # Extract the 8 sector columns and the sentiment column.\n",
    "                # Assumes these columns are named exactly as shown.\n",
    "                features = row.iloc[0][['bec_1', 'bec_2', 'bec_3', 'bec_4', \n",
    "                                         'bec_5', 'bec_6', 'bec_7', 'bec_8', 'sentiment_index','tradeagreementindex']].values\n",
    "                tensor_data[t, n, :] = features.astype(float)\n",
    "            else:\n",
    "                # If a record is missing for a given year/country pair, fill with zeros (or choose another strategy)\n",
    "                tensor_data[t, n, :] = np.zeros(D)\n",
    "                \n",
    "    return tensor_data, years, country_pairs\n",
    "\n",
    "def group_into_windows(tensor_data, window_size=7):\n",
    "    \"\"\"\n",
    "    Given a tensor of shape (T, N, D), group the data into overlapping windows.\n",
    "    Each window is of length window_size (here, 7 time periods).\n",
    "    Returns a numpy array of shape (num_samples, window_size, N, D).\n",
    "    \"\"\"\n",
    "    T, N, D = tensor_data.shape\n",
    "    num_samples = T - window_size + 1  # sliding window with stride 1\n",
    "    windows = []\n",
    "    for i in range(num_samples):\n",
    "        window = tensor_data[i: i + window_size]  # shape: (window_size, N, D)\n",
    "        windows.append(window)\n",
    "    windows = np.stack(windows)  # shape: (num_samples, window_size, N, D)\n",
    "    return windows\n",
    "\n",
    "def split_input_target_direct(windows, input_len=4, horizon=3):\n",
    "    \"\"\"\n",
    "    Splits each window into input and a single target that is horizon steps forward.\n",
    "    \n",
    "    windows: numpy array of shape (num_samples, window_size, N, D)\n",
    "              where window_size = input_len + horizon.\n",
    "    input_len: number of time steps used as input.\n",
    "    horizon: steps forward to pick the target (here, horizon=3).\n",
    "    \n",
    "    Returns:\n",
    "      x: inputs of shape (num_samples, input_len, N, D)\n",
    "      y: targets of shape (num_samples, N, 8), which are the first 8 features of the target time step.\n",
    "    \"\"\"\n",
    "    # x: first input_len time steps (e.g., years 2006-2009 if input_len=4)\n",
    "    x = windows[:, :input_len]  \n",
    "    # y_full: the time step exactly horizon steps forward (i.e., index input_len + horizon - 1)\n",
    "    y_full = windows[:, input_len + horizon-1]  \n",
    "    # y: only the first 8 features from the predicted time step (ignoring sentiment_index and tradeagreementindex)\n",
    "    y = y_full[..., :8]\n",
    "    return x, y\n",
    "\n",
    "def train_val_split(x, y, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data into train and validation sets by ratio.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    split_index = int(num_samples * (1 - val_ratio))\n",
    "    x_train, y_train = x[:split_index], y[:split_index]\n",
    "    x_val, y_val = x[split_index:], y[split_index:]\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AGCRN.lib.dataloader import normalize_dataset\n",
    "\n",
    "#convert csv to tensor\n",
    "training_data_tensor, years, country_pairs = csv_to_tensor('../data/final/training_model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for year 2006:\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.06073920e+09\n",
      "  5.48504801e-01 0.00000000e+00]\n",
      " [3.79788246e+06 1.35599141e+06 6.93940766e+06 ... 9.82446937e+07\n",
      "  7.03075909e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.95395251e+09\n",
      "  6.19748649e-01 0.00000000e+00]\n",
      " ...\n",
      " [5.67775428e+07 1.05682782e+09 1.89697853e+08 ... 4.53273661e+08\n",
      "  7.30780274e-01 1.40000000e+01]\n",
      " [1.19379420e+08 3.08204908e+08 2.02183745e+08 ... 9.48527231e+07\n",
      "  6.67942802e-01 1.40000000e+01]\n",
      " [7.46406599e+08 1.21952570e+09 1.19232143e+09 ... 1.21779717e+09\n",
      "  6.90423445e-01 0.00000000e+00]]\n",
      "Features for ('ARE', 'AUS') in 2006:\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.06073920e+09\n",
      " 5.48504801e-01 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# 4. Inspect data for a specific time slice.\n",
    "# For instance, data for the first year:\n",
    "print(\"Data for year {}:\".format(years[0]))\n",
    "print(training_data_tensor[0])  # This prints the data for all nodes/features for the first year.\n",
    "\n",
    "# 5. Inspect data for a specific country pair at a given year.\n",
    "# For example, for the first country pair in the first year:\n",
    "print(\"Features for {} in {}:\".format(country_pairs[0], years[0]))\n",
    "print(training_data_tensor[0, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Dictionary\n",
    "\n",
    "0. Singapore\n",
    "1. China\n",
    "2. Malaysia\n",
    "3. United States\n",
    "4. Hong Kong, China\n",
    "5. Indonesia\n",
    "6. Korea, Rep.\n",
    "7. Japan\n",
    "8. Thailand\n",
    "9. Australia\n",
    "10. Vietnam\n",
    "11. India\n",
    "12. United Arab Emirates\n",
    "13. Philippines\n",
    "14. Germany\n",
    "15. France\n",
    "16. Switzerland\n",
    "17. Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ARE', 'AUS'),\n",
       " ('ARE', 'CHE'),\n",
       " ('ARE', 'CHN'),\n",
       " ('ARE', 'DEU'),\n",
       " ('ARE', 'FRA'),\n",
       " ('ARE', 'HKG'),\n",
       " ('ARE', 'IDN'),\n",
       " ('ARE', 'IND'),\n",
       " ('ARE', 'JPN'),\n",
       " ('ARE', 'KOR'),\n",
       " ('ARE', 'MYS'),\n",
       " ('ARE', 'NLD'),\n",
       " ('ARE', 'PHL'),\n",
       " ('ARE', 'SGP'),\n",
       " ('ARE', 'THA'),\n",
       " ('ARE', 'USA'),\n",
       " ('ARE', 'VNM'),\n",
       " ('AUS', 'ARE'),\n",
       " ('AUS', 'CHE'),\n",
       " ('AUS', 'CHN'),\n",
       " ('AUS', 'DEU'),\n",
       " ('AUS', 'FRA'),\n",
       " ('AUS', 'HKG'),\n",
       " ('AUS', 'IDN'),\n",
       " ('AUS', 'IND'),\n",
       " ('AUS', 'JPN'),\n",
       " ('AUS', 'KOR'),\n",
       " ('AUS', 'MYS'),\n",
       " ('AUS', 'NLD'),\n",
       " ('AUS', 'PHL'),\n",
       " ('AUS', 'SGP'),\n",
       " ('AUS', 'THA'),\n",
       " ('AUS', 'USA'),\n",
       " ('AUS', 'VNM'),\n",
       " ('CHE', 'ARE'),\n",
       " ('CHE', 'AUS'),\n",
       " ('CHE', 'CHN'),\n",
       " ('CHE', 'DEU'),\n",
       " ('CHE', 'FRA'),\n",
       " ('CHE', 'HKG'),\n",
       " ('CHE', 'IDN'),\n",
       " ('CHE', 'IND'),\n",
       " ('CHE', 'JPN'),\n",
       " ('CHE', 'KOR'),\n",
       " ('CHE', 'MYS'),\n",
       " ('CHE', 'NLD'),\n",
       " ('CHE', 'PHL'),\n",
       " ('CHE', 'SGP'),\n",
       " ('CHE', 'THA'),\n",
       " ('CHE', 'USA'),\n",
       " ('CHE', 'VNM'),\n",
       " ('CHN', 'ARE'),\n",
       " ('CHN', 'AUS'),\n",
       " ('CHN', 'CHE'),\n",
       " ('CHN', 'DEU'),\n",
       " ('CHN', 'FRA'),\n",
       " ('CHN', 'HKG'),\n",
       " ('CHN', 'IDN'),\n",
       " ('CHN', 'IND'),\n",
       " ('CHN', 'JPN'),\n",
       " ('CHN', 'KOR'),\n",
       " ('CHN', 'MYS'),\n",
       " ('CHN', 'NLD'),\n",
       " ('CHN', 'PHL'),\n",
       " ('CHN', 'SGP'),\n",
       " ('CHN', 'THA'),\n",
       " ('CHN', 'USA'),\n",
       " ('CHN', 'VNM'),\n",
       " ('DEU', 'ARE'),\n",
       " ('DEU', 'AUS'),\n",
       " ('DEU', 'CHE'),\n",
       " ('DEU', 'CHN'),\n",
       " ('DEU', 'FRA'),\n",
       " ('DEU', 'HKG'),\n",
       " ('DEU', 'IDN'),\n",
       " ('DEU', 'IND'),\n",
       " ('DEU', 'JPN'),\n",
       " ('DEU', 'KOR'),\n",
       " ('DEU', 'MYS'),\n",
       " ('DEU', 'NLD'),\n",
       " ('DEU', 'PHL'),\n",
       " ('DEU', 'SGP'),\n",
       " ('DEU', 'THA'),\n",
       " ('DEU', 'USA'),\n",
       " ('DEU', 'VNM'),\n",
       " ('FRA', 'ARE'),\n",
       " ('FRA', 'AUS'),\n",
       " ('FRA', 'CHE'),\n",
       " ('FRA', 'CHN'),\n",
       " ('FRA', 'DEU'),\n",
       " ('FRA', 'HKG'),\n",
       " ('FRA', 'IDN'),\n",
       " ('FRA', 'IND'),\n",
       " ('FRA', 'JPN'),\n",
       " ('FRA', 'KOR'),\n",
       " ('FRA', 'MYS'),\n",
       " ('FRA', 'NLD'),\n",
       " ('FRA', 'PHL'),\n",
       " ('FRA', 'SGP'),\n",
       " ('FRA', 'THA'),\n",
       " ('FRA', 'USA'),\n",
       " ('FRA', 'VNM'),\n",
       " ('HKG', 'ARE'),\n",
       " ('HKG', 'AUS'),\n",
       " ('HKG', 'CHE'),\n",
       " ('HKG', 'CHN'),\n",
       " ('HKG', 'DEU'),\n",
       " ('HKG', 'FRA'),\n",
       " ('HKG', 'IDN'),\n",
       " ('HKG', 'IND'),\n",
       " ('HKG', 'JPN'),\n",
       " ('HKG', 'KOR'),\n",
       " ('HKG', 'MYS'),\n",
       " ('HKG', 'NLD'),\n",
       " ('HKG', 'PHL'),\n",
       " ('HKG', 'SGP'),\n",
       " ('HKG', 'THA'),\n",
       " ('HKG', 'USA'),\n",
       " ('HKG', 'VNM'),\n",
       " ('IDN', 'ARE'),\n",
       " ('IDN', 'AUS'),\n",
       " ('IDN', 'CHE'),\n",
       " ('IDN', 'CHN'),\n",
       " ('IDN', 'DEU'),\n",
       " ('IDN', 'FRA'),\n",
       " ('IDN', 'HKG'),\n",
       " ('IDN', 'IND'),\n",
       " ('IDN', 'JPN'),\n",
       " ('IDN', 'KOR'),\n",
       " ('IDN', 'MYS'),\n",
       " ('IDN', 'NLD'),\n",
       " ('IDN', 'PHL'),\n",
       " ('IDN', 'SGP'),\n",
       " ('IDN', 'THA'),\n",
       " ('IDN', 'USA'),\n",
       " ('IDN', 'VNM'),\n",
       " ('IND', 'ARE'),\n",
       " ('IND', 'AUS'),\n",
       " ('IND', 'CHE'),\n",
       " ('IND', 'CHN'),\n",
       " ('IND', 'DEU'),\n",
       " ('IND', 'FRA'),\n",
       " ('IND', 'HKG'),\n",
       " ('IND', 'IDN'),\n",
       " ('IND', 'JPN'),\n",
       " ('IND', 'KOR'),\n",
       " ('IND', 'MYS'),\n",
       " ('IND', 'NLD'),\n",
       " ('IND', 'PHL'),\n",
       " ('IND', 'SGP'),\n",
       " ('IND', 'THA'),\n",
       " ('IND', 'USA'),\n",
       " ('IND', 'VNM'),\n",
       " ('JPN', 'ARE'),\n",
       " ('JPN', 'AUS'),\n",
       " ('JPN', 'CHE'),\n",
       " ('JPN', 'CHN'),\n",
       " ('JPN', 'DEU'),\n",
       " ('JPN', 'FRA'),\n",
       " ('JPN', 'HKG'),\n",
       " ('JPN', 'IDN'),\n",
       " ('JPN', 'IND'),\n",
       " ('JPN', 'KOR'),\n",
       " ('JPN', 'MYS'),\n",
       " ('JPN', 'NLD'),\n",
       " ('JPN', 'PHL'),\n",
       " ('JPN', 'SGP'),\n",
       " ('JPN', 'THA'),\n",
       " ('JPN', 'USA'),\n",
       " ('JPN', 'VNM'),\n",
       " ('KOR', 'ARE'),\n",
       " ('KOR', 'AUS'),\n",
       " ('KOR', 'CHE'),\n",
       " ('KOR', 'CHN'),\n",
       " ('KOR', 'DEU'),\n",
       " ('KOR', 'FRA'),\n",
       " ('KOR', 'HKG'),\n",
       " ('KOR', 'IDN'),\n",
       " ('KOR', 'IND'),\n",
       " ('KOR', 'JPN'),\n",
       " ('KOR', 'MYS'),\n",
       " ('KOR', 'NLD'),\n",
       " ('KOR', 'PHL'),\n",
       " ('KOR', 'SGP'),\n",
       " ('KOR', 'THA'),\n",
       " ('KOR', 'USA'),\n",
       " ('KOR', 'VNM'),\n",
       " ('MYS', 'ARE'),\n",
       " ('MYS', 'AUS'),\n",
       " ('MYS', 'CHE'),\n",
       " ('MYS', 'CHN'),\n",
       " ('MYS', 'DEU'),\n",
       " ('MYS', 'FRA'),\n",
       " ('MYS', 'HKG'),\n",
       " ('MYS', 'IDN'),\n",
       " ('MYS', 'IND'),\n",
       " ('MYS', 'JPN'),\n",
       " ('MYS', 'KOR'),\n",
       " ('MYS', 'NLD'),\n",
       " ('MYS', 'PHL'),\n",
       " ('MYS', 'SGP'),\n",
       " ('MYS', 'THA'),\n",
       " ('MYS', 'USA'),\n",
       " ('MYS', 'VNM'),\n",
       " ('NLD', 'ARE'),\n",
       " ('NLD', 'AUS'),\n",
       " ('NLD', 'CHE'),\n",
       " ('NLD', 'CHN'),\n",
       " ('NLD', 'DEU'),\n",
       " ('NLD', 'FRA'),\n",
       " ('NLD', 'HKG'),\n",
       " ('NLD', 'IDN'),\n",
       " ('NLD', 'IND'),\n",
       " ('NLD', 'JPN'),\n",
       " ('NLD', 'KOR'),\n",
       " ('NLD', 'MYS'),\n",
       " ('NLD', 'PHL'),\n",
       " ('NLD', 'SGP'),\n",
       " ('NLD', 'THA'),\n",
       " ('NLD', 'USA'),\n",
       " ('NLD', 'VNM'),\n",
       " ('PHL', 'ARE'),\n",
       " ('PHL', 'AUS'),\n",
       " ('PHL', 'CHE'),\n",
       " ('PHL', 'CHN'),\n",
       " ('PHL', 'DEU'),\n",
       " ('PHL', 'FRA'),\n",
       " ('PHL', 'HKG'),\n",
       " ('PHL', 'IDN'),\n",
       " ('PHL', 'IND'),\n",
       " ('PHL', 'JPN'),\n",
       " ('PHL', 'KOR'),\n",
       " ('PHL', 'MYS'),\n",
       " ('PHL', 'NLD'),\n",
       " ('PHL', 'SGP'),\n",
       " ('PHL', 'THA'),\n",
       " ('PHL', 'USA'),\n",
       " ('PHL', 'VNM'),\n",
       " ('SGP', 'ARE'),\n",
       " ('SGP', 'AUS'),\n",
       " ('SGP', 'CHE'),\n",
       " ('SGP', 'CHN'),\n",
       " ('SGP', 'DEU'),\n",
       " ('SGP', 'FRA'),\n",
       " ('SGP', 'HKG'),\n",
       " ('SGP', 'IDN'),\n",
       " ('SGP', 'IND'),\n",
       " ('SGP', 'JPN'),\n",
       " ('SGP', 'KOR'),\n",
       " ('SGP', 'MYS'),\n",
       " ('SGP', 'NLD'),\n",
       " ('SGP', 'PHL'),\n",
       " ('SGP', 'THA'),\n",
       " ('SGP', 'USA'),\n",
       " ('SGP', 'VNM'),\n",
       " ('THA', 'ARE'),\n",
       " ('THA', 'AUS'),\n",
       " ('THA', 'CHE'),\n",
       " ('THA', 'CHN'),\n",
       " ('THA', 'DEU'),\n",
       " ('THA', 'FRA'),\n",
       " ('THA', 'HKG'),\n",
       " ('THA', 'IDN'),\n",
       " ('THA', 'IND'),\n",
       " ('THA', 'JPN'),\n",
       " ('THA', 'KOR'),\n",
       " ('THA', 'MYS'),\n",
       " ('THA', 'NLD'),\n",
       " ('THA', 'PHL'),\n",
       " ('THA', 'SGP'),\n",
       " ('THA', 'USA'),\n",
       " ('THA', 'VNM'),\n",
       " ('USA', 'ARE'),\n",
       " ('USA', 'AUS'),\n",
       " ('USA', 'CHE'),\n",
       " ('USA', 'CHN'),\n",
       " ('USA', 'DEU'),\n",
       " ('USA', 'FRA'),\n",
       " ('USA', 'HKG'),\n",
       " ('USA', 'IDN'),\n",
       " ('USA', 'IND'),\n",
       " ('USA', 'JPN'),\n",
       " ('USA', 'KOR'),\n",
       " ('USA', 'MYS'),\n",
       " ('USA', 'NLD'),\n",
       " ('USA', 'PHL'),\n",
       " ('USA', 'SGP'),\n",
       " ('USA', 'THA'),\n",
       " ('USA', 'VNM'),\n",
       " ('VNM', 'ARE'),\n",
       " ('VNM', 'AUS'),\n",
       " ('VNM', 'CHE'),\n",
       " ('VNM', 'CHN'),\n",
       " ('VNM', 'DEU'),\n",
       " ('VNM', 'FRA'),\n",
       " ('VNM', 'HKG'),\n",
       " ('VNM', 'IDN'),\n",
       " ('VNM', 'IND'),\n",
       " ('VNM', 'JPN'),\n",
       " ('VNM', 'KOR'),\n",
       " ('VNM', 'MYS'),\n",
       " ('VNM', 'NLD'),\n",
       " ('VNM', 'PHL'),\n",
       " ('VNM', 'SGP'),\n",
       " ('VNM', 'THA'),\n",
       " ('VNM', 'USA')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize the dataset by Standard Normalization\n",
      "Windows shape (num_samples, 7, N, D): (9, 6, 306, 10)\n",
      "Input shape (num_samples, 4, N, D): (9, 3, 306, 10)\n",
      "Target shape (num_samples, N, D): (9, 306, 8)\n",
      "Train samples: 7\n",
      "Validation samples: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#do normalisation\n",
    "data_to_normalize = training_data_tensor[:, :, :8]\n",
    "normalized_data, scaler = normalize_dataset(data_to_normalize, normalizer='std',column_wise=True)\n",
    "remaining_features = training_data_tensor[:, :, 8:]\n",
    "# Get the shape dimensions\n",
    "T, N, _ = remaining_features.shape\n",
    "\n",
    "# Initialize the scaler with the desired feature range (0, 1)\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Reshape the first column of remaining_features to 2D (T*N, 1)\n",
    "col_data = remaining_features[:, :, 0].reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the column data using the scaler\n",
    "col_scaled = scaler2.fit_transform(col_data)\n",
    "# Concatenate along the last axis\n",
    "normalized_training_data = np.concatenate((normalized_data, remaining_features), axis=-1)\n",
    "\n",
    "# 2. Group data into overlapping windows of 7 time periods (3 input + 3 ahead).\n",
    "windows = group_into_windows(normalized_training_data, window_size=6)\n",
    "print(\"Windows shape (num_samples, 7, N, D):\", windows.shape)\n",
    "\n",
    "# 3. Split each window into 3 input time periods and a single target (3 steps forward).\n",
    "x, y = split_input_target_direct(windows, input_len=3, horizon=3)\n",
    "print(\"Input shape (num_samples, 4, N, D):\", x.shape)\n",
    "print(\"Target shape (num_samples, N, D):\", y.shape)\n",
    "\n",
    "# 4. Perform train/validation split.\n",
    "x_train, y_train, x_val, y_val = train_val_split(x, y, val_ratio=0.2)\n",
    "print(\"Train samples:\", x_train.shape[0])\n",
    "print(\"Validation samples:\", x_val.shape[0])\n",
    "\n",
    "x_train_tensor=torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor=torch.tensor(y_train, dtype=torch.float32)\n",
    "train_dataset=TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "x_val_tensor=torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor=torch.tensor(y_val, dtype=torch.float32)\n",
    "val_dataset=TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create the dataset and data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Experiment log path in: c:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\model\\logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creat Log File in:  c:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\model\\logs\\run.log\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 1: 0/1 Loss: 0.772359\n",
      "2025-04-06 14:02: **********Train Epoch 1: averaged Loss: 0.772359, tf_ratio: 0.952381\n",
      "2025-04-06 14:02: **********Val Epoch 1: average Loss: 1.235979\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 2: 0/1 Loss: 0.770021\n",
      "2025-04-06 14:02: **********Train Epoch 2: averaged Loss: 0.770021, tf_ratio: 0.950061\n",
      "2025-04-06 14:02: **********Val Epoch 2: average Loss: 1.231963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 3: 0/1 Loss: 0.767568\n",
      "2025-04-06 14:02: **********Train Epoch 3: averaged Loss: 0.767568, tf_ratio: 0.947635\n",
      "2025-04-06 14:02: **********Val Epoch 3: average Loss: 1.227639\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 4: 0/1 Loss: 0.764887\n",
      "2025-04-06 14:02: **********Train Epoch 4: averaged Loss: 0.764887, tf_ratio: 0.945098\n",
      "2025-04-06 14:02: **********Val Epoch 4: average Loss: 1.222878\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 5: 0/1 Loss: 0.761887\n",
      "2025-04-06 14:02: **********Train Epoch 5: averaged Loss: 0.761887, tf_ratio: 0.942445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: **********Val Epoch 5: average Loss: 1.217555\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 6: 0/1 Loss: 0.758472\n",
      "2025-04-06 14:02: **********Train Epoch 6: averaged Loss: 0.758472, tf_ratio: 0.939672\n",
      "2025-04-06 14:02: **********Val Epoch 6: average Loss: 1.211541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 7: 0/1 Loss: 0.754540\n",
      "2025-04-06 14:02: **********Train Epoch 7: averaged Loss: 0.754540, tf_ratio: 0.936774\n",
      "2025-04-06 14:02: **********Val Epoch 7: average Loss: 1.204692\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 8: 0/1 Loss: 0.749972\n",
      "2025-04-06 14:02: **********Train Epoch 8: averaged Loss: 0.749972, tf_ratio: 0.933747\n",
      "2025-04-06 14:02: **********Val Epoch 8: average Loss: 1.196840\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 9: 0/1 Loss: 0.744636\n",
      "2025-04-06 14:02: **********Train Epoch 9: averaged Loss: 0.744636, tf_ratio: 0.930586\n",
      "2025-04-06 14:02: **********Val Epoch 9: average Loss: 1.187795\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 10: 0/1 Loss: 0.738384\n",
      "2025-04-06 14:02: **********Train Epoch 10: averaged Loss: 0.738384, tf_ratio: 0.927286\n",
      "2025-04-06 14:02: **********Val Epoch 10: average Loss: 1.177350\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 11: 0/1 Loss: 0.731069\n",
      "2025-04-06 14:02: **********Train Epoch 11: averaged Loss: 0.731069, tf_ratio: 0.923842\n",
      "2025-04-06 14:02: **********Val Epoch 11: average Loss: 1.165311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 12: 0/1 Loss: 0.722580\n",
      "2025-04-06 14:02: **********Train Epoch 12: averaged Loss: 0.722580, tf_ratio: 0.920249\n",
      "2025-04-06 14:02: **********Val Epoch 12: average Loss: 1.151546\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 13: 0/1 Loss: 0.712931\n",
      "2025-04-06 14:02: **********Train Epoch 13: averaged Loss: 0.712931, tf_ratio: 0.916501\n",
      "2025-04-06 14:02: **********Val Epoch 13: average Loss: 1.136118\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 14: 0/1 Loss: 0.702443\n",
      "2025-04-06 14:02: **********Train Epoch 14: averaged Loss: 0.702443, tf_ratio: 0.912594\n",
      "2025-04-06 14:02: **********Val Epoch 14: average Loss: 1.119560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 15: 0/1 Loss: 0.692095\n",
      "2025-04-06 14:02: **********Train Epoch 15: averaged Loss: 0.692095, tf_ratio: 0.908523\n",
      "2025-04-06 14:02: **********Val Epoch 15: average Loss: 1.103523\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 16: 0/1 Loss: 0.683865\n",
      "2025-04-06 14:02: **********Train Epoch 16: averaged Loss: 0.683865, tf_ratio: 0.904282\n",
      "2025-04-06 14:02: **********Val Epoch 16: average Loss: 1.091295\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 17: 0/1 Loss: 0.679668\n",
      "2025-04-06 14:02: **********Train Epoch 17: averaged Loss: 0.679668, tf_ratio: 0.899866\n",
      "2025-04-06 14:02: **********Val Epoch 17: average Loss: 1.085439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: *********************************Current best model saved!\n",
      "2025-04-06 14:02: Train Epoch 18: 0/1 Loss: 0.677158\n",
      "2025-04-06 14:02: **********Train Epoch 18: averaged Loss: 0.677158, tf_ratio: 0.895269\n",
      "2025-04-06 14:02: **********Val Epoch 18: average Loss: 1.084393\n",
      "2025-04-06 14:02: *********************************Current best model saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 19: 0/1 Loss: 0.671979\n",
      "2025-04-06 14:02: **********Train Epoch 19: averaged Loss: 0.671979, tf_ratio: 0.890488\n",
      "2025-04-06 14:02: **********Val Epoch 19: average Loss: 1.086369\n",
      "2025-04-06 14:02: Train Epoch 20: 0/1 Loss: 0.664062\n",
      "2025-04-06 14:02: **********Train Epoch 20: averaged Loss: 0.664062, tf_ratio: 0.885516\n",
      "2025-04-06 14:02: **********Val Epoch 20: average Loss: 1.090724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 21: 0/1 Loss: 0.655584\n",
      "2025-04-06 14:02: **********Train Epoch 21: averaged Loss: 0.655584, tf_ratio: 0.880348\n",
      "2025-04-06 14:02: **********Val Epoch 21: average Loss: 1.096871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 22: 0/1 Loss: 0.648067\n",
      "2025-04-06 14:02: **********Train Epoch 22: averaged Loss: 0.648067, tf_ratio: 0.874981\n",
      "2025-04-06 14:02: **********Val Epoch 22: average Loss: 1.103945\n",
      "2025-04-06 14:02: Train Epoch 23: 0/1 Loss: 0.641643\n",
      "2025-04-06 14:02: **********Train Epoch 23: averaged Loss: 0.641643, tf_ratio: 0.869408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: **********Val Epoch 23: average Loss: 1.111114\n",
      "2025-04-06 14:02: Train Epoch 24: 0/1 Loss: 0.635638\n",
      "2025-04-06 14:02: **********Train Epoch 24: averaged Loss: 0.635638, tf_ratio: 0.863625\n",
      "2025-04-06 14:02: **********Val Epoch 24: average Loss: 1.117851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 25: 0/1 Loss: 0.629310\n",
      "2025-04-06 14:02: **********Train Epoch 25: averaged Loss: 0.629310, tf_ratio: 0.857629\n",
      "2025-04-06 14:02: **********Val Epoch 25: average Loss: 1.123959\n",
      "2025-04-06 14:02: Train Epoch 26: 0/1 Loss: 0.622219\n",
      "2025-04-06 14:02: **********Train Epoch 26: averaged Loss: 0.622219, tf_ratio: 0.851414\n",
      "2025-04-06 14:02: **********Val Epoch 26: average Loss: 1.129487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 27: 0/1 Loss: 0.614298\n",
      "2025-04-06 14:02: **********Train Epoch 27: averaged Loss: 0.614298, tf_ratio: 0.844977\n",
      "2025-04-06 14:02: **********Val Epoch 27: average Loss: 1.134602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: Train Epoch 28: 0/1 Loss: 0.605800\n",
      "2025-04-06 14:02: **********Train Epoch 28: averaged Loss: 0.605800, tf_ratio: 0.838313\n",
      "2025-04-06 14:02: **********Val Epoch 28: average Loss: 1.139480\n",
      "2025-04-06 14:02: Train Epoch 29: 0/1 Loss: 0.597180\n",
      "2025-04-06 14:02: **********Train Epoch 29: averaged Loss: 0.597180, tf_ratio: 0.831421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:02: **********Val Epoch 29: average Loss: 1.144210\n",
      "2025-04-06 14:03: Train Epoch 30: 0/1 Loss: 0.588888\n",
      "2025-04-06 14:03: **********Train Epoch 30: averaged Loss: 0.588888, tf_ratio: 0.824296\n",
      "2025-04-06 14:03: **********Val Epoch 30: average Loss: 1.148777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:03: Train Epoch 31: 0/1 Loss: 0.581148\n",
      "2025-04-06 14:03: **********Train Epoch 31: averaged Loss: 0.581148, tf_ratio: 0.816937\n",
      "2025-04-06 14:03: **********Val Epoch 31: average Loss: 1.153077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:03: Train Epoch 32: 0/1 Loss: 0.573856\n",
      "2025-04-06 14:03: **********Train Epoch 32: averaged Loss: 0.573856, tf_ratio: 0.809341\n",
      "2025-04-06 14:03: **********Val Epoch 32: average Loss: 1.156904\n",
      "2025-04-06 14:03: Train Epoch 33: 0/1 Loss: 0.566744\n",
      "2025-04-06 14:03: **********Train Epoch 33: averaged Loss: 0.566744, tf_ratio: 0.801506\n",
      "2025-04-06 14:03: **********Val Epoch 33: average Loss: 1.159859\n",
      "2025-04-06 14:03: Validation performance didn't improve for 15 epochs. Training stops.\n",
      "2025-04-06 14:03: Total training time: 0.1092min, best loss: 1.084393\n",
      "2025-04-06 14:03: Saving current best model to c:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\model\\logs\\best_model.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n",
      "output shape: torch.Size([7, 3, 306, 8])\n",
      "label shape: torch.Size([7, 306, 8])\n",
      "last_output shape: torch.Size([7, 306, 8])\n",
      "output shape: torch.Size([2, 3, 306, 8])\n",
      "label shape: torch.Size([2, 306, 8])\n",
      "last_output shape: torch.Size([2, 306, 8])\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from AGCRN.model.BasicTrainer import Trainer\n",
    "from agcrn_model import AGCRNFinal\n",
    "import os\n",
    "\n",
    "model=AGCRNFinal(args)\n",
    "model=model.to(args.device)\n",
    "for p in model.parameters():\n",
    "    if p.dim() >= 2:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    else:\n",
    "        # For biases or 1D parameters, just fill with zeros or some small constant\n",
    "        nn.init.zeros_(p)\n",
    "\n",
    "#load dataset here\n",
    "\n",
    "#init loss function, optimizer\n",
    "loss=torch.nn.MSELoss().to(args.device)\n",
    "optimizer=optim.Adam(model.parameters(),lr=args.lr_init,eps=1.0e-8,weight_decay=0.0,amsgrad=False)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler=None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "\n",
    "#config log path\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "current_dir = os.getcwd()\n",
    "log_dir = os.path.join(current_dir,'logs')\n",
    "args.log_dir = log_dir\n",
    "\n",
    "#start training\n",
    "trainer = Trainer(model, loss, optimizer, train_loader, val_loader, scaler=scaler, #need to get these \n",
    "                  args=args, lr_scheduler=lr_scheduler)\n",
    "if args.mode == 'train':\n",
    "    trainer.train()\n",
    "# elif args.mode == 'test':\n",
    "#     model.load_state_dict(torch.load('./pre-trained/{}.pth'.format(args.dataset)))\n",
    "#     print(\"Load saved model\")\n",
    "#     trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "# else:\n",
    "#     raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL DEFINITION\n",
    "# model = AGCRN(\n",
    "#     number_of_nodes=num_country_pairs,\n",
    "#     in_channels=in_channels,\n",
    "#     out_channels=out_channels,\n",
    "#     K=K,\n",
    "#     embedding_dimensions=embedding_dims\n",
    "# )\n",
    "\n",
    "# # 3) Create the node embedding E separately (following your interface).\n",
    "# #    We'll just do a random init. This is learnable, so we wrap it in nn.Parameter.\n",
    "# E = nn.Parameter(torch.zeros(num_country_pairs, embedding_dims), requires_grad=True)\n",
    "\n",
    "# # 4) \"prediction head\" to map from [out_channels] -> [num_sectors]\n",
    "# prediction_head = nn.Linear(out_channels, num_sectors)\n",
    "\n",
    "# # 5) Combine everything in a single optimizer. We must include the node embedding (E) as well.\n",
    "# optimizer = optim.Adam(\n",
    "#     list(model.parameters()) + list(prediction_head.parameters()) + [E],\n",
    "#     lr=lr\n",
    "# )\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # 6) Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for X_batch, Y_batch in dataloader:\n",
    "#         # X_batch: [batch_size, num_nodes, in_channels]\n",
    "#         # Y_batch: [batch_size, num_nodes, num_sectors]\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         H = None\n",
    "#         # Unroll over T time steps\n",
    "#         for t in range(T):\n",
    "#             X_t = X_batch[:, t, :, :]  # [batch_size, num_nodes, in_channels]\n",
    "#             H = model(X_t, E, H)  # H is the hidden state, E is the node embedding\n",
    "            \n",
    "\n",
    "#         # Now map from [out_channels] -> 1 dimension\n",
    "#         # We'll do this for each node:\n",
    "#         Y_pred = prediction_head(H)\n",
    "#         print('Y_pred.shape', Y_pred.shape)\n",
    "#         # Compute MSE loss with target\n",
    "#         loss = criterion(Y_pred, Y_batch)\n",
    "\n",
    "#         # Backprop & update\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "#     avg_loss = total_loss / len(dataset)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
