{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "num_nodes = 5  # Number of sectors\n",
    "seq_len = 12  # Sequence length (e.g., months)\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Dummy data for sectorial export trade volumes\n",
    "np.random.seed(0)  # For reproducibility\n",
    "X = np.random.rand(batch_size, num_nodes, seq_len)  # Input data\n",
    "X = torch.from_numpy(X).float()  # Convert to PyTorch tensor\n",
    "\n",
    "# Dummy adjacency matrix (assuming sectors are interconnected)\n",
    "A = torch.ones(num_nodes, num_nodes)  # Simple adjacency matrix\n",
    "\n",
    "# Dummy static features (if applicable)\n",
    "FE = None  # No static features in this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.attention.mtgnn import MTGNNLayer, GraphConstructor\n",
    "# Parameters for MTGNNLayer\n",
    "dilation_exponential = 2\n",
    "rf_size_i = 4\n",
    "kernel_size = 1\n",
    "j = 1\n",
    "residual_channels = 32\n",
    "conv_channels = 32\n",
    "skip_channels = 64\n",
    "kernel_set = [2, 4, 8]\n",
    "new_dilation = 1\n",
    "layer_norm_affline = True\n",
    "gcn_true = True\n",
    "seq_length = seq_len\n",
    "receptive_field = 8\n",
    "dropout = 0.1\n",
    "gcn_depth = 2\n",
    "num_nodes = num_nodes\n",
    "propalpha = 0.5\n",
    "\n",
    "# Initialize MTGNN layer\n",
    "mtgnn_layer = MTGNNLayer(\n",
    "    dilation_exponential,\n",
    "    rf_size_i,\n",
    "    kernel_size,\n",
    "    j,\n",
    "    residual_channels,\n",
    "    conv_channels,\n",
    "    skip_channels,\n",
    "    kernel_set,\n",
    "    new_dilation,\n",
    "    layer_norm_affline,\n",
    "    gcn_true,\n",
    "    seq_length,\n",
    "    receptive_field,\n",
    "    dropout,\n",
    "    gcn_depth,\n",
    "    num_nodes,\n",
    "    propalpha,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "graph_constructor = GraphConstructor(\n",
    "    nnodes=num_nodes, k=3, dim=16, alpha=0.5, xd=None\n",
    ")\n",
    "\n",
    "# Construct adjacency matrix using graph constructor (optional)\n",
    "A_tilde = graph_constructor(torch.arange(num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m X_skip \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Perform forward pass\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmtgnn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_skip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch_geometric_temporal\\nn\\attention\\mtgnn.py:435\u001b[0m, in \u001b[0;36mMTGNNLayer.forward\u001b[1;34m(self, X, X_skip, A_tilde, idx, training)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03mMaking a forward pass of MTGNN layer.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m        with shape (batch_size, in_dim, num_nodes, seq_len).\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    434\u001b[0m X_residual \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m--> 435\u001b[0m X_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m X_filter \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(X_filter)\n\u001b[0;32m    437\u001b[0m X_gate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gate_conv(X)\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch_geometric_temporal\\nn\\attention\\mtgnn.py:142\u001b[0m, in \u001b[0;36mDilatedInception.forward\u001b[1;34m(self, X_in)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kernel_set)):\n\u001b[0;32m    141\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_conv[i](X_in))\n\u001b[1;32m--> 142\u001b[0m     min_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(min_seq_len, \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Trim outputs to the minimum sequence length\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kernel_set)):\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "X_skip = X\n",
    "\n",
    "# Perform forward pass\n",
    "output = mtgnn_layer(X, X_skip, A, torch.arange(num_nodes), training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mtgnn_layer.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # Example number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = mtgnn_layer(X, X_skip, A, torch.arange(num_nodes), training=True)\n",
    "    \n",
    "    # Calculate loss (assuming target is similar to output for demonstration)\n",
    "    target = output.detach()  # For demonstration; use actual target data\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (24) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 167\u001b[0m\n\u001b[0;32m    164\u001b[0m A_tilde \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(num_nodes, num_nodes)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_tilde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[12], line 126\u001b[0m, in \u001b[0;36mSectorialTradePredictor.forward\u001b[1;34m(self, X, A_tilde, training)\u001b[0m\n\u001b[0;32m    124\u001b[0m     current_seq_len \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    125\u001b[0m     idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(current_seq_len)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(X\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 126\u001b[0m     X, X_skip \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_skip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_tilde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Final convolutions on the aggregated skip connection to obtain predictions.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m Y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(X_skip)\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch_geometric_temporal\\nn\\attention\\mtgnn.py:441\u001b[0m, in \u001b[0;36mMTGNNLayer.forward\u001b[1;34m(self, X, X_skip, A_tilde, idx, training)\u001b[0m\n\u001b[0;32m    439\u001b[0m X \u001b[38;5;241m=\u001b[39m X_filter \u001b[38;5;241m*\u001b[39m X_gate\n\u001b[0;32m    440\u001b[0m X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dropout, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m--> 441\u001b[0m X_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_skip_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_skip\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gcn_true:\n\u001b[0;32m    443\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixprop_conv1(X, A_tilde) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixprop_conv2(\n\u001b[0;32m    444\u001b[0m         X, A_tilde\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    445\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (24) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "from torch_geometric_temporal.nn.attention.mtgnn import MTGNNLayer\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Assume that the following components have been implemented and imported:\n",
    "# - DilatedInception\n",
    "# - MixProp\n",
    "# - LayerNormalization\n",
    "# - MTGNNLayer (as shown in your snippet)\n",
    "# ------------------------------------------------------------------------------\n",
    "# For demonstration purposes, we assume these components exist in your library.\n",
    "\n",
    "class SectorialTradePredictor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        seq_length: int,\n",
    "        forecast_horizon: int,\n",
    "        num_layers: int = 3,\n",
    "        residual_channels: int = 32,\n",
    "        conv_channels: int = 32,\n",
    "        skip_channels: int = 64,\n",
    "        kernel_size: int = 2,\n",
    "        dilation_exponential: int = 2,\n",
    "        kernel_set: list = [2, 3, 6, 7],\n",
    "        layer_norm_affline: bool = True,\n",
    "        gcn_true: bool = True,\n",
    "        gcn_depth: int = 2,\n",
    "        dropout: float = 0.3,\n",
    "        propalpha: float = 0.05,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_nodes (int): Number of countries (graph nodes).\n",
    "            in_channels (int): Number of input channels (e.g., sectors).\n",
    "            out_channels (int): Number of output channels (typically same as in_channels).\n",
    "            seq_length (int): Length of historical time series.\n",
    "            forecast_horizon (int): Number of future time steps to predict.\n",
    "            num_layers (int): Number of stacked MTGNN layers.\n",
    "            residual_channels (int): Channels for residual mapping.\n",
    "            conv_channels (int): Channels for the dilated convolutions.\n",
    "            skip_channels (int): Channels for skip connections.\n",
    "            kernel_size (int): Kernel size for dilated convolutions.\n",
    "            dilation_exponential (int): Dilation factor base.\n",
    "            kernel_set (list): List of kernel sizes for the DilatedInception block.\n",
    "            layer_norm_affline (bool): Whether layer norm is affine.\n",
    "            gcn_true (bool): Whether to include graph convolution in the MTGNNLayer.\n",
    "            gcn_depth (int): Depth for the graph convolution.\n",
    "            dropout (float): Dropout rate.\n",
    "            propalpha (float): MixHop propagation parameter.\n",
    "        \"\"\"\n",
    "        super(SectorialTradePredictor, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_nodes = num_nodes\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # Calculate overall receptive field (this simple calculation may need adjustment)\n",
    "        self.receptive_field = 1\n",
    "        for i in range(num_layers):\n",
    "            dilation = dilation_exponential ** i\n",
    "            self.receptive_field += (kernel_size - 1) * dilation\n",
    "\n",
    "        # Initial convolution to lift input dimension to residual_channels\n",
    "        self.input_conv = nn.Conv2d(in_channels, residual_channels, kernel_size=(1, 1))\n",
    "        \n",
    "        # Build a stack of MTGNN layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            dilation = dilation_exponential ** i\n",
    "            layer = MTGNNLayer(\n",
    "                dilation_exponential=dilation_exponential,\n",
    "                rf_size_i=self.receptive_field,  # using the full receptive field\n",
    "                kernel_size=kernel_size,\n",
    "                j=i,\n",
    "                residual_channels=residual_channels,\n",
    "                conv_channels=conv_channels,\n",
    "                skip_channels=skip_channels,\n",
    "                kernel_set=kernel_set,\n",
    "                new_dilation=dilation,\n",
    "                layer_norm_affline=layer_norm_affline,\n",
    "                gcn_true=gcn_true,\n",
    "                seq_length=seq_length,\n",
    "                receptive_field=self.receptive_field,\n",
    "                dropout=dropout,\n",
    "                gcn_depth=gcn_depth,\n",
    "                num_nodes=num_nodes,\n",
    "                propalpha=propalpha,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        # Final convolutions to produce the prediction.\n",
    "        # The first one processes the aggregated skip connection;\n",
    "        # the second one maps to (forecast_horizon * out_channels).\n",
    "        self.end_conv_1 = nn.Conv2d(skip_channels, skip_channels, kernel_size=(1, 1))\n",
    "        self.end_conv_2 = nn.Conv2d(skip_channels, forecast_horizon * out_channels, kernel_size=(1, 1))\n",
    "    \n",
    "    def forward(self, X: torch.FloatTensor, A_tilde: Optional[torch.FloatTensor], training: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (torch.FloatTensor): Input tensor with shape (batch_size, in_channels, num_nodes, seq_length)\n",
    "            A_tilde (torch.FloatTensor or None): Predefined adjacency matrix (num_nodes, num_nodes) with trade relationship scores.\n",
    "            training (bool): Whether the model is in training mode.\n",
    "        \n",
    "        Returns:\n",
    "            torch.FloatTensor: Predictions with shape (batch_size, out_channels, num_nodes, forecast_horizon)\n",
    "        \"\"\"\n",
    "        # Initial projection to match residual channels\n",
    "        X = self.input_conv(X)\n",
    "        batch_size, _, num_nodes, seq_len = X.shape\n",
    "        \n",
    "        # Initialize the skip connection tensor.\n",
    "        # We use the out_channels of the skip_conv from the first layer.\n",
    "        skip_channels = self.layers[0]._skip_conv.out_channels\n",
    "        X_skip = torch.zeros(batch_size, skip_channels, num_nodes, seq_len, device=X.device)\n",
    "        \n",
    "        # Pass through each MTGNN layer\n",
    "        for layer in self.layers:\n",
    "            # Create an index tensor for layer normalization (based on current sequence length)\n",
    "            current_seq_len = X.size(3)\n",
    "            idx = torch.arange(current_seq_len).unsqueeze(0).expand(batch_size, -1).to(X.device)\n",
    "            X, X_skip = layer(X, X_skip, A_tilde, idx, training)\n",
    "        \n",
    "        # Final convolutions on the aggregated skip connection to obtain predictions.\n",
    "        Y = F.relu(X_skip)\n",
    "        Y = F.relu(self.end_conv_1(Y))\n",
    "        Y = self.end_conv_2(Y)\n",
    "        # Assuming Y has shape (batch_size, forecast_horizon*out_channels, num_nodes, 1)\n",
    "        # We squeeze the last dimension and then reshape to (batch_size, out_channels, num_nodes, forecast_horizon)\n",
    "        Y = Y.squeeze(-1)\n",
    "        Y = Y.view(batch_size, self.forecast_horizon, -1, num_nodes)\n",
    "        # Optionally, permute dimensions to (batch_size, out_channels, num_nodes, forecast_horizon)\n",
    "        Y = Y.permute(0, 2, 3, 1)\n",
    "        return Y\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Example usage:\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example parameters:\n",
    "    num_nodes = 10         # e.g., 10 countries\n",
    "    in_channels = 5        # e.g., 5 sectors (input features)\n",
    "    out_channels = 5       # predict the same number of sectors\n",
    "    seq_length = 24        # using the last 24 time steps (e.g., months)\n",
    "    forecast_horizon = 3   # forecast the next 3 time steps\n",
    "    batch_size = 16\n",
    "\n",
    "    # Instantiate the predictor\n",
    "    model = SectorialTradePredictor(\n",
    "        num_nodes=num_nodes,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        seq_length=seq_length,\n",
    "        forecast_horizon=forecast_horizon\n",
    "    )\n",
    "    \n",
    "    # Dummy input tensor: (batch_size, in_channels, num_nodes, seq_length)\n",
    "    X = torch.randn(batch_size, in_channels, num_nodes, seq_length)\n",
    "    # Dummy adjacency matrix (A_tilde): (num_nodes, num_nodes)\n",
    "    A_tilde = torch.rand(num_nodes, num_nodes)\n",
    "    \n",
    "    # Run the model\n",
    "    predictions = model(X, A_tilde, training=True)\n",
    "    print(\"Predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectorTradeForecaster(nn.Module):\n",
    "    def __init__(self, num_sectors, num_countries, seq_len, adj_matrix):\n",
    "        super().__init__()\n",
    "        self.num_sectors = num_sectors\n",
    "        self.num_countries = num_countries\n",
    "        self.seq_len = seq_len\n",
    "        self.register_buffer('A_tilde', adj_matrix)  # Trade relationship matrix\n",
    "\n",
    "        # MTGNN Layer Parameters\n",
    "        self.mtgnn = MTGNNLayer(\n",
    "            dilation_exponential=1,\n",
    "            rf_size_i=1,\n",
    "            kernel_size=3,\n",
    "            j=2,  # Adjust based on desired receptive field\n",
    "            residual_channels=num_sectors,\n",
    "            conv_channels=64,\n",
    "            skip_channels=64,\n",
    "            kernel_set=[2, 3, 6, 7],\n",
    "            new_dilation=1,\n",
    "            layer_norm_affline=True,\n",
    "            gcn_true=True,\n",
    "            seq_length=seq_len,\n",
    "            receptive_field=5,  # Match input sequence after j=2\n",
    "            dropout=0.2,\n",
    "            gcn_depth=2,\n",
    "            num_nodes=num_countries,\n",
    "            propalpha=0.05\n",
    "        )\n",
    "\n",
    "        # Final prediction layer\n",
    "        self.output_conv = nn.Conv2d(64, num_sectors, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        X_skip = 0  # Initialize skip connection\n",
    "        _, X_skip = self.mtgnn(x, X_skip, self.A_tilde, None, self.training)\n",
    "        return self.output_conv(X_skip).squeeze(-1)  # (batch, sectors, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 5, 30, 12])\n",
      "Target shape: torch.Size([32, 5, 30])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TradeVolumeDataset(Dataset):\n",
    "    def __init__(self, data, seq_length, pred_length=1, split='train', split_ratio=0.8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Input tensor of shape (num_countries, num_sectors, total_timesteps)\n",
    "            seq_length: Length of input sequence\n",
    "            pred_length: Length of prediction sequence\n",
    "            split: 'train' or 'val'\n",
    "            split_ratio: Ratio for train-val split\n",
    "        \"\"\"\n",
    "        # Convert to (num_sectors, num_countries, timesteps) for MTGNN compatibility\n",
    "        self.data = data.permute(1, 0, 2)\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "        self.split = split\n",
    "        \n",
    "        # Normalize data\n",
    "        self.scaler = StandardScaler()\n",
    "        scaled_data = self.scaler.fit_transform(self.data.reshape(-1, self.data.shape[-1]))\n",
    "        self.data = torch.tensor(scaled_data.reshape(self.data.shape), dtype=torch.float32)\n",
    "        \n",
    "        # Split data\n",
    "        total_timesteps = self.data.shape[-1]\n",
    "        split_idx = int(total_timesteps * split_ratio)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.data = self.data[..., :split_idx]\n",
    "        else:\n",
    "            self.data = self.data[..., split_idx:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[-1] - self.seq_length - self.pred_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get input sequence\n",
    "        start = idx\n",
    "        end = start + self.seq_length\n",
    "        x = self.data[..., start:end]\n",
    "        \n",
    "        # Get target (next time step for all sectors and countries)\n",
    "        y = self.data[..., end:end+self.pred_length].squeeze(-1)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "def create_dataloaders(data, seq_length, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders\n",
    "    \n",
    "    Args:\n",
    "        data: Raw input tensor (num_countries, num_sectors, total_timesteps)\n",
    "        seq_length: Input sequence length\n",
    "        batch_size: Batch size\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader\n",
    "    \"\"\"\n",
    "    train_dataset = TradeVolumeDataset(\n",
    "        data=data,\n",
    "        seq_length=seq_length,\n",
    "        split='train'\n",
    "    )\n",
    "    \n",
    "    val_dataset = TradeVolumeDataset(\n",
    "        data=data,\n",
    "        seq_length=seq_length,\n",
    "        split='val'\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Example usage -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    num_countries = 30\n",
    "    num_sectors = 5\n",
    "    total_timesteps = 100\n",
    "    seq_length = 12\n",
    "    \n",
    "    # Synthetic data: (countries, sectors, timesteps)\n",
    "    raw_data = torch.rand(num_countries, num_sectors, total_timesteps)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        data=raw_data,\n",
    "        seq_length=seq_length,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Verify batch shape\n",
    "    sample_x, sample_y = next(iter(train_loader))\n",
    "    print(f\"Input shape: {sample_x.shape}\")  # Should be (batch, sectors, countries, seq_length)\n",
    "    print(f\"Target shape: {sample_y.shape}\")  # Should be (batch, sectors, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (30 x 6). Kernel size: (1 x 8). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# y_batch: (batch, sectors, countries)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Predicted next time step\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[0;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m, in \u001b[0;36mSectorTradeForecaster.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     34\u001b[0m     X_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Initialize skip connection\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     _, X_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_skip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA_tilde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv(X_skip)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch_geometric_temporal\\nn\\attention\\mtgnn.py:441\u001b[0m, in \u001b[0;36mMTGNNLayer.forward\u001b[1;34m(self, X, X_skip, A_tilde, idx, training)\u001b[0m\n\u001b[0;32m    439\u001b[0m X \u001b[38;5;241m=\u001b[39m X_filter \u001b[38;5;241m*\u001b[39m X_gate\n\u001b[0;32m    440\u001b[0m X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dropout, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m--> 441\u001b[0m X_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_skip_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m X_skip\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gcn_true:\n\u001b[0;32m    443\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixprop_conv1(X, A_tilde) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixprop_conv2(\n\u001b[0;32m    444\u001b[0m         X, A_tilde\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    445\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (30 x 6). Kernel size: (1 x 8). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "adj_matrix = torch.rand(num_countries, num_countries)  # Replace with real data\n",
    "\n",
    "model = SectorTradeForecaster(num_sectors, num_countries, seq_len, adj_matrix)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for x_batch, y_batch in train_loader:  # y_batch: (batch, sectors, countries)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)  # Predicted next time step\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
