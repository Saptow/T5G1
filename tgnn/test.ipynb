{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Implementation in DGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Attention Score Computation does not involve node features:\n",
    "Mr GPT says to: <br>\n",
    "Option A: Incorporate into Attention Mechanism\n",
    "Concatenate or combine edge features with the node features when computing the attention scores. For example, you could modify the edge_attention method to include the trade volume or political score (after appropriate transformation).\n",
    "\n",
    "\n",
    "Option B: Use a Separate Aggregation for Trade Volumes\n",
    "Since your final output is the total outgoing trade volume per country, you can leave the TGAT message passing largely as is and perform an extra aggregation step on the edge features. For example:\n",
    "\n",
    "Aggregate the trade volumes of outgoing edges for each country. <br>\n",
    "g.update_all(fn.copy_e('trade_vol', 'm'), fn.sum('m', 'total_trade_vol'))\n",
    "total_trade_vol = g.ndata['total_trade_vol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked TGAT Implementation \n",
    "Given we want to shut off certain nodes while doing the prediction, (for eg, the trade analyst wants to only account for the interactions in a certain region), our training might have to account for random shutting off during inference. This is one way we can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n",
    "\n",
    "# Define a Masked TGAT Layer where we apply masks on edges and nodes\n",
    "class MaskedTGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, dropout=0.1):\n",
    "        super(MaskedTGATLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats\n",
    "        # Attention function outputs a scalar per head\n",
    "        self.attn_fc = nn.Linear(2 * out_feats, 1, bias=False)\n",
    "        self.fc = nn.Linear(in_feats, out_feats * num_heads, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # Concatenate source and destination transformed features\n",
    "        z_cat = torch.cat([edges.src['z'], edges.dst['z']], dim=-1)\n",
    "        a = self.attn_fc(z_cat)\n",
    "        a = torch.nn.functional.leaky_relu(a)\n",
    "        # Apply edge mask if available (assumed shape: (E, num_heads, 1))\n",
    "        if 'mask' in edges.data:\n",
    "            a = a * edges.data['mask']\n",
    "        return {'e': a}\n",
    "\n",
    "    def forward(self, g, h, node_mask=None):\n",
    "        with g.local_scope():\n",
    "            # Linear transformation and reshape to (N, num_heads, out_feats)\n",
    "            z = self.fc(h)\n",
    "            z = z.view(z.shape[0], self.num_heads, self.out_feats)\n",
    "            g.ndata['z'] = z\n",
    "\n",
    "            # Ensure each edge has a mask; if not, default to ones\n",
    "            if 'mask' not in g.edata:\n",
    "                g.edata['mask'] = torch.ones(g.number_of_edges(), self.num_heads, 1).to(z.device)\n",
    "\n",
    "            # Compute edge attention values, incorporating the mask\n",
    "            g.apply_edges(self.edge_attention)\n",
    "            # Multiply node features with attention scores and aggregate messages\n",
    "            g.update_all(fn.u_mul_e('z', 'e', 'm'), fn.mean('m', 'h_new'))\n",
    "            # Flatten aggregated features\n",
    "            h_new = g.ndata['h_new'].reshape(g.ndata['h_new'].shape[0], self.num_heads * self.out_feats)\n",
    "            # Apply node mask if provided (mask should be broadcastable to h_new's shape)\n",
    "            if node_mask is not None:\n",
    "                h_new = h_new * node_mask\n",
    "            return h_new\n",
    "\n",
    "# Define a simple TGAT model using the masked layer\n",
    "class MaskedTGAT(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_heads, num_layers):\n",
    "        super(MaskedTGAT, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(MaskedTGATLayer(in_feats, hidden_feats, num_heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            # The next layer takes flattened node features as input\n",
    "            self.layers.append(MaskedTGATLayer(hidden_feats * num_heads, hidden_feats, num_heads))\n",
    "        self.fc_out = nn.Linear(hidden_feats * num_heads, out_feats)\n",
    "\n",
    "    def forward(self, g, h, node_mask=None):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, node_mask)\n",
    "        return self.fc_out(h)\n",
    "\n",
    "# Create a graph and set up masks for nodes and edges\n",
    "def create_masked_graph():\n",
    "    # Create a simple graph with 5 nodes and 5 directed edges\n",
    "    src_nodes = [0, 1, 2, 3, 4]\n",
    "    dst_nodes = [1, 2, 3, 4, 0]\n",
    "    g = dgl.graph((src_nodes, dst_nodes))\n",
    "    \n",
    "    # Example node features (e.g., representing country indicators)\n",
    "    g.ndata['feat'] = torch.randn(len(src_nodes), 10)\n",
    "    \n",
    "    # Define an edge mask: 1 indicates the edge is active, 0 means it's switched off.\n",
    "    # Here we switch off the second edge (index 1).\n",
    "    edge_mask = torch.tensor([[1], [0], [1], [1], [1]], dtype=torch.float32)\n",
    "    # If using multiple heads (e.g., num_heads=2), expand the mask shape to (E, num_heads, 1)\n",
    "    edge_mask = edge_mask.unsqueeze(1).repeat(1, 2, 1)\n",
    "    g.edata['mask'] = edge_mask\n",
    "    \n",
    "    # Define a node mask: here, node 2 is switched off (mask=0) and others remain active (mask=1).\n",
    "    # For a node feature output dimension of D (here, num_heads*out_feats), the mask can be broadcast.\n",
    "    node_mask = torch.tensor([[1], [1], [0], [1], [1]], dtype=torch.float32)\n",
    "    \n",
    "    return g, g.ndata['feat'], node_mask\n",
    "\n",
    "# Example training loop using the masked TGAT model\n",
    "def train_masked_tgat():\n",
    "    g, features, node_mask = create_masked_graph()\n",
    "    model = MaskedTGAT(in_feats=10, hidden_feats=16, out_feats=1, num_heads=2, num_layers=2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Fake regression labels for demonstration (e.g., total trade volume per country)\n",
    "    labels = torch.tensor([[0.5], [0.7], [0.3], [0.9], [0.4]], dtype=torch.float32)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        # Forward pass: pass the node features and the node mask into the model\n",
    "        logits = model(g, features, node_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "train_masked_tgat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "included edge features \n",
    "\n",
    "supposedly included time periods through timestamp -- but not fully sure how its supposed to be done in terms of including in data. \n",
    "GPT mentions that the timestamp embedding time_enc will be added to the edge features which will serve as the time part\n",
    "\n",
    "also worth noting TGAT does not do layers -- so all the time periods / timestamps is supposedly all included and processed within this one layer model \n",
    "\n",
    "[also for note possibly explosive amount of edges]\n",
    "\n",
    "-> next action should probably be to figure out how to include time-based data that will be conceptually similar to what we want to do & try training it on a small scale set of (1) time period (2) node feature (3) edge feature (4) ?output but probably not since training without predicting won't involve it yet. \n",
    "[by Wed night]\n",
    "\n",
    "not sure if i missed anything but this setup should be sufficient at the super base level of what the model needs data-wise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    103\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m \u001b[43mtrain_tgat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 96\u001b[0m, in \u001b[0;36mtrain_tgat\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m150\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 96\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels)\n\u001b[0;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 62\u001b[0m, in \u001b[0;36mTGAT.forward\u001b[1;34m(self, g, h)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, g, h):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 62\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(h)\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m, in \u001b[0;36mTGATLayer.forward\u001b[1;34m(self, g, h)\u001b[0m\n\u001b[0;32m     43\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mview(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_feats)\n\u001b[0;32m     44\u001b[0m g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m z\n\u001b[1;32m---> 45\u001b[0m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attention\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m g\u001b[38;5;241m.\u001b[39mupdate_all(fn\u001b[38;5;241m.\u001b[39mu_mul_e(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m), fn\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_new\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     47\u001b[0m h_new \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_new\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_new\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_feats)\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\dgl\\heterograph.py:4700\u001b[0m, in \u001b[0;36mDGLGraph.apply_edges\u001b[1;34m(self, func, edges, etype)\u001b[0m\n\u001b[0;32m   4698\u001b[0m     edata \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39minvoke_gsddmm(g, func)\n\u001b[0;32m   4699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4700\u001b[0m     edata \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_edge_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mnumber_of_etypes() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m etype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_e_repr(etid, eid, edata)\n",
      "File \u001b[1;32mc:\\Users\\rob-l\\Documents\\NUS\\Y3S2\\DSE3101\\T5G1\\venv\\Lib\\site-packages\\dgl\\core.py:96\u001b[0m, in \u001b[0;36minvoke_edge_udf\u001b[1;34m(graph, eid, etype, func, orig_eid)\u001b[0m\n\u001b[0;32m     87\u001b[0m dstdata \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39m_node_frames[dtid]\u001b[38;5;241m.\u001b[39msubframe(v)\n\u001b[0;32m     88\u001b[0m ebatch \u001b[38;5;241m=\u001b[39m EdgeBatch(\n\u001b[0;32m     89\u001b[0m     graph,\n\u001b[0;32m     90\u001b[0m     eid \u001b[38;5;28;01mif\u001b[39;00m orig_eid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m orig_eid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     dstdata,\n\u001b[0;32m     95\u001b[0m )\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mebatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m, in \u001b[0;36mTGATLayer.edge_attention\u001b[1;34m(self, edges)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21medge_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m, edges):\n\u001b[0;32m     27\u001b[0m     time_enc \u001b[38;5;241m=\u001b[39m temporal_encoding(edges\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_dim)\n\u001b[1;32m---> 28\u001b[0m     z_cat \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdst\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdummy1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdummy2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdummy3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdummy4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_enc\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_fc(z_cat)\n\u001b[0;32m     37\u001b[0m     a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mleaky_relu(a)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n",
    "import numpy as np\n",
    "\n",
    "# Temporal Encoding Function\n",
    "def temporal_encoding(timestamps, dim):\n",
    "    freqs = torch.arange(0, dim // 2, dtype=torch.float32)\n",
    "    freqs = 1.0 / (10000 ** (2 * freqs / dim))\n",
    "    encodings = torch.cat([torch.sin(timestamps * freqs), torch.cos(timestamps * freqs)], dim=-1)\n",
    "    return encodings\n",
    "\n",
    "# Define the Temporal Attention Layer\n",
    "class TGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, time_dim=4, dropout=0.1):\n",
    "        super(TGATLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats\n",
    "        self.time_dim = time_dim\n",
    "        self.attn_fc = nn.Linear(2 * out_feats + 4 + time_dim, 1, bias=False)  # Added 4 edge features and time encoding\n",
    "        self.fc = nn.Linear(in_feats, out_feats * num_heads, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        time_enc = temporal_encoding(edges.data['timestamp'].unsqueeze(-1), self.time_dim)\n",
    "        z_cat = torch.cat([\n",
    "            edges.src['z'], edges.dst['z'], \n",
    "            edges.data['dummy1'].unsqueeze(-1), \n",
    "            edges.data['dummy2'].unsqueeze(-1),\n",
    "            edges.data['dummy3'].unsqueeze(-1),\n",
    "            edges.data['dummy4'].unsqueeze(-1),\n",
    "            time_enc\n",
    "        ], dim=-1)\n",
    "        a = self.attn_fc(z_cat)\n",
    "        a = torch.nn.functional.leaky_relu(a)\n",
    "        return {'e': a}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            z = self.fc(h)\n",
    "            z = z.view(z.shape[0], self.num_heads, self.out_feats)\n",
    "            g.ndata['z'] = z\n",
    "            g.apply_edges(self.edge_attention)\n",
    "            g.update_all(fn.u_mul_e('z', 'e', 'm'), fn.mean('m', 'h_new'))\n",
    "            h_new = g.ndata['h_new'].reshape(g.ndata['h_new'].shape[0], self.num_heads * self.out_feats)\n",
    "            return h_new\n",
    "\n",
    "# Define the TGAT Model\n",
    "class TGAT(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_heads, num_layers, time_dim=4):\n",
    "        super(TGAT, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(TGATLayer(in_feats, hidden_feats, num_heads, time_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(TGATLayer(hidden_feats * num_heads, hidden_feats, num_heads, time_dim))\n",
    "        self.fc_out = nn.Linear(hidden_feats * num_heads, out_feats)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return self.fc_out(h)\n",
    "\n",
    "# Create a small temporal graph dataset with edge features]\n",
    "def create_tg():\n",
    "    src_nodes = [0, 1, 2, 3, 4, 1, 2, 3, 4, 0]  # Bidirectional edges\n",
    "    dst_nodes = [1, 2, 3, 4, 0, 0, 1, 2, 3, 4]\n",
    "    timestamps = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)\n",
    "    trade_volumes = [100, 200, 150, 300, 250, 180, 220, 170, 280, 240]\n",
    "    dummy1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    dummy2 = [5, 4, 3, 2, 1, 0, 1, 2, 3, 4]\n",
    "    dummy3 = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    dummy4 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    g = dgl.graph((src_nodes, dst_nodes))\n",
    "    g.edata['timestamp'] = timestamps\n",
    "    g.edata['trade_vol'] = torch.tensor(trade_volumes, dtype=torch.float32)\n",
    "    g.edata['dummy1'] = torch.tensor(dummy1, dtype=torch.float32)\n",
    "    g.edata['dummy2'] = torch.tensor(dummy2, dtype=torch.float32)\n",
    "    g.edata['dummy3'] = torch.tensor(dummy3, dtype=torch.float32)\n",
    "    g.edata['dummy4'] = torch.tensor(dummy4, dtype=torch.float32)\n",
    "    g.ndata['feat'] = torch.randn(g.num_nodes(), 10)\n",
    "    return g\n",
    "\n",
    "# Train the TGAT Model\n",
    "def train_tgat():\n",
    "    g = create_tg()\n",
    "    model = TGAT(in_feats=10, hidden_feats=16, out_feats=1, num_heads=2, num_layers=2, time_dim=4)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    labels = torch.tensor([200, 300, 250, 400, 150], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        logits = model(g, g.ndata['feat'])\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "train_tgat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Implementation\n",
    "\n",
    "Todo: reference the following url to see how to implement temporal component. \n",
    "https://www.kaggle.com/code/dipanjandas96/temporal-graph-attention-network-tgan/notebook#INDUCTIVE-REPRESENTATION-LEARNING-ON-TEMPORAL-GRAPHS\n",
    "\n",
    "Also, see if the stacking temporal implementation works/need to modify aggregation mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 74965.0703\n",
      "Epoch 10 | Loss: 73102.9844\n",
      "Epoch 20 | Loss: 26777.5215\n",
      "Epoch 30 | Loss: 17917.1855\n",
      "Epoch 40 | Loss: 18315.6816\n",
      "Epoch 50 | Loss: 8199.2002\n",
      "Epoch 60 | Loss: 4419.8477\n",
      "Epoch 70 | Loss: 472.5968\n",
      "Epoch 80 | Loss: 1095.7872\n",
      "Epoch 90 | Loss: 1613.3792\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n",
    "\n",
    "# Temporal Encoding Function inspired by the TGAT paper\n",
    "def temporal_encoding(timestamps, dim):\n",
    "    # timestamps: shape (..., 1)\n",
    "    freqs = torch.arange(0, dim // 2, dtype=torch.float32, device=timestamps.device)\n",
    "    freqs = 1.0 / (10000 ** (2 * freqs / dim))\n",
    "    encodings = torch.cat([torch.sin(timestamps * freqs), torch.cos(timestamps * freqs)], dim=-1)\n",
    "    return encodings\n",
    "\n",
    "# Define the Temporal Attention Layer (TGATLayer)\n",
    "class TGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, time_dim=4, dropout=0.1):\n",
    "        super(TGATLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats\n",
    "        self.time_dim = time_dim\n",
    "        # Input dimension for the attention FC: \n",
    "        # 2 * out_feats (source and destination) + 4 dummy edge features (each of dim 1) + time_dim.\n",
    "        self.attn_fc = nn.Linear(2 * out_feats + 4 + time_dim, 1, bias=False)\n",
    "        self.fc = nn.Linear(in_feats, out_feats * num_heads, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edges.src['z'] and edges.dst['z'] have shape: (E, num_heads, out_feats)\n",
    "\n",
    "        # Reshape dummy features to (E, 1, 1) and then repeat along head dimension.\n",
    "        dummy1 = edges.data['dummy1'].unsqueeze(1).unsqueeze(-1).repeat(1, self.num_heads, 1)\n",
    "        dummy2 = edges.data['dummy2'].unsqueeze(1).unsqueeze(-1).repeat(1, self.num_heads, 1)\n",
    "        dummy3 = edges.data['dummy3'].unsqueeze(1).unsqueeze(-1).repeat(1, self.num_heads, 1)\n",
    "        dummy4 = edges.data['dummy4'].unsqueeze(1).unsqueeze(-1).repeat(1, self.num_heads, 1)\n",
    "\n",
    "        # Compute temporal encoding and repeat along head dimension.\n",
    "        time_enc = temporal_encoding(edges.data['timestamp'].unsqueeze(-1), self.time_dim)\\\n",
    "                     .unsqueeze(1).repeat(1, self.num_heads, 1)\n",
    "\n",
    "        # Concatenate source, destination, the 4 dummy features, and the temporal encoding along the last dimension.\n",
    "        z_cat = torch.cat([\n",
    "            edges.src['z'],            # shape: (E, num_heads, out_feats)\n",
    "            edges.dst['z'],            # shape: (E, num_heads, out_feats)\n",
    "            dummy1,                    # shape: (E, num_heads, 1)\n",
    "            dummy2,                    # shape: (E, num_heads, 1)\n",
    "            dummy3,                    # shape: (E, num_heads, 1)\n",
    "            dummy4,                    # shape: (E, num_heads, 1)\n",
    "            time_enc                   # shape: (E, num_heads, time_dim)\n",
    "        ], dim=-1)  # Final shape: (E, num_heads, 2*out_feats + 4 + time_dim)\n",
    "\n",
    "        a = self.attn_fc(z_cat)       # shape: (E, num_heads, 1)\n",
    "        a = torch.nn.functional.leaky_relu(a)\n",
    "        return {'e': a}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            # Project node features and reshape for multi-head attention\n",
    "            z = self.fc(h)  # shape: (N, out_feats * num_heads)\n",
    "            z = z.view(z.shape[0], self.num_heads, self.out_feats)  # shape: (N, num_heads, out_feats)\n",
    "            g.ndata['z'] = z\n",
    "            # Compute edge attention values incorporating temporal encodings\n",
    "            g.apply_edges(self.edge_attention)\n",
    "            # Message passing: multiply source node representations with attention coefficients\n",
    "            g.update_all(fn.u_mul_e('z', 'e', 'm'), fn.mean('m', 'h_new'))\n",
    "            # Reshape aggregated messages from multi-head outputs\n",
    "            h_new = g.ndata['h_new'].reshape(g.ndata['h_new'].shape[0], self.num_heads * self.out_feats)\n",
    "            return self.dropout(h_new)\n",
    "\n",
    "# Define the stacked TGAT model\n",
    "class TGAT(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_heads, num_layers, time_dim=4):\n",
    "        super(TGAT, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # First layer projects from input features to hidden space\n",
    "        self.layers.append(TGATLayer(in_feats, hidden_feats, num_heads, time_dim))\n",
    "        # Additional layers to capture multi-hop temporal interactions\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(TGATLayer(hidden_feats * num_heads, hidden_feats, num_heads, time_dim))\n",
    "        # Final linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_feats * num_heads, out_feats)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return self.fc_out(h)\n",
    "\n",
    "# Create a small temporal graph dataset with edge features\n",
    "def create_tg():\n",
    "    src_nodes = [0, 1, 2, 3, 4, 1, 2, 3, 4, 0]  # Bidirectional edges\n",
    "    dst_nodes = [1, 2, 3, 4, 0, 0, 1, 2, 3, 4]\n",
    "    timestamps = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)\n",
    "    trade_volumes = [100, 200, 150, 300, 250, 180, 220, 170, 280, 240]\n",
    "    dummy1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    dummy2 = [5, 4, 3, 2, 1, 0, 1, 2, 3, 4]\n",
    "    dummy3 = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    dummy4 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    g = dgl.graph((src_nodes, dst_nodes))\n",
    "    g.edata['timestamp'] = timestamps\n",
    "    g.edata['trade_vol'] = torch.tensor(trade_volumes, dtype=torch.float32)\n",
    "    g.edata['dummy1'] = torch.tensor(dummy1, dtype=torch.float32)\n",
    "    g.edata['dummy2'] = torch.tensor(dummy2, dtype=torch.float32)\n",
    "    g.edata['dummy3'] = torch.tensor(dummy3, dtype=torch.float32)\n",
    "    g.edata['dummy4'] = torch.tensor(dummy4, dtype=torch.float32)\n",
    "    g.ndata['feat'] = torch.randn(g.num_nodes(), 10)\n",
    "    return g\n",
    "\n",
    "# Train the stacked TGAT Model\n",
    "def train_tgat():\n",
    "    g = create_tg()\n",
    "    # Stack 2 TGAT layers (experiment with additional layers if desired)\n",
    "    model = TGAT(in_feats=10, hidden_feats=16, out_feats=1, num_heads=2, num_layers=2, time_dim=4)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    labels = torch.tensor([200, 300, 250, 400, 150], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        logits = model(g, g.ndata['feat'])\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_tgat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROM THE PAPER\n",
    "Link: https://github.com/dmlc/dgl/tree/0.9.x/examples/pytorch/tgn\n",
    "Can refer to data_preprocessing.py to see how we need to pre-process the data and fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import traceback\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "from tgn import TGN\n",
    "# from data_preprocess import TemporalWikipediaDataset, TemporalRedditDataset, TemporalDataset\n",
    "from dataloading_tgnn import (FastTemporalEdgeCollator, FastTemporalSampler,\n",
    "                         SimpleTemporalEdgeCollator, SimpleTemporalSampler,\n",
    "                         TemporalEdgeDataLoader, TemporalSampler, TemporalEdgeCollator)\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VALID_SPLIT = 0.85\n",
    "\n",
    "# set random Seed\n",
    "np.random.seed(2021)\n",
    "torch.manual_seed(2021)\n",
    "\n",
    "\n",
    "def train(model, dataloader, sampler, criterion, optimizer, args):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_cnt = 0\n",
    "    last_t = time.time()\n",
    "    \n",
    "    # Assuming it returns (batch_id, positive_pair_g, None, blocks) where None is for the negative graph\n",
    "    for _, positive_pair_g, _, blocks in dataloader: #check the dataloader structure\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Just use the positive graph for edge feature prediction\n",
    "        pred_edge_features = model.embed(positive_pair_g, blocks)\n",
    "        \n",
    "        # Get ground truth edge features from the graph\n",
    "        true_edge_features = positive_pair_g.edata['feat']  # Adjust based on your actual edge feature field name\n",
    "        \n",
    "        # Compute MSE loss between predicted and ground truth edge features\n",
    "        loss = criterion(pred_edge_features, true_edge_features)\n",
    "        \n",
    "        total_loss += float(loss) * args.batch_size\n",
    "        \n",
    "        retain_graph = True if batch_cnt == 0 and not args.fast_mode else False\n",
    "        loss.backward(retain_graph=retain_graph)\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.detach_memory()\n",
    "        \n",
    "        if not args.not_use_memory:\n",
    "            # Update memory based on the batch graph\n",
    "            model.update_memory(positive_pair_g)\n",
    "            \n",
    "        if args.fast_mode:\n",
    "            sampler.attach_last_update(model.memory.last_update_t)\n",
    "            \n",
    "        print(\"Batch: \", batch_cnt, \"Time: \", time.time()-last_t)\n",
    "        last_t = time.time()\n",
    "        batch_cnt += 1\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def test_val(model, dataloader, sampler, criterion, args):\n",
    "    model.eval()\n",
    "    batch_size = args.batch_size\n",
    "    total_loss = 0\n",
    "    mse_scores = []\n",
    "    batch_cnt = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, positive_pair_g, _, blocks in dataloader:\n",
    "\n",
    "            pred_edge_features = model.embed(positive_pair_g, blocks)\n",
    "            \n",
    "            # Get the ground truth edge features\n",
    "            true_edge_features = positive_pair_g.edata['feat']  # Adjust field name if needed\n",
    "            \n",
    "            # Calculate MSE loss\n",
    "            loss = criterion(pred_edge_features, true_edge_features)\n",
    "            total_loss += float(loss) * batch_size\n",
    "            \n",
    "            # Calculate per-batch MSE for reporting\n",
    "            batch_mse = ((pred_edge_features - true_edge_features) ** 2).mean().item()\n",
    "            mse_scores.append(batch_mse)\n",
    "            \n",
    "            # Update memory if needed\n",
    "            if not args.not_use_memory:\n",
    "                model.update_memory(positive_pair_g)\n",
    "                \n",
    "            if args.fast_mode:\n",
    "                sampler.attach_last_update(model.memory.last_update_t)\n",
    "                \n",
    "            batch_cnt += 1\n",
    "    \n",
    "    # Return the average MSE across all batches\n",
    "    avg_mse = float(torch.tensor(mse_scores).mean())\n",
    "    return avg_mse\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=50,\n",
    "#                         help='epochs for training on entire dataset')\n",
    "#     parser.add_argument(\"--batch_size\", type=int,\n",
    "#                         default=200, help=\"Size of each batch\")\n",
    "#     parser.add_argument(\"--embedding_dim\", type=int, default=100,\n",
    "#                         help=\"Embedding dim for link prediction\")\n",
    "#     parser.add_argument(\"--memory_dim\", type=int, default=100,\n",
    "#                         help=\"dimension of memory\")\n",
    "#     parser.add_argument(\"--temporal_dim\", type=int, default=100,\n",
    "#                         help=\"Temporal dimension for time encoding\")\n",
    "#     parser.add_argument(\"--memory_updater\", type=str, default='gru',\n",
    "#                         help=\"Recurrent unit for memory update\")\n",
    "#     parser.add_argument(\"--aggregator\", type=str, default='last',\n",
    "#                         help=\"Aggregation method for memory update\")\n",
    "#     parser.add_argument(\"--n_neighbors\", type=int, default=10,\n",
    "#                         help=\"number of neighbors while doing embedding\")\n",
    "#     parser.add_argument(\"--sampling_method\", type=str, default='topk',\n",
    "#                         help=\"In embedding how node aggregate from its neighor\")\n",
    "#     parser.add_argument(\"--num_heads\", type=int, default=8,\n",
    "#                         help=\"Number of heads for multihead attention mechanism\")\n",
    "#     parser.add_argument(\"--fast_mode\", action=\"store_true\", default=False,\n",
    "#                         help=\"Fast Mode uses batch temporal sampling, history within same batch cannot be obtained\")\n",
    "#     parser.add_argument(\"--simple_mode\", action=\"store_true\", default=False,\n",
    "#                         help=\"Simple Mode directly delete the temporal edges from the original static graph\")\n",
    "#     parser.add_argument(\"--num_negative_samples\", type=int, default=1,\n",
    "#                         help=\"number of negative samplers per positive samples\")\n",
    "#     parser.add_argument(\"--dataset\", type=str, default=\"wikipedia\",\n",
    "#                         help=\"dataset selection wikipedia/reddit\")\n",
    "#     parser.add_argument(\"--k_hop\", type=int, default=1,\n",
    "#                         help=\"sampling k-hop neighborhood\")\n",
    "#     parser.add_argument(\"--not_use_memory\", action=\"store_true\", default=False,\n",
    "#                         help=\"Enable memory for TGN Model disable memory for TGN Model\")\n",
    "class Inputs: #specify arguments here\n",
    "    def __init__(self):\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 200 #refers to number of edges processed together in a single batch\n",
    "        self.embedding_dim = 100 #number of out-feat from each layer\n",
    "        self.memory_dim = 100 #more memory, more complexity; try to play around using CV but don't increase it too much because it can cause overfitting and very heavy computational costs (i think can think of it like a lag?)\n",
    "        self.temporal_dim = 100 #dimension for time encoding; can lower since we are doing by quarterly/yearly\n",
    "        self.memory_updater = 'gru' #gru or lstm\n",
    "        self.aggregator = 'last'\n",
    "        self.n_neighbors = 10 #number of neighbours to consider while embedding\n",
    "        self.sampling_method = 'topk' # or 'uniform' can see in dataloading_tgnn.py\n",
    "        self.num_heads = 8 # this essentially acts as the feedforward network between time periods\n",
    "        self.fast_mode = False\n",
    "        self.simple_mode = False #dont care this first\n",
    "        self.num_negative_samples = 1 #this also\n",
    "        self.k_hop = 1 # basically we can control indirect effects through k-hop\n",
    "        self.not_use_memory = False\n",
    "args = Inputs()\n",
    "\n",
    "assert not (\n",
    "    args.fast_mode and args.simple_mode), \"you can only choose one sampling mode\"\n",
    "if args.k_hop != 1:\n",
    "    assert args.simple_mode, \"this k-hop parameter only support simple mode\"\n",
    "\n",
    "# Load Dataset\n",
    "data=None\n",
    "\n",
    "# Pre-process data, mask new node in test set from original graph\n",
    "num_nodes = data.num_nodes()\n",
    "num_edges = data.num_edges()\n",
    "\n",
    "trainval_div = int(VALID_SPLIT*num_edges)\n",
    "\n",
    "# Select new node from test set and remove them from entire graph\n",
    "test_split_ts = data.edata['timestamp'][trainval_div]\n",
    "test_nodes = torch.cat([data.edges()[0][trainval_div:], data.edges()[\n",
    "                        1][trainval_div:]]).unique().numpy()\n",
    "test_new_nodes = np.random.choice(\n",
    "    test_nodes, int(0.1*len(test_nodes)), replace=False)\n",
    "\n",
    "in_subg = dgl.in_subgraph(data, test_new_nodes)\n",
    "out_subg = dgl.out_subgraph(data, test_new_nodes)\n",
    "# Remove edge who happen before the test set to prevent from learning the connection info\n",
    "new_node_in_eid_delete = in_subg.edata[dgl.EID][in_subg.edata['timestamp'] < test_split_ts]\n",
    "new_node_out_eid_delete = out_subg.edata[dgl.EID][out_subg.edata['timestamp'] < test_split_ts]\n",
    "new_node_eid_delete = torch.cat(\n",
    "    [new_node_in_eid_delete, new_node_out_eid_delete]).unique()\n",
    "\n",
    "graph_new_node = copy.deepcopy(data)\n",
    "# relative order preseved\n",
    "graph_new_node.remove_edges(new_node_eid_delete)\n",
    "\n",
    "# Now for no new node graph, all edge id need to be removed\n",
    "in_eid_delete = in_subg.edata[dgl.EID]\n",
    "out_eid_delete = out_subg.edata[dgl.EID]\n",
    "eid_delete = torch.cat([in_eid_delete, out_eid_delete]).unique()\n",
    "\n",
    "graph_no_new_node = copy.deepcopy(data)\n",
    "graph_no_new_node.remove_edges(eid_delete)\n",
    "\n",
    "# graph_no_new_node and graph_new_node should have same set of nid\n",
    "\n",
    "# Sampler Initialization\n",
    "if args.simple_mode:\n",
    "    fan_out = [args.n_neighbors for _ in range(args.k_hop)]\n",
    "    sampler = SimpleTemporalSampler(graph_no_new_node, fan_out)\n",
    "    new_node_sampler = SimpleTemporalSampler(data, fan_out)\n",
    "    edge_collator = SimpleTemporalEdgeCollator\n",
    "elif args.fast_mode:\n",
    "    sampler = FastTemporalSampler(graph_no_new_node, k=args.n_neighbors)\n",
    "    new_node_sampler = FastTemporalSampler(data, k=args.n_neighbors)\n",
    "    edge_collator = FastTemporalEdgeCollator\n",
    "else:\n",
    "    sampler = TemporalSampler(k=args.n_neighbors)\n",
    "    edge_collator = TemporalEdgeCollator\n",
    "\n",
    "neg_sampler = None #negative sampler is not used\n",
    "\n",
    "# Set Train, validation, test and new node test id\n",
    "train_seed = torch.arange(int(TRAIN_SPLIT*graph_no_new_node.num_edges()))\n",
    "valid_seed = torch.arange(int(\n",
    "    TRAIN_SPLIT*graph_no_new_node.num_edges()), trainval_div-new_node_eid_delete.size(0))\n",
    "test_seed = torch.arange(\n",
    "    trainval_div-new_node_eid_delete.size(0), graph_no_new_node.num_edges())\n",
    "test_new_node_seed = torch.arange(\n",
    "    trainval_div-new_node_eid_delete.size(0), graph_new_node.num_edges())\n",
    "\n",
    "g_sampling = None if args.fast_mode else dgl.add_reverse_edges(\n",
    "    graph_no_new_node, copy_edata=True)\n",
    "new_node_g_sampling = None if args.fast_mode else dgl.add_reverse_edges(\n",
    "    graph_new_node, copy_edata=True)\n",
    "if not args.fast_mode:\n",
    "    new_node_g_sampling.ndata[dgl.NID] = new_node_g_sampling.nodes()\n",
    "    g_sampling.ndata[dgl.NID] = new_node_g_sampling.nodes()\n",
    "\n",
    "# we highly recommend that you always set the num_workers=0, otherwise the sampled subgraph may not be correct.\n",
    "train_dataloader = TemporalEdgeDataLoader(graph_no_new_node,\n",
    "                                            train_seed,\n",
    "                                            sampler,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            negative_sampler=neg_sampler,\n",
    "                                            shuffle=False,\n",
    "                                            drop_last=False,\n",
    "                                            num_workers=0,\n",
    "                                            collator=edge_collator,\n",
    "                                            g_sampling=g_sampling)\n",
    "\n",
    "valid_dataloader = TemporalEdgeDataLoader(graph_no_new_node,\n",
    "                                            valid_seed,\n",
    "                                            sampler,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            negative_sampler=neg_sampler,\n",
    "                                            shuffle=False,\n",
    "                                            drop_last=False,\n",
    "                                            num_workers=0,\n",
    "                                            collator=edge_collator,\n",
    "                                            g_sampling=g_sampling)\n",
    "\n",
    "test_dataloader = TemporalEdgeDataLoader(graph_no_new_node,\n",
    "                                            test_seed,\n",
    "                                            sampler,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            negative_sampler=neg_sampler,\n",
    "                                            shuffle=False,\n",
    "                                            drop_last=False,\n",
    "                                            num_workers=0,\n",
    "                                            collator=edge_collator,\n",
    "                                            g_sampling=g_sampling)\n",
    "\n",
    "test_new_node_dataloader = TemporalEdgeDataLoader(graph_new_node,\n",
    "                                                    test_new_node_seed,\n",
    "                                                    new_node_sampler if args.fast_mode else sampler,\n",
    "                                                    batch_size=args.batch_size,\n",
    "                                                    negative_sampler=neg_sampler,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    num_workers=0,\n",
    "                                                    collator=edge_collator,\n",
    "                                                    g_sampling=new_node_g_sampling)\n",
    "\n",
    "edge_dim = data.edata['feats'].shape[1]\n",
    "num_node = data.num_nodes()\n",
    "\n",
    "model = TGN(edge_feat_dim=edge_dim,\n",
    "            memory_dim=args.memory_dim,\n",
    "            temporal_dim=args.temporal_dim,\n",
    "            embedding_dim=args.embedding_dim,\n",
    "            num_heads=args.num_heads,\n",
    "            num_nodes=num_node,\n",
    "            n_neighbors=args.n_neighbors,\n",
    "            memory_updater_type=args.memory_updater,\n",
    "            layers=args.k_hop)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Implement Logging mechanism\n",
    "f = open(\"logging.txt\", 'w')\n",
    "if args.fast_mode:\n",
    "    sampler.reset()\n",
    "try:\n",
    "    for i in range(args.epochs):\n",
    "        train_loss = train(model, train_dataloader, sampler,\n",
    "                            criterion, optimizer, args)\n",
    "\n",
    "        val_mse = test_val(\n",
    "            model, valid_dataloader, sampler, criterion, args)\n",
    "        memory_checkpoint = model.store_memory()\n",
    "        if args.fast_mode:\n",
    "            new_node_sampler.sync(sampler)\n",
    "        test_mse = test_val(\n",
    "            model, test_dataloader, sampler, criterion, args)\n",
    "        model.restore_memory(memory_checkpoint)\n",
    "        if args.fast_mode:\n",
    "            sample_nn = new_node_sampler\n",
    "        else:\n",
    "            sample_nn = sampler\n",
    "        nn_test_mse = test_val(\n",
    "            model, test_new_node_dataloader, sample_nn, criterion, args)\n",
    "        log_content = []\n",
    "        log_content.append(f\"Epoch: {i}; Training Loss: {train_loss} | \"+ f\"Validation MSE: {val_mse}\")\n",
    "        log_content.append(\n",
    "            f\"Epoch: {i}; Test MSE: {test_mse}\")\n",
    "        log_content.append(f\"Epoch: {i}; New Node Test MSE: {nn_test_mse}\")\n",
    "\n",
    "        f.writelines(log_content)\n",
    "        model.reset_memory()\n",
    "        if i < args.epochs-1 and args.fast_mode:\n",
    "            sampler.reset()\n",
    "        print(log_content[0], log_content[1], log_content[2])\n",
    "except KeyboardInterrupt:\n",
    "    traceback.print_exc()\n",
    "    error_content = \"Training Interreputed!\"\n",
    "    f.writelines(error_content)\n",
    "    f.close()\n",
    "print(\"========Training is Done========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Sample Graph \n",
    "\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "import random\n",
    "\n",
    "# Define nodes (countries)\n",
    "nodes = [\"USA\", \"China\", \"Germany\", \"India\", \"Brazil\"]\n",
    "node_ids = {name: i for i, name in enumerate(nodes)}\n",
    "\n",
    "# Generate node features (e.g., GDP, population, trade openness)\n",
    "#node_features = torch.rand(len(nodes), 5)  # 5D feature vector per node\n",
    "\n",
    "# Define edges and timestamps\n",
    "edges_src = []\n",
    "edges_dst = []\n",
    "timestamps = []\n",
    "edge_features = []\n",
    "\n",
    "time_step = 0\n",
    "for year in range(2000, 2005):  \n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(len(nodes)):\n",
    "            if i != j:\n",
    "                edges_src.append(i)\n",
    "                edges_dst.append(j)\n",
    "                timestamps.append(time_step)\n",
    "                edge_features.append(torch.rand(3))  # 3D edge feature\n",
    "    time_step += 1\n",
    "\n",
    "# Convert to tensors\n",
    "edges_src = torch.tensor(edges_src)\n",
    "edges_dst = torch.tensor(edges_dst)\n",
    "timestamps = torch.tensor(timestamps, dtype=torch.float32)\n",
    "edge_features = torch.stack(edge_features)\n",
    "\n",
    "# Create DGL Graph\n",
    "g = dgl.graph((edges_src, edges_dst))\n",
    "g.edata['timestamp'] = timestamps\n",
    "g.edata['feat'] = edge_features\n",
    "#g.ndata['feat'] = node_features  # ✅ Add node features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tgn(g, epochs=50, batch_size=200, n_neighbors=10):\n",
    "    \"\"\" Train a Temporal GNN model on a given DGL graph g \"\"\"\n",
    "    from tgn import TGN\n",
    "    from dataloading_tgnn import TemporalEdgeDataLoader, TemporalSampler, TemporalEdgeCollator\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    # Model Parameters\n",
    "    num_nodes = g.num_nodes()\n",
    "    edge_dim = g.edata['feat'].shape[1]\n",
    "\n",
    "    model = TGN(\n",
    "        edge_feat_dim=edge_dim,\n",
    "        memory_dim=100,\n",
    "        temporal_dim=100,\n",
    "        embedding_dim=100,\n",
    "        num_heads=8,\n",
    "        num_nodes=num_nodes,\n",
    "        n_neighbors=n_neighbors, # need to review this value \n",
    "        memory_updater_type='gru', # or lstm iirc\n",
    "        layers=1\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    TRAIN_SPLIT = 0.7\n",
    "    VALID_SPLIT = 0.85\n",
    "    num_edges = g.num_edges()\n",
    "    trainval_div = int(VALID_SPLIT * num_edges)\n",
    "\n",
    "    sampler = TemporalSampler(k=n_neighbors)\n",
    "    edge_collator = TemporalEdgeCollator\n",
    "\n",
    "    train_seed = torch.arange(int(TRAIN_SPLIT * num_edges))\n",
    "    valid_seed = torch.arange(int(TRAIN_SPLIT * num_edges), trainval_div)\n",
    "    test_seed = torch.arange(trainval_div, num_edges)\n",
    "\n",
    "    train_dataloader = TemporalEdgeDataLoader(g, train_seed, sampler, batch_size=batch_size, collator=edge_collator)\n",
    "    valid_dataloader = TemporalEdgeDataLoader(g, valid_seed, sampler, batch_size=batch_size, collator=edge_collator)\n",
    "    test_dataloader = TemporalEdgeDataLoader(g, test_seed, sampler, batch_size=batch_size, collator=edge_collator)\n",
    "\n",
    "    # Training Loop\n",
    "    from train import train, test_val\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, sampler, criterion, optimizer, args)\n",
    "        val_mse = test_val(model, valid_dataloader, sampler, criterion, args)\n",
    "        test_mse = test_val(model, test_dataloader, sampler, criterion, args)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val MSE = {val_mse:.4f}, Test MSE = {test_mse:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"tgn_model.pth\")\n",
    "\n",
    "    return model  # Return trained model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
