{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Implementation in DGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Attention Score Computation does not involve node features:\n",
    "Mr GPT says to: <br>\n",
    "Option A: Incorporate into Attention Mechanism\n",
    "Concatenate or combine edge features with the node features when computing the attention scores. For example, you could modify the edge_attention method to include the trade volume or political score (after appropriate transformation).\n",
    "\n",
    "\n",
    "Option B: Use a Separate Aggregation for Trade Volumes\n",
    "Since your final output is the total outgoing trade volume per country, you can leave the TGAT message passing largely as is and perform an extra aggregation step on the edge features. For example:\n",
    "\n",
    "Aggregate the trade volumes of outgoing edges for each country. <br>\n",
    "g.update_all(fn.copy_e('trade_vol', 'm'), fn.sum('m', 'total_trade_vol'))\n",
    "total_trade_vol = g.ndata['total_trade_vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.6800\n",
      "Epoch 10 | Loss: 0.4489\n",
      "Epoch 20 | Loss: 0.1027\n",
      "Epoch 30 | Loss: 0.0203\n",
      "Epoch 40 | Loss: 0.0000\n",
      "Epoch 50 | Loss: 0.0004\n",
      "Epoch 60 | Loss: 0.0009\n",
      "Epoch 70 | Loss: 0.0012\n",
      "Epoch 80 | Loss: 0.0014\n",
      "Epoch 90 | Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n",
    "import numpy as np\n",
    "\n",
    "# Define the Temporal Attention Layer\n",
    "class TGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, dropout=0.1):\n",
    "        super(TGATLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats\n",
    "        # Change attention layer to output a scalar per head\n",
    "        self.attn_fc = nn.Linear(2 * out_feats, 1, bias=False)\n",
    "        self.fc = nn.Linear(in_feats, out_feats * num_heads, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # Concatenate source and destination features along the last dimension.\n",
    "        # Both edges.src['z'] and edges.dst['z'] have shape (E, num_heads, out_feats),\n",
    "        # so concatenating along dim=-1 yields shape (E, num_heads, 2*out_feats)\n",
    "        z_cat = torch.cat([edges.src['z'], edges.dst['z']], dim=-1)\n",
    "        a = self.attn_fc(z_cat)  # Now shape: (E, num_heads, 1)\n",
    "        a = torch.nn.functional.leaky_relu(a)\n",
    "        return {'e': a}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            # Linear transformation and reshape to (N, num_heads, out_feats)\n",
    "            z = self.fc(h)  # shape: (N, out_feats * num_heads)\n",
    "            z = z.view(z.shape[0], self.num_heads, self.out_feats)\n",
    "            g.ndata['z'] = z\n",
    "\n",
    "            # Compute attention scores on edges\n",
    "            g.apply_edges(self.edge_attention)\n",
    "\n",
    "            # Message passing: multiply each source feature with its attention coefficient; e\n",
    "            g.update_all(fn.u_mul_e('z', 'e', 'm'), fn.mean('m', 'h_new'))\n",
    "            # Flatten the aggregated features to (N, num_heads * out_feats)\n",
    "            h_new = g.ndata['h_new'].reshape(g.ndata['h_new'].shape[0], self.num_heads * self.out_feats)\n",
    "            return h_new\n",
    "\n",
    "# Define the TGAT Model\n",
    "class TGAT(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_heads, num_layers):\n",
    "        super(TGAT, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(TGATLayer(in_feats, hidden_feats, num_heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(TGATLayer(hidden_feats * num_heads, hidden_feats, num_heads))\n",
    "        # Change output to one neuron for regression (or more if predicting per sector)\n",
    "        self.fc_out = nn.Linear(hidden_feats * num_heads, out_feats)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return self.fc_out(h)\n",
    "\n",
    "# Create a small temporal graph dataset\n",
    "def create_tg():\n",
    "    src_nodes = [0, 1, 2, 3, 4]\n",
    "    dst_nodes = [1, 2, 3, 4, 0]\n",
    "    timestamps = [1, 2, 3, 4, 5]  # Simulated time edges\n",
    "\n",
    "    g = dgl.graph((src_nodes, dst_nodes))\n",
    "    g.edata['timestamp'] = torch.tensor(timestamps, dtype=torch.float32)\n",
    "    g.ndata['feat'] = torch.randn(len(src_nodes), 10)  # Random node features\n",
    "    return g\n",
    "\n",
    "# Train the TGAT Model\n",
    "def train_tgat():\n",
    "    g = create_tg()  # Make sure to update create_tg() to use your country and edge features\n",
    "    model = TGAT(in_feats=5, hidden_feats=16, out_feats=1, num_heads=2, num_layers=2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Assuming you have ground truth total trade volumes for each country\n",
    "    labels = torch.tensor([200, 300, 250, 400, 150], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        logits = model(g, g.ndata['feat'])\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Optionally, after model training, perform edge aggregation to get trade volumes directly:\n",
    "    # g.update_all(fn.copy_e('trade_vol', 'm'), fn.sum('m', 'total_trade_vol'))\n",
    "    # print(\"Aggregated Trade Volumes:\", g.ndata['total_trade_vol'])\n",
    "\n",
    "# Run the training process\n",
    "train_tgat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked TGAT Implementation \n",
    "Given we want to shut off certain nodes while doing the prediction, (for eg, the trade analyst wants to only account for the interactions in a certain region), our training might have to account for random shutting off during inference. This is one way we can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n",
    "\n",
    "# Define a Masked TGAT Layer where we apply masks on edges and nodes\n",
    "class MaskedTGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, dropout=0.1):\n",
    "        super(MaskedTGATLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats\n",
    "        # Attention function outputs a scalar per head\n",
    "        self.attn_fc = nn.Linear(2 * out_feats, 1, bias=False)\n",
    "        self.fc = nn.Linear(in_feats, out_feats * num_heads, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # Concatenate source and destination transformed features\n",
    "        z_cat = torch.cat([edges.src['z'], edges.dst['z']], dim=-1)\n",
    "        a = self.attn_fc(z_cat)\n",
    "        a = torch.nn.functional.leaky_relu(a)\n",
    "        # Apply edge mask if available (assumed shape: (E, num_heads, 1))\n",
    "        if 'mask' in edges.data:\n",
    "            a = a * edges.data['mask']\n",
    "        return {'e': a}\n",
    "\n",
    "    def forward(self, g, h, node_mask=None):\n",
    "        with g.local_scope():\n",
    "            # Linear transformation and reshape to (N, num_heads, out_feats)\n",
    "            z = self.fc(h)\n",
    "            z = z.view(z.shape[0], self.num_heads, self.out_feats)\n",
    "            g.ndata['z'] = z\n",
    "\n",
    "            # Ensure each edge has a mask; if not, default to ones\n",
    "            if 'mask' not in g.edata:\n",
    "                g.edata['mask'] = torch.ones(g.number_of_edges(), self.num_heads, 1).to(z.device)\n",
    "\n",
    "            # Compute edge attention values, incorporating the mask\n",
    "            g.apply_edges(self.edge_attention)\n",
    "            # Multiply node features with attention scores and aggregate messages\n",
    "            g.update_all(fn.u_mul_e('z', 'e', 'm'), fn.mean('m', 'h_new'))\n",
    "            # Flatten aggregated features\n",
    "            h_new = g.ndata['h_new'].reshape(g.ndata['h_new'].shape[0], self.num_heads * self.out_feats)\n",
    "            # Apply node mask if provided (mask should be broadcastable to h_new's shape)\n",
    "            if node_mask is not None:\n",
    "                h_new = h_new * node_mask\n",
    "            return h_new\n",
    "\n",
    "# Define a simple TGAT model using the masked layer\n",
    "class MaskedTGAT(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_heads, num_layers):\n",
    "        super(MaskedTGAT, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(MaskedTGATLayer(in_feats, hidden_feats, num_heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            # The next layer takes flattened node features as input\n",
    "            self.layers.append(MaskedTGATLayer(hidden_feats * num_heads, hidden_feats, num_heads))\n",
    "        self.fc_out = nn.Linear(hidden_feats * num_heads, out_feats)\n",
    "\n",
    "    def forward(self, g, h, node_mask=None):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, node_mask)\n",
    "        return self.fc_out(h)\n",
    "\n",
    "# Create a graph and set up masks for nodes and edges\n",
    "def create_masked_graph():\n",
    "    # Create a simple graph with 5 nodes and 5 directed edges\n",
    "    src_nodes = [0, 1, 2, 3, 4]\n",
    "    dst_nodes = [1, 2, 3, 4, 0]\n",
    "    g = dgl.graph((src_nodes, dst_nodes))\n",
    "    \n",
    "    # Example node features (e.g., representing country indicators)\n",
    "    g.ndata['feat'] = torch.randn(len(src_nodes), 10)\n",
    "    \n",
    "    # Define an edge mask: 1 indicates the edge is active, 0 means it's switched off.\n",
    "    # Here we switch off the second edge (index 1).\n",
    "    edge_mask = torch.tensor([[1], [0], [1], [1], [1]], dtype=torch.float32)\n",
    "    # If using multiple heads (e.g., num_heads=2), expand the mask shape to (E, num_heads, 1)\n",
    "    edge_mask = edge_mask.unsqueeze(1).repeat(1, 2, 1)\n",
    "    g.edata['mask'] = edge_mask\n",
    "    \n",
    "    # Define a node mask: here, node 2 is switched off (mask=0) and others remain active (mask=1).\n",
    "    # For a node feature output dimension of D (here, num_heads*out_feats), the mask can be broadcast.\n",
    "    node_mask = torch.tensor([[1], [1], [0], [1], [1]], dtype=torch.float32)\n",
    "    \n",
    "    return g, g.ndata['feat'], node_mask\n",
    "\n",
    "# Example training loop using the masked TGAT model\n",
    "def train_masked_tgat():\n",
    "    g, features, node_mask = create_masked_graph()\n",
    "    model = MaskedTGAT(in_feats=10, hidden_feats=16, out_feats=1, num_heads=2, num_layers=2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Fake regression labels for demonstration (e.g., total trade volume per country)\n",
    "    labels = torch.tensor([[0.5], [0.7], [0.3], [0.9], [0.4]], dtype=torch.float32)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        # Forward pass: pass the node features and the node mask into the model\n",
    "        logits = model(g, features, node_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "train_masked_tgat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Geometric Temporal Implementation (using TGN, but I think we should deprioritise this for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric_temporal.nn.tgn import TGN  # Ensure you have the correct import for TGN\n",
    "from torch_geometric_temporal.signal import DynamicGraphTemporalSignal\n",
    "import numpy as np\n",
    "\n",
    "# Define a TGN-based model that accepts edge features\n",
    "class TradeTGN(nn.Module):\n",
    "    def __init__(self, node_in_feats, edge_in_feats, memory_dim, hidden_dim, out_feats, num_heads):\n",
    "        super(TradeTGN, self).__init__()\n",
    "        # TGN module that accepts node features, edge features, and timestamps.\n",
    "        self.tgn = TGN(\n",
    "            node_in_channels=node_in_feats,\n",
    "            edge_in_channels=edge_in_feats,\n",
    "            memory_dim=memory_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, out_feats)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, timestamps):\n",
    "        # The TGN module computes node embeddings using the provided edge features.\n",
    "        h = self.tgn(x, edge_index, edge_attr, timestamps)\n",
    "        return self.fc(h)\n",
    "\n",
    "# Create a temporal graph dataset for your trade use case.\n",
    "def create_trade_temporal_graph():\n",
    "    num_nodes = 10          # For example, 10 countries\n",
    "    num_snapshots = 5       # e.g., quarterly snapshots\n",
    "    edge_indices = []\n",
    "    edge_times = []\n",
    "    node_features = []\n",
    "    edge_features = []      # We'll generate edge features here\n",
    "    labels = []\n",
    "\n",
    "    for t in range(num_snapshots):\n",
    "        # Simulate 20 bilateral trade relationships at time step t\n",
    "        src_nodes = torch.randint(0, num_nodes, (20,))\n",
    "        dst_nodes = torch.randint(0, num_nodes, (20,))\n",
    "        edge_time = torch.tensor([t] * 20, dtype=torch.float32)\n",
    "        edge_index = torch.stack([src_nodes, dst_nodes], dim=0)\n",
    "\n",
    "        # Node features: [need_diversion, impediment_diversion, GDP_agriculture, GDP_industry, GDP_services]\n",
    "        node_feat = torch.randn(num_nodes, 5)  # Replace with your actual data\n",
    "\n",
    "        # Edge features: [trade_agriculture, trade_industry, trade_services, political_score]\n",
    "        edge_feat = torch.randn(20, 4)  # Replace with your real bilateral trade & political data\n",
    "\n",
    "        # Compute label: total outgoing trade volume per country (sum of trade volumes across sectors)\n",
    "        trade_volumes = torch.sum(edge_feat[:, :3], dim=1)\n",
    "        country_trade_volume = torch.zeros(num_nodes)\n",
    "        for i in range(20):\n",
    "            country_trade_volume[src_nodes[i]] += trade_volumes[i]\n",
    "\n",
    "        edge_indices.append(edge_index)\n",
    "        edge_times.append(edge_time)\n",
    "        node_features.append(node_feat)\n",
    "        edge_features.append(edge_feat)\n",
    "        labels.append(country_trade_volume)\n",
    "\n",
    "    # IMPORTANT: If your dataset class does not support edge features, you might need to extend it.\n",
    "    # For simplicity, here we return the DynamicGraphTemporalSignal without edge features.\n",
    "    # In practice, you’d modify the snapshot objects to include the corresponding edge_attr.\n",
    "    return DynamicGraphTemporalSignal(edge_indices, edge_times, node_features, labels)\n",
    "\n",
    "# Train the TGN model\n",
    "def train_trade_tgn():\n",
    "    dataset = create_trade_temporal_graph()\n",
    "    model = TradeTGN(node_in_feats=5, edge_in_feats=4, memory_dim=16, hidden_dim=32, out_feats=1, num_heads=2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        loss_sum = 0\n",
    "        # Here, we assume that each snapshot now includes an edge_attr attribute.\n",
    "        # If not, you can manage edge features using a parallel list or extend your dataset.\n",
    "        for snapshot in dataset:\n",
    "            x, edge_index, edge_time = snapshot.x, snapshot.edge_index, snapshot.edge_time\n",
    "            # For demonstration, we simulate edge_attr here; replace with actual snapshot.edge_attr if available.\n",
    "            edge_attr = torch.randn(edge_index.size(1), 4)\n",
    "            y_true = snapshot.y.unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x, edge_index, edge_attr, edge_time)\n",
    "            loss = loss_fn(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss_sum / len(dataset):.4f}\")\n",
    "\n",
    "# Run training\n",
    "train_trade_tgn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROM THE PAPER\n",
    "Link: https://github.com/dmlc/dgl/tree/0.9.x/examples/pytorch/tgn\n",
    "\n",
    "TGN, i think for DGL, is if we need more advanced memory mechanisms, which we might not need for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.8.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.4\n",
      "attrs                     25.1.0\n",
      "babel                     2.17.0\n",
      "beautifulsoup4            4.13.3\n",
      "bleach                    6.2.0\n",
      "blinker                   1.9.0\n",
      "certifi                   2025.1.31\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.1\n",
      "click                     8.1.8\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "contourpy                 1.3.1\n",
      "cycler                    0.12.1\n",
      "dash                      2.18.2\n",
      "dash-core-components      2.0.0\n",
      "dash-html-components      2.0.0\n",
      "dash-table                5.0.0\n",
      "debugpy                   1.8.12\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "dgl                       2.2.1\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.18.0\n",
      "Flask                     3.0.3\n",
      "fonttools                 4.56.0\n",
      "fqdn                      1.5.1\n",
      "fsspec                    2025.3.0\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.1\n",
      "idna                      3.10\n",
      "importlib_metadata        8.6.1\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.32.0\n",
      "ipywidgets                8.1.5\n",
      "isoduration               20.11.0\n",
      "itsdangerous              2.2.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.5\n",
      "joblib                    1.4.2\n",
      "json5                     0.10.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.15.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.5\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.13\n",
      "kiwisolver                1.4.8\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib                3.10.0\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.2\n",
      "mpmath                    1.3.0\n",
      "narwhals                  1.27.1\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.4.2\n",
      "notebook                  7.3.2\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.2.3\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pillow                    11.1.0\n",
      "pip                       23.2.1\n",
      "platformdirs              4.3.6\n",
      "plotly                    6.0.0\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.50\n",
      "psutil                    7.0.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.1\n",
      "pyparsing                 3.2.1\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.2.1\n",
      "pytz                      2025.1\n",
      "pywin32                   308\n",
      "pywinpty                  2.0.15\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.1\n",
      "referencing               0.36.2\n",
      "requests                  2.32.3\n",
      "retrying                  1.3.4\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.23.1\n",
      "scikit-learn              1.6.1\n",
      "scipy                     1.15.2\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.8.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "stack-data                0.6.3\n",
      "sympy                     1.13.1\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.5.0\n",
      "tinycss2                  1.4.0\n",
      "torch                     2.6.0\n",
      "torchdata                 0.11.0\n",
      "tornado                   6.4.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20241206\n",
      "typing_extensions         4.12.2\n",
      "tzdata                    2025.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.3.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "Werkzeug                  3.0.6\n",
      "widgetsnbextension        4.0.13\n",
      "zipp                      3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from modules import MemoryModule, MemoryOperation, MsgLinkPredictor, TemporalTransformerConv, TimeEncode #find modules from the github link\n",
    "\n",
    "\n",
    "class TGN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 edge_feat_dim,\n",
    "                 memory_dim,\n",
    "                 temporal_dim,\n",
    "                 embedding_dim,\n",
    "                 num_heads,\n",
    "                 num_nodes,\n",
    "                 n_neighbors=10,\n",
    "                 memory_updater_type='gru',\n",
    "                 layers=1):\n",
    "        super(TGN, self).__init__()\n",
    "        self.memory_dim = memory_dim\n",
    "        self.edge_feat_dim = edge_feat_dim\n",
    "        self.temporal_dim = temporal_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.memory_updater_type = memory_updater_type\n",
    "        self.num_nodes = num_nodes\n",
    "        self.layers = layers\n",
    "\n",
    "        self.temporal_encoder = TimeEncode(self.temporal_dim)\n",
    "\n",
    "        self.memory = MemoryModule(self.num_nodes,\n",
    "                                   self.memory_dim)\n",
    "\n",
    "        self.memory_ops = MemoryOperation(self.memory_updater_type,\n",
    "                                          self.memory,\n",
    "                                          self.edge_feat_dim,\n",
    "                                          self.temporal_encoder)\n",
    "\n",
    "        self.embedding_attn = TemporalTransformerConv(self.edge_feat_dim,\n",
    "                                                      self.memory_dim,\n",
    "                                                      self.temporal_encoder,\n",
    "                                                      self.embedding_dim,\n",
    "                                                      self.num_heads,\n",
    "                                                      layers=self.layers,\n",
    "                                                      allow_zero_in_degree=True)\n",
    "\n",
    "        self.msg_linkpredictor = MsgLinkPredictor(embedding_dim)\n",
    "\n",
    "    def embed(self, postive_graph, negative_graph, blocks):\n",
    "        emb_graph = blocks[0]\n",
    "        emb_memory = self.memory.memory[emb_graph.ndata[dgl.NID], :]\n",
    "        emb_t = emb_graph.ndata['timestamp']\n",
    "        embedding = self.embedding_attn(emb_graph, emb_memory, emb_t)\n",
    "        emb2pred = dict(\n",
    "            zip(emb_graph.ndata[dgl.NID].tolist(), emb_graph.nodes().tolist()))\n",
    "        # Since postive graph and negative graph has same is mapping\n",
    "        feat_id = [emb2pred[int(n)] for n in postive_graph.ndata[dgl.NID]]\n",
    "        feat = embedding[feat_id]\n",
    "        pred_pos, pred_neg = self.msg_linkpredictor(\n",
    "            feat, postive_graph, negative_graph)\n",
    "        return pred_pos, pred_neg\n",
    "\n",
    "    def update_memory(self, subg):\n",
    "        new_g = self.memory_ops(subg)\n",
    "        self.memory.set_memory(new_g.ndata[dgl.NID], new_g.ndata['memory'])\n",
    "        self.memory.set_last_update_t(\n",
    "            new_g.ndata[dgl.NID], new_g.ndata['timestamp'])\n",
    "\n",
    "    # Some memory operation wrappers\n",
    "    def detach_memory(self):\n",
    "        self.memory.detach_memory()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory.reset_memory()\n",
    "\n",
    "    def store_memory(self):\n",
    "        memory_checkpoint = {}\n",
    "        memory_checkpoint['memory'] = copy.deepcopy(self.memory.memory)\n",
    "        memory_checkpoint['last_t'] = copy.deepcopy(self.memory.last_update_t)\n",
    "        return memory_checkpoint\n",
    "\n",
    "    def restore_memory(self, memory_checkpoint):\n",
    "        self.memory.memory = memory_checkpoint['memory']\n",
    "        self.memory.last_update_time = memory_checkpoint['last_t']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
