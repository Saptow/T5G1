{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple implementation from gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.6800\n",
      "Epoch 10 | Loss: 0.4489\n",
      "Epoch 20 | Loss: 0.1027\n",
      "Epoch 30 | Loss: 0.0203\n",
      "Epoch 40 | Loss: 0.0000\n",
      "Epoch 50 | Loss: 0.0004\n",
      "Epoch 60 | Loss: 0.0009\n",
      "Epoch 70 | Loss: 0.0012\n",
      "Epoch 80 | Loss: 0.0014\n",
      "Epoch 90 | Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n",
    "import numpy as np\n",
    "\n",
    "# Define the Temporal Attention Layer\n",
    "class TGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, dropout=0.1):\n",
    "        super(TGATLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats\n",
    "        # Change attention layer to output a scalar per head\n",
    "        self.attn_fc = nn.Linear(2 * out_feats, 1, bias=False)\n",
    "        self.fc = nn.Linear(in_feats, out_feats * num_heads, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # Concatenate source and destination features along the last dimension.\n",
    "        # Both edges.src['z'] and edges.dst['z'] have shape (E, num_heads, out_feats),\n",
    "        # so concatenating along dim=-1 yields shape (E, num_heads, 2*out_feats)\n",
    "        z_cat = torch.cat([edges.src['z'], edges.dst['z']], dim=-1)\n",
    "        a = self.attn_fc(z_cat)  # Now shape: (E, num_heads, 1)\n",
    "        a = torch.nn.functional.leaky_relu(a)\n",
    "        return {'e': a}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            # Linear transformation and reshape to (N, num_heads, out_feats)\n",
    "            z = self.fc(h)  # shape: (N, out_feats * num_heads)\n",
    "            z = z.view(z.shape[0], self.num_heads, self.out_feats)\n",
    "            g.ndata['z'] = z\n",
    "\n",
    "            # Compute attention scores on edges\n",
    "            g.apply_edges(self.edge_attention)\n",
    "\n",
    "            # Message passing: multiply each source feature with its attention coefficient\n",
    "            g.update_all(fn.u_mul_e('z', 'e', 'm'), fn.mean('m', 'h_new'))\n",
    "            # Flatten the aggregated features to (N, num_heads * out_feats)\n",
    "            h_new = g.ndata['h_new'].reshape(g.ndata['h_new'].shape[0], self.num_heads * self.out_feats)\n",
    "            return h_new\n",
    "\n",
    "# Define the TGAT Model\n",
    "class TGAT(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_heads, num_layers):\n",
    "        super(TGAT, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(TGATLayer(in_feats, hidden_feats, num_heads))\n",
    "        for _ in range(num_layers - 1):\n",
    "            # The input for the next layer is flattened from (num_heads, hidden_feats)\n",
    "            self.layers.append(TGATLayer(hidden_feats * num_heads, hidden_feats, num_heads))\n",
    "        self.fc_out = nn.Linear(hidden_feats * num_heads, out_feats)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return self.fc_out(h)\n",
    "\n",
    "# Create a small temporal graph dataset\n",
    "def create_tg():\n",
    "    src_nodes = [0, 1, 2, 3, 4]\n",
    "    dst_nodes = [1, 2, 3, 4, 0]\n",
    "    timestamps = [1, 2, 3, 4, 5]  # Simulated time edges\n",
    "\n",
    "    g = dgl.graph((src_nodes, dst_nodes))\n",
    "    g.edata['timestamp'] = torch.tensor(timestamps, dtype=torch.float32)\n",
    "    g.ndata['feat'] = torch.randn(len(src_nodes), 10)  # Random node features\n",
    "    return g\n",
    "\n",
    "# Train the TGAT Model\n",
    "def train_tgat():\n",
    "    g = create_tg()\n",
    "    model = TGAT(in_feats=10, hidden_feats=16, out_feats=2, num_heads=2, num_layers=2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Fake labels for demonstration\n",
    "    labels = torch.tensor([0, 1, 0, 1, 0], dtype=torch.long)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        logits = model(g, g.ndata['feat'])\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Run the training process\n",
    "train_tgat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROM THE PAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.8.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.4\n",
      "attrs                     25.1.0\n",
      "babel                     2.17.0\n",
      "beautifulsoup4            4.13.3\n",
      "bleach                    6.2.0\n",
      "blinker                   1.9.0\n",
      "certifi                   2025.1.31\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.1\n",
      "click                     8.1.8\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "contourpy                 1.3.1\n",
      "cycler                    0.12.1\n",
      "dash                      2.18.2\n",
      "dash-core-components      2.0.0\n",
      "dash-html-components      2.0.0\n",
      "dash-table                5.0.0\n",
      "debugpy                   1.8.12\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "dgl                       2.2.1\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.18.0\n",
      "Flask                     3.0.3\n",
      "fonttools                 4.56.0\n",
      "fqdn                      1.5.1\n",
      "fsspec                    2025.3.0\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.1\n",
      "idna                      3.10\n",
      "importlib_metadata        8.6.1\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.32.0\n",
      "ipywidgets                8.1.5\n",
      "isoduration               20.11.0\n",
      "itsdangerous              2.2.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.5\n",
      "joblib                    1.4.2\n",
      "json5                     0.10.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.15.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.5\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.13\n",
      "kiwisolver                1.4.8\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib                3.10.0\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.2\n",
      "mpmath                    1.3.0\n",
      "narwhals                  1.27.1\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.4.2\n",
      "notebook                  7.3.2\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.2.3\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pillow                    11.1.0\n",
      "pip                       23.2.1\n",
      "platformdirs              4.3.6\n",
      "plotly                    6.0.0\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.50\n",
      "psutil                    7.0.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.1\n",
      "pyparsing                 3.2.1\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.2.1\n",
      "pytz                      2025.1\n",
      "pywin32                   308\n",
      "pywinpty                  2.0.15\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.1\n",
      "referencing               0.36.2\n",
      "requests                  2.32.3\n",
      "retrying                  1.3.4\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.23.1\n",
      "scikit-learn              1.6.1\n",
      "scipy                     1.15.2\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.8.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "stack-data                0.6.3\n",
      "sympy                     1.13.1\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.5.0\n",
      "tinycss2                  1.4.0\n",
      "torch                     2.6.0\n",
      "torchdata                 0.11.0\n",
      "tornado                   6.4.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20241206\n",
      "typing_extensions         4.12.2\n",
      "tzdata                    2025.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.3.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "Werkzeug                  3.0.6\n",
      "widgetsnbextension        4.0.13\n",
      "zipp                      3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from modules import MemoryModule, MemoryOperation, MsgLinkPredictor, TemporalTransformerConv, TimeEncode\n",
    "\n",
    "\n",
    "class TGN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 edge_feat_dim,\n",
    "                 memory_dim,\n",
    "                 temporal_dim,\n",
    "                 embedding_dim,\n",
    "                 num_heads,\n",
    "                 num_nodes,\n",
    "                 n_neighbors=10,\n",
    "                 memory_updater_type='gru',\n",
    "                 layers=1):\n",
    "        super(TGN, self).__init__()\n",
    "        self.memory_dim = memory_dim\n",
    "        self.edge_feat_dim = edge_feat_dim\n",
    "        self.temporal_dim = temporal_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.memory_updater_type = memory_updater_type\n",
    "        self.num_nodes = num_nodes\n",
    "        self.layers = layers\n",
    "\n",
    "        self.temporal_encoder = TimeEncode(self.temporal_dim)\n",
    "\n",
    "        self.memory = MemoryModule(self.num_nodes,\n",
    "                                   self.memory_dim)\n",
    "\n",
    "        self.memory_ops = MemoryOperation(self.memory_updater_type,\n",
    "                                          self.memory,\n",
    "                                          self.edge_feat_dim,\n",
    "                                          self.temporal_encoder)\n",
    "\n",
    "        self.embedding_attn = TemporalTransformerConv(self.edge_feat_dim,\n",
    "                                                      self.memory_dim,\n",
    "                                                      self.temporal_encoder,\n",
    "                                                      self.embedding_dim,\n",
    "                                                      self.num_heads,\n",
    "                                                      layers=self.layers,\n",
    "                                                      allow_zero_in_degree=True)\n",
    "\n",
    "        self.msg_linkpredictor = MsgLinkPredictor(embedding_dim)\n",
    "\n",
    "    def embed(self, postive_graph, negative_graph, blocks):\n",
    "        emb_graph = blocks[0]\n",
    "        emb_memory = self.memory.memory[emb_graph.ndata[dgl.NID], :]\n",
    "        emb_t = emb_graph.ndata['timestamp']\n",
    "        embedding = self.embedding_attn(emb_graph, emb_memory, emb_t)\n",
    "        emb2pred = dict(\n",
    "            zip(emb_graph.ndata[dgl.NID].tolist(), emb_graph.nodes().tolist()))\n",
    "        # Since postive graph and negative graph has same is mapping\n",
    "        feat_id = [emb2pred[int(n)] for n in postive_graph.ndata[dgl.NID]]\n",
    "        feat = embedding[feat_id]\n",
    "        pred_pos, pred_neg = self.msg_linkpredictor(\n",
    "            feat, postive_graph, negative_graph)\n",
    "        return pred_pos, pred_neg\n",
    "\n",
    "    def update_memory(self, subg):\n",
    "        new_g = self.memory_ops(subg)\n",
    "        self.memory.set_memory(new_g.ndata[dgl.NID], new_g.ndata['memory'])\n",
    "        self.memory.set_last_update_t(\n",
    "            new_g.ndata[dgl.NID], new_g.ndata['timestamp'])\n",
    "\n",
    "    # Some memory operation wrappers\n",
    "    def detach_memory(self):\n",
    "        self.memory.detach_memory()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory.reset_memory()\n",
    "\n",
    "    def store_memory(self):\n",
    "        memory_checkpoint = {}\n",
    "        memory_checkpoint['memory'] = copy.deepcopy(self.memory.memory)\n",
    "        memory_checkpoint['last_t'] = copy.deepcopy(self.memory.last_update_t)\n",
    "        return memory_checkpoint\n",
    "\n",
    "    def restore_memory(self, memory_checkpoint):\n",
    "        self.memory.memory = memory_checkpoint['memory']\n",
    "        self.memory.last_update_time = memory_checkpoint['last_t']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
