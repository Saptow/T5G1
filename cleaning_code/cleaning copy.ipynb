{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRI (Services Trade Restrictiveness Index) Dataset\n",
    "The STRI is an innovative tool that offers an overview of regulatory barriers across 22 major sectors and 51 countries. Based on the qualitative information in the database, composite indices quantify the identified restrictions across five standard policy categories, with values between zero and one. The five policy categories are restrictions on foreign entry, restrictions to movement of people, other discriminatory measures, barriers to competition and regulatory transparency. Complete openness to trade and investment gives a score of zero, while being completely closed to foreign services providers yields a score of one. \n",
    "\n",
    "Information on columns:\n",
    "1. REF_AREA: 51 countries\n",
    "2. ECONOMIC_ACTIVITY: 19 activities\n",
    "3. TIME_PERIOD: 2014-2024\n",
    "4. OBS_VALUE: The STRI value, which quantifies trade restrictiveness (0-1 scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read STRI.csv\n",
    "df = pd.read_csv('./original/STRI.csv')\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[['REF_AREA', 'Economic activity', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "\n",
    "# Rename economic activity to 'ECONOMIC_ACTIVITY'\n",
    "df = df.rename(columns={'Economic activity': 'ECONOMIC_ACTIVITY'})\n",
    "\n",
    "# Rename REF_AREA to 'COUNTRY'\n",
    "df = df.rename(columns={'REF_AREA': 'COUNTRY'})\n",
    "\n",
    "# Save the new dataframe to 'STRI_cleaned.csv'\n",
    "df.to_csv('./cleaned/STRI_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTM (Non-Tariff Measures) Dataset\n",
    "A trade and market access information system combining data on trade, customs tariffs, and non-tariff measures. TRAINS contains HS-based tariff data for over 170 countries and for several years. The data covers all requirements that can potentially affect international trade for a specific product in a specific country and for a specific trading partner at one point in time. The TRAINS NTM database offers organized information categorized by product, measure type, countries imposing the measure, affected countries and several other variables. \n",
    "\n",
    "Information on columns:\n",
    "1. NTM_CODE: Refer to https://wits.worldbank.org/wits/wits/witshelp/content/data_retrieval/p/intro/C2.Non_Tariff_Measures.htm \n",
    "2. NTM_DESCRIPTION\n",
    "3. COUNTRY_IMPOSING\n",
    "4. IMPLEMENTATION_DATE\n",
    "5. COUNTRY_AFFECTED\n",
    "6. IS_UNILATERAL: One-sided measure or not\n",
    "7. REPEAL_DATE: When that measure will end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NTM.csv \n",
    "df = pd.read_csv(\"./original/NTM.csv\")\n",
    "\n",
    "# Select only the relevant columns\n",
    "relevant_columns = [\n",
    "  \"ntmCode\",\n",
    "  \"ntmDescription\",\n",
    "  \"countryImposingNTMs\",\n",
    "  \"implementationDate\",\n",
    "  \"affectedCountriesNames\",\n",
    "  \"isUnilateral\",\n",
    "  \"repealDate\",\n",
    "]\n",
    "\n",
    "# Create a new dataframe with only the relevant columns\n",
    "cleaned_df = df[relevant_columns].copy()\n",
    "\n",
    "# Rename the columns to be more descriptive\n",
    "cleaned_df = cleaned_df.rename(\n",
    "  columns={\n",
    "    \"ntmCode\": \"NTM_CODE\",\n",
    "    \"ntmDescription\": \"NTM_DESCRIPTION\",\n",
    "    \"countryImposingNTMs\": \"COUNTRY_IMPOSING\",\n",
    "    \"implementationDate\": \"IMPLEMENTATION_DATE\",\n",
    "    \"affectedCountriesNames\": \"COUNTRY_AFFECTED\",\n",
    "    \"isUnilateral\": \"IS_UNILATERAL\",\n",
    "    \"repealDate\": \"REPEAL_DATE\",\n",
    "  }\n",
    ")\n",
    "\n",
    "# Change repeal date of missing values to 9999-12-31T00:00:00\n",
    "cleaned_df[\"REPEAL_DATE\"] = cleaned_df[\"REPEAL_DATE\"].fillna(\"9999-12-31T00:00:00\")\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_df.to_csv(\"./cleaned/NTM_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITS (World Integrated Trade Solution) Dataset\n",
    "It captures trade volumes and values between countries, systematically categorized by sectors and product classifications such as the Harmonized System (HS). This database provides data across different time intervals, including quarterly and yearly trade records, ensuring sufficient temporal granularity for capturing trends and economic fluctuations.\n",
    "\n",
    "Information on columns:\n",
    "1. Year: 2020-2022\n",
    "2. COUNTRY: The trading partner country name\n",
    "3. EXPORT_USD: Value of exports in thousands of US dollars\n",
    "4. IMPORT_USD: Value of imports in thousands of US dollars\n",
    "5. EXPORT_SHARE: Percentage of global export\n",
    "6. IMPORT_SHARE: Percentage of import export\n",
    "7. EXPORT_PRODUCTS: Measure of trade diversity\n",
    "8. IMPORT_PRODUCTS: Measure of trade diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine WITS_2020.csv, WITS_2021.csv, and WITS_2022.csv into a single dataframe\n",
    "df_2014 = pd.read_csv(\"./original/WITS_2014.csv\", encoding='latin1')\n",
    "df_2015 = pd.read_csv(\"./original/WITS_2015.csv\", encoding='latin1')\n",
    "df_2016 = pd.read_csv(\"./original/WITS_2016.csv\", encoding='latin1')\n",
    "df_2017 = pd.read_csv(\"./original/WITS_2017.csv\", encoding='latin1')\n",
    "df_2018 = pd.read_csv(\"./original/WITS_2018.csv\", encoding='latin1')\n",
    "df_2019 = pd.read_csv(\"./original/WITS_2019.csv\", encoding='latin1')\n",
    "df_2020 = pd.read_csv(\"./original/WITS_2020.csv\", encoding='latin1')\n",
    "df_2021 = pd.read_csv(\"./original/WITS_2021.csv\", encoding=\"latin1\")\n",
    "df_2022 = pd.read_csv(\"./original/WITS_2022.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Add a column to each dataframe to indicate the year\n",
    "df_2014[\"Year\"] = 2014\n",
    "df_2015[\"Year\"] = 2015\n",
    "df_2016[\"Year\"] = 2016\n",
    "df_2017[\"Year\"] = 2017\n",
    "df_2018[\"Year\"] = 2018\n",
    "df_2019[\"Year\"] = 2019\n",
    "df_2020[\"Year\"] = 2020\n",
    "df_2021[\"Year\"] = 2021\n",
    "df_2022[\"Year\"] = 2022\n",
    "\n",
    "# Combine the dataframes into a single dataframe\n",
    "combined_df = pd.concat([df_2014, df_2015, df_2016, df_2017, df_2018, df_2019, df_2020, df_2021, df_2022], ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "# combined_df.to_csv(\"WITS_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for consistency\n",
    "combined_df = combined_df.rename(columns={\n",
    "    \"Year\": \"YEAR\",\n",
    "    \"Partner Name\": \"COUNTRY\",\n",
    "    \"Export (US$ Thousand)\": \"EXPORT_USD\",\n",
    "    \"Import (US$ Thousand)\": \"IMPORT_USD\",\n",
    "    \"Export Partner Share (%)\": \"EXPORT_SHARE\",\n",
    "    \"Import Partner Share (%)\": \"IMPORT_SHARE\",\n",
    "    \"No Of exported HS6 digit Products\": \"EXPORT_PRODUCTS\",\n",
    "    \"No Of imported HS6 digit Products\": \"IMPORT_PRODUCTS\",\n",
    "})\n",
    "\n",
    "# Change export and import values to * 1000\n",
    "combined_df[\"EXPORT_USD\"] = combined_df[\"EXPORT_USD\"] * 1000\n",
    "combined_df[\"IMPORT_USD\"] = combined_df[\"IMPORT_USD\"] * 1000\n",
    "\n",
    "relevant_columns = [\n",
    "    \"YEAR\",\n",
    "    \"COUNTRY\",\n",
    "    \"EXPORT_USD\",\n",
    "    \"IMPORT_USD\",\n",
    "    \"EXPORT_SHARE\",\n",
    "    \"IMPORT_SHARE\",\n",
    "    \"EXPORT_PRODUCTS\",\n",
    "    \"IMPORT_PRODUCTS\",\n",
    "] \n",
    "\n",
    "# Change missing values in EXPORT_USD, IMPORT_USD, EXPORT_SHARE, and IMPORT_SHARE to 0\n",
    "combined_df[\"EXPORT_USD\"] = combined_df[\"EXPORT_USD\"].fillna(0)\n",
    "combined_df[\"IMPORT_USD\"] = combined_df[\"IMPORT_USD\"].fillna(0)\n",
    "combined_df[\"EXPORT_SHARE\"] = combined_df[\"EXPORT_SHARE\"].fillna(0)\n",
    "combined_df[\"IMPORT_SHARE\"] = combined_df[\"IMPORT_SHARE\"].fillna(0)\n",
    "\n",
    "# Change missing values in EXPORT_PRODUCTS and IMPORT_PRODUCTS to 0\n",
    "combined_df[\"EXPORT_PRODUCTS\"] = combined_df[\"EXPORT_PRODUCTS\"].fillna(0)\n",
    "combined_df[\"IMPORT_PRODUCTS\"] = combined_df[\"IMPORT_PRODUCTS\"].fillna(0)\n",
    "\n",
    "# Create a new dataframe with only the relevant columns\n",
    "cleaned_df = combined_df[relevant_columns]\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_df.to_csv(\"./cleaned/WITS_combined_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR (Geopolitical Risk Index) Dataset\n",
    "It is an indicator used to measure the level of geopolitical risk worldwide at a specific point in time, along with 44 country-specific indexes. The GPR index is derived from an automated text search of digital archives from 10 major newspapers. It is calculated by measuring the proportion of news articles each month that discuss adverse geopolitical events. The index categorizes these events into the following eight groups: war threats, peace threats, military buildups, nuclear threats, terror threats, beginning of war, escalation of war, and terror acts.\n",
    "\n",
    "Information on columns:\n",
    "\n",
    "Current risk\n",
    "1. MONTH\n",
    "2. COUNTRY\n",
    "3. GPR_SCORE\n",
    "\n",
    "Historical risk\n",
    "1. MONTH\n",
    "2. COUNTRY\n",
    "3. GPR_SCORE\n",
    "\n",
    "Global risk\n",
    "1. MONTH - The time period for the measurement\n",
    "2. GPR - The main Geopolitical Risk Index value\n",
    "3. GPRT - \"Geopolitical Risk Threats\" component\n",
    "4. GPRA - \"Geopolitical Risk Acts\" component\n",
    "5. GPRH - Historical Geopolitical Risk Index\n",
    "6. GPRHT - Historical Geopolitical Risk Threats\n",
    "7. GPRHA - Historical Geopolitical Risk Acts\n",
    "8. SHARE_GPR - Share or proportion of current geopolitical risk\n",
    "9. SHARE_GPRH - Share or proportion of historical geopolitical risk\n",
    "10. SHAREH_CAT_1 through SHAREH_CAT_8 - Shares of the eight event categories mentioned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Use the pandas import that's already available in the notebook\n",
    "\n",
    "def read_excel_file(path, sheet_name=\"Sheet1\"):\n",
    "  try:\n",
    "    xls = pd.ExcelFile(path)\n",
    "    return xls.parse(sheet_name)\n",
    "  except Exception as e:\n",
    "    print(f\"Error reading file {path}: {e}\")\n",
    "    raise\n",
    "\n",
    "def melt_risk_data(df, prefix):\n",
    "  # Select columns starting with the given prefix\n",
    "  cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "  long_df = df.melt(\n",
    "    id_vars=[\"month\"],\n",
    "    value_vars=cols,\n",
    "    var_name=\"COUNTRY\",\n",
    "    value_name=\"GPR_SCORE\",\n",
    "  )\n",
    "  # Remove the prefix from the country names\n",
    "  long_df[\"COUNTRY\"] = long_df[\"COUNTRY\"].str.replace(prefix, \"\", regex=False)\n",
    "  long_df.dropna(subset=[\"GPR_SCORE\"], inplace=True)\n",
    "  return long_df[[\"month\", \"COUNTRY\", \"GPR_SCORE\"]]\n",
    "\n",
    "# Set paths\n",
    "raw_gpr_path = Path(\"./original/data_gpr_export.xlsx\")\n",
    "output_dir = Path(\"./cleaned\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_gpr_base = output_dir / \"GPR_export\"\n",
    "\n",
    "# Read and preprocess the raw DataFrame\n",
    "raw_gpr_df = read_excel_file(raw_gpr_path)\n",
    "\n",
    "# Rename the first column to \"month\" if necessary and convert it to datetime\n",
    "first_col = raw_gpr_df.columns[0]\n",
    "if first_col != \"month\":\n",
    "  raw_gpr_df.rename(columns={first_col: \"month\"}, inplace=True)\n",
    "raw_gpr_df[\"month\"] = pd.to_datetime(raw_gpr_df[\"month\"], errors=\"coerce\")\n",
    "\n",
    "# Create long format DataFrames for current and historical risk data\n",
    "current_risk_df = melt_risk_data(raw_gpr_df, \"GPRC_\")\n",
    "historical_risk_df = melt_risk_data(raw_gpr_df, \"GPRHC_\")\n",
    "\n",
    "# Define global risk metric columns and extract them if available\n",
    "global_risk_columns = [\n",
    "  \"month\",\n",
    "  \"GPR\",\n",
    "  \"GPRT\",\n",
    "  \"GPRA\",\n",
    "  \"GPRH\",\n",
    "  \"GPRHT\",\n",
    "  \"GPRHA\",\n",
    "  \"SHARE_GPR\",\n",
    "  \"SHARE_GPRH\",\n",
    "  \"SHAREH_CAT_1\",\n",
    "  \"SHAREH_CAT_2\",\n",
    "  \"SHAREH_CAT_3\",\n",
    "  \"SHAREH_CAT_4\",\n",
    "  \"SHAREH_CAT_5\",\n",
    "  \"SHAREH_CAT_6\",\n",
    "  \"SHAREH_CAT_7\",\n",
    "  \"SHAREH_CAT_8\",\n",
    "]\n",
    "global_risk_df = raw_gpr_df[global_risk_columns].dropna(how=\"any\")\n",
    "\n",
    "# Rename month to MONTH\n",
    "current_risk_df.rename(columns={\"month\": \"MONTH\"}, inplace=True)\n",
    "global_risk_df.rename(columns={\"month\": \"MONTH\"}, inplace=True)\n",
    "historical_risk_df.rename(columns={\"month\": \"MONTH\"}, inplace=True)\n",
    "\n",
    "# Save the cleaned datasets as separate CSV files\n",
    "global_risk_df.to_csv(f\"{output_gpr_base}_global_risk.csv\", index=False)\n",
    "current_risk_df.to_csv(f\"{output_gpr_base}_current_risk.csv\", index=False)\n",
    "historical_risk_df.to_csv(f\"{output_gpr_base}_historical_risk.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(current_risk_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(historical_risk_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(global_risk_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./cleaned/FBIC_99-23.csv\")\n",
    "\n",
    "columns_to_keep = [\n",
    "    # Country identifiers\n",
    "    \"iso3a\",\n",
    "    \"iso3b\",\n",
    "    \"year\",\n",
    "    # Trade volume metrics\n",
    "    \"exportsallgoodatob_alldata\",\n",
    "    \"importsallgoodafromb_alldata\",\n",
    "    \"totaltradeawithb\",\n",
    "    \"totaltradeabgdpb\",\n",
    "    # Geopolitical relationship indicators\n",
    "    \"fbic\",\n",
    "    \"bandwidth\",\n",
    "    \"dependence\",\n",
    "    # Diplomatic relations\n",
    "    \"norm_lor_avg\",\n",
    "    # Economic agreements and institutions\n",
    "    \"tradeagreementindex\",\n",
    "    # Security and alliance metrics\n",
    "    \"norm_allianceindex\",\n",
    "    \"securitybandwidth\",\n",
    "    \"securitydependence\",\n",
    "]\n",
    "\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "df.to_csv(\"./cleaned/FBIC_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_countries = [\n",
    "    \"SGP\",\n",
    "    \"CHN\",\n",
    "    \"MYS\",\n",
    "    \"USA\",\n",
    "    \"HKG\",\n",
    "    \"IDN\",\n",
    "    \"KOR\",\n",
    "    \"JPN\",\n",
    "    \"THA\",\n",
    "    \"AUS\",\n",
    "    \"VNM\",\n",
    "    \"IND\",\n",
    "    \"ARE\",\n",
    "    \"PHL\",\n",
    "    \"DEU\",\n",
    "    \"FRA\",\n",
    "    \"CHE\",\n",
    "    \"NLD\",\n",
    "]\n",
    "\n",
    "# Only keep rows iso3a and iso3b are in relevant_countries\n",
    "df = df[df[\"iso3a\"].isin(relevant_countries) & df[\"iso3b\"].isin(relevant_countries)]\n",
    "\n",
    "df.to_csv(\"./cleaned/FBIC_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_pair(a, b, available_pairs):\n",
    "    pair1 = f\"{a}|{b}\"\n",
    "    pair2 = f\"{b}|{a}\"\n",
    "    if pair1 in available_pairs:\n",
    "        return pair1\n",
    "    elif pair2 in available_pairs:\n",
    "        return pair2\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the maritime index and set its index to Country_Pair\n",
    "maritime_index = pd.read_csv(\"./cleaned/maritime_index.csv\")\n",
    "maritime_index = maritime_index.set_index(\"Country_Pair\")\n",
    "\n",
    "# Create a new column in your main dataframe that forms the country pair key\n",
    "available_pairs = set(maritime_index.index)\n",
    "df[\"Country_Pair\"] = df.apply(\n",
    "    lambda row: get_country_pair(row[\"iso3a\"], row[\"iso3b\"], available_pairs), axis=1\n",
    ")\n",
    "\n",
    "# Now, map the connectivity value from the maritime index using the new Country_Pair column\n",
    "df[\"maritime_connectivity\"] = df[\"Country_Pair\"].map(\n",
    "    maritime_index[\"Connectivity_Index\"]\n",
    ")\n",
    "\n",
    "# Drop the Country_Pair column\n",
    "df = df.drop(columns=[\"Country_Pair\"])\n",
    "\n",
    "# Save the result\n",
    "df.to_csv(\"./cleaned/FBIC_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Read the sentiment index\n",
    "sentiment_index = pd.read_csv(\"./original/sentiment.csv\")\n",
    "\n",
    "# Use MinMaxScaler to normalize the sentiment index\n",
    "scaler = MinMaxScaler()\n",
    "sentiment_index[\"sentiment_index\"] = scaler.fit_transform(\n",
    "    sentiment_index[[\"AvgTone_Avg\"]]\n",
    ")\n",
    "\n",
    "# Drop the AvgTone_Avg column\n",
    "sentiment_index = sentiment_index.drop(columns=[\"AvgTone_Avg\"])\n",
    "\n",
    "# Save the normalized sentiment index\n",
    "sentiment_index.to_csv(\"./cleaned/sentiment_index_normalized.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
