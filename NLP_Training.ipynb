{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7erPlkI-WOx",
        "outputId": "48722da2-4c44-4e35-be92-6bd35356afaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trafilatura\n",
        "!pip install trafilatura dateparser\n",
        "!pip install langdetect\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmzmPuuqAQ2I",
        "outputId": "78b5e651-e49c-4b25-ffd9-10a6f488f374"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2025.1.31)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.4.1)\n",
            "Requirement already satisfied: courlan>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.3.2)\n",
            "Requirement already satisfied: htmldate>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.9.3)\n",
            "Requirement already satisfied: justext>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.0.2)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (5.3.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2.3.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (0.13)\n",
            "Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura) (1.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n",
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2025.1.31)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.4.1)\n",
            "Requirement already satisfied: courlan>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.3.2)\n",
            "Requirement already satisfied: htmldate>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.9.3)\n",
            "Requirement already satisfied: justext>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.0.2)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (5.3.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (5.3.1)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (0.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.2)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=ed4f5355daad8583739fed10dcb4f67c3292c215d887e2fcc77ad310fbc1e3cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import trafilatura\n",
        "import json\n",
        "import dateparser\n",
        "from langdetect import detect  # langdetect for manual language filtering\n",
        "from trafilatura.settings import DEFAULT_CONFIG\n",
        "from copy import deepcopy\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# Function to load EasyList (ad-blocking) rules\n",
        "def load_easylist_rules():\n",
        "    \"\"\"\n",
        "    Loads EasyList rules from the provided URL and parses the rules into a list.\n",
        "    \"\"\"\n",
        "    url = \"https://easylist.to/easylist/easylist.txt\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.text.splitlines()  # Split the response into a list of lines\n",
        "    else:\n",
        "        print(\"Error: Could not retrieve EasyList.\")\n",
        "        return []\n",
        "\n",
        "# Function to remove ad-related content based on EasyList rules\n",
        "def remove_ad_content(content, easylist_rules):\n",
        "    \"\"\"\n",
        "    Removes ad-related content from the extracted content using EasyList rules.\n",
        "    \"\"\"\n",
        "    for rule in easylist_rules:\n",
        "        if rule.startswith('||'):  # Domain-based ad blocking rule\n",
        "            ad_domain = rule[2:]\n",
        "            content = re.sub(r'https?://(?:[a-zA-Z0-9-]+\\.)?' + re.escape(ad_domain), '', content)\n",
        "\n",
        "    return content\n",
        "\n",
        "def remove_unwanted_content(content, unwanted_keywords):\n",
        "    \"\"\"\n",
        "    Removes the sentence containing the first occurrence of any unwanted keywords,\n",
        "    removes everything after the ellipsis (\"...\"), and everything after the unwanted keyword.\n",
        "    Also removes everything after \"Related:\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove unwanted backslashes\n",
        "    content = content.replace(\"\\\\\", \"\")\n",
        "\n",
        "    # Create the regex pattern for unwanted keywords in the sentence\n",
        "    # This matches the sentence that contains the unwanted keyword and everything after it\n",
        "    pattern = r\"([^.]*\\b(?:{})\\b[^.]*\\.).*\".format(\"|\".join(re.escape(keyword) for keyword in unwanted_keywords))\n",
        "\n",
        "    # Substitute the sentence containing the unwanted keyword and everything after it with an empty string\n",
        "    content = re.sub(pattern, \"\", content, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove everything after \"Related:\" or \"related content:\" (case-insensitive)\n",
        "    content = re.sub(r\"([^.]*\\b(?:Related:|related content:)\\b[^.]*\\.).*|(\\b(?:Related:|related content:)\\b.*)\", \"\", content, flags=re.IGNORECASE)\n",
        "\n",
        "\n",
        "\n",
        "    return content.strip()\n",
        "\n",
        "\n",
        "def extract_articles(url, unwanted_keywords):\n",
        "    \"\"\"\n",
        "    Extracts article content and publication date from a given URL using Trafilatura.\n",
        "    Cleans content by removing newline characters, ensures no redirects, no external URLs, and language is English.\n",
        "    Removes content after any unwanted keywords.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Modify the config settings directly before use\n",
        "        my_config = deepcopy(DEFAULT_CONFIG)\n",
        "\n",
        "        # Disable external URLs and no redirects\n",
        "        my_config['DEFAULT']['EXTERNAL_URLS'] = 'off'  # Disable external URL extraction\n",
        "        my_config['DEFAULT']['MAX_REDIRECTS'] = '0'    # Disable URL redirection\n",
        "\n",
        "        # Set download timeout and sleep time\n",
        "        my_config['DEFAULT']['DOWNLOAD_TIMEOUT'] = '120'  # Set timeout to 120 seconds\n",
        "        my_config['DEFAULT']['SLEEP_TIME'] = '5'         # Set sleep time between requests to 5 seconds\n",
        "\n",
        "        # Fetch the content from the original URL using the modified config\n",
        "        downloaded_html = trafilatura.fetch_url(url, config=my_config)\n",
        "\n",
        "        if not downloaded_html:\n",
        "            raise ValueError(\"Failed to download content\")\n",
        "\n",
        "        # Extract content using Trafilatura with the custom config\n",
        "        extracted = trafilatura.extract(downloaded_html, output_format=\"json\", with_metadata=True, config=my_config)\n",
        "        if not extracted:\n",
        "            raise ValueError(\"Trafilatura extraction failed\")\n",
        "\n",
        "        data = json.loads(extracted)\n",
        "\n",
        "        # Clean and format the content\n",
        "        content = data.get(\"text\", \"\").replace(\"\\n\", \" \").strip()\n",
        "\n",
        "\n",
        "        # Remove content after the unwanted keywords\n",
        "        content = remove_unwanted_content(content, unwanted_keywords)\n",
        "\n",
        "        # Remove ad-related content based on EasyList rules\n",
        "        easylist_rules = load_easylist_rules()  # Load EasyList rules\n",
        "        content = remove_ad_content(content, easylist_rules)\n",
        "\n",
        "\n",
        "        if not content or len(content)<1000: #1000 characters roughly 200 words\n",
        "            raise ValueError(\"Content too short\")\n",
        "\n",
        "        # Manually detect language (ensure it's English)\n",
        "        try:\n",
        "            detected_language = detect(content)  # Returns 'en' for English\n",
        "            if detected_language != 'en':  # If not English, return empty content\n",
        "                print(f\"Warning: Non-English content detected. Skipping URL.\")\n",
        "                return {\"content\": \"\", \"publish_date\": None}\n",
        "        except Exception as e:\n",
        "            print(f\"Error detecting language: {e}\")\n",
        "            return {\"content\": \"\", \"publish_date\": None}\n",
        "\n",
        "\n",
        "        # Continue to extract publication date\n",
        "        publish_date_str = data.get(\"date\", None)\n",
        "        publish_date = dateparser.parse(publish_date_str) if publish_date_str else None\n",
        "\n",
        "        formatted_date = publish_date.strftime(\"%d-%m-%Y\") if publish_date else None\n",
        "\n",
        "        return {\"content\": content, \"publish_date\": formatted_date}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return {\"content\": \"\", \"publish_date\": None}\n",
        "\n",
        "unwanted_keywords = [\"disclaimer\", \"terms and conditions\", \"for more information\", \"privacy policy\", \"cookies\", \"contact us\", \"cookie policy\",\"FAQ\", \"unsubscribe\", \"user agreement\",\"site policy\", \"sign up\", \"stay updated\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z6_RR8HuAhkF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "! pip install --upgrade transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRz_R52qBn2g",
        "outputId": "39680508-67a9-434c-8ff1-741f36e76980"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.3\n",
            "    Uninstalling transformers-4.50.3:\n",
            "      Successfully uninstalled transformers-4.50.3\n",
            "Successfully installed transformers-4.51.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycountry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TViRXEECDPbJ",
        "outputId": "e9db0bd7-c917-44e2-ae92-dee70867153f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycountry\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/6.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry\n",
            "Successfully installed pycountry-24.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define and initialize regression model from base\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    PreTrainedModel,\n",
        "    AutoConfig\n",
        ")\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "class DebertaForRegression(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Outputs a scalar tone score in [-1, 1] by taking the expected value of\n",
        "    the 3‑class sentiment probabilities:  (-1, 0, +1) · softmax(logits).\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.classifier = AutoModelForSequenceClassification.from_pretrained(\n",
        "            config._name_or_path, config=config\n",
        "        )\n",
        "        self.register_buffer(\"value_map\", torch.tensor([-1.0, 0.0, 1.0]))\n",
        "\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, num_items_in_batch: int | None = None,  **kwargs ):\n",
        "        out = self.classifier(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            **kwargs\n",
        "        )\n",
        "        logits = out.logits                      # (B, 3)\n",
        "        probs  = F.softmax(logits, dim=-1)       # (B, 3)\n",
        "\n",
        "        # Expected value: (B, 3) · (3,)  → (B,)\n",
        "        tone   = (probs * self.value_map).sum(dim=-1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(tone, labels.float())\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss   = loss,          # scalar or None\n",
        "            logits = tone,          # (B,) continuous in [-1, 1]\n",
        "            hidden_states = out.hidden_states,\n",
        "            attentions    = out.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "zLgQ_-3kEE4m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pycountry\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load fine-tuned model\n",
        "model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/deberta-regression-checkpoints/checkpoint-1008\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "model = DebertaForRegression.from_pretrained(checkpoint_path, config=config)\n",
        "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Country normalization setup\n",
        "countries_to_keep = {\n",
        "    'CHN', 'MYS', 'USA', 'HKG', 'IDN', 'KOR', 'JPN', 'THA', 'AUS', 'VNM',\n",
        "    'IND', 'PHL', 'DEU', 'FRA', 'CHE', 'NLD', 'SGP'\n",
        "}\n",
        "\n",
        "# Build name-to-ISO3 mapping\n",
        "name_to_iso3 = {}\n",
        "for country in pycountry.countries:\n",
        "    iso3 = country.alpha_3\n",
        "    if iso3 not in countries_to_keep:\n",
        "        continue\n",
        "    for key in [\"name\", \"official_name\", \"common_name\"]:\n",
        "        val = getattr(country, key, None)\n",
        "        if val:\n",
        "            name_to_iso3[val.lower()] = iso3\n",
        "\n",
        "# Manual aliases for informal names\n",
        "manual_aliases = {\n",
        "    \"u.s.\": \"USA\",\n",
        "    \"u.s\": \"USA\",\n",
        "    \"america\": \"USA\",\n",
        "    \"s. korea\": \"KOR\",\n",
        "    \"south korea\": \"KOR\",\n",
        "    \"hong kong sar\": \"HKG\",\n",
        "    \"hong kong special administrative region\": \"HKG\",\n",
        "}\n",
        "\n",
        "name_to_iso3.update({k.lower(): v for k, v in manual_aliases.items()})\n",
        "\n",
        "# Trading bloc aliases\n",
        "trading_bloc_aliases = {\n",
        "    \"apec\": [\"CHN\", \"MYS\", \"USA\", \"HKG\", \"IDN\", \"KOR\", \"JPN\", \"THA\", \"AUS\", \"VNM\", \"PHL\", \"SGP\"],\n",
        "    \"eu\": [\"DEU\", \"FRA\", \"NLD\"],\n",
        "    \"brics\": [\"CHN\", \"IND\"],\n",
        "    \"nafta\": [\"USA\"],\n",
        "    \"usmca\": [\"USA\"],\n",
        "    \"asean\": [\"MYS\", \"IDN\", \"SGP\", \"THA\", \"VNM\", \"PHL\"],\n",
        "    \"saarc\": [\"IND\"],\n",
        "}\n",
        "\n",
        "# UN M49 region mapping\n",
        "un_region_mapping = {\n",
        "    \"eastern asia\": [\"CHN\", \"HKG\", \"JPN\", \"KOR\"],\n",
        "    \"south-eastern asia\": [\"MYS\", \"IDN\", \"THA\", \"VNM\", \"PHL\", \"SGP\"],\n",
        "    \"southern asia\": [\"IND\"],\n",
        "    \"western europe\": [\"DEU\", \"FRA\", \"CHE\", \"NLD\"],\n",
        "    \"oceania\": [\"AUS\"],\n",
        "    \"northern america\": [\"USA\"]\n",
        "}\n",
        "\n",
        "# Normalize country name to ISO3\n",
        "def normalize_country(name):\n",
        "    return name_to_iso3.get(name.strip().lower(), None)\n",
        "\n",
        "\n",
        "# Function to extract relevant countries from the article text\n",
        "def extract_countries_from_text(text):\n",
        "    found = set()\n",
        "    lowered = text.lower()\n",
        "\n",
        "    # Direct country match\n",
        "    for name in name_to_iso3:\n",
        "        if name in lowered:\n",
        "            iso = normalize_country(name)\n",
        "            if iso:\n",
        "                found.add(iso)\n",
        "\n",
        "    # Match trading blocs\n",
        "    for bloc, members in trading_bloc_aliases.items():\n",
        "        if bloc in lowered:\n",
        "            found.update(members)\n",
        "\n",
        "    # Match regions\n",
        "    for region, members in un_region_mapping.items():\n",
        "        if region in lowered:\n",
        "            found.update(members)\n",
        "\n",
        "    return sorted(found)\n",
        "\n",
        "# Generate all unique 2-country combinations\n",
        "def generate_country_pairs(countries):\n",
        "    return ['-'.join(sorted(pair)) for pair in combinations(countries, 2)]\n",
        "\n",
        "# Predict sentiment for all detected country pairs\n",
        "def predict_all_pairs(text, debug=False):\n",
        "    \"\"\"\n",
        "    Predict sentiment scores for each country pair detected in the given text.\n",
        "    This function processes external input (article) and detects country pairs.\n",
        "    \"\"\"\n",
        "    detected = extract_countries_from_text(text)\n",
        "    pairs = generate_country_pairs(detected)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"\\n Detected countries: {detected}\")\n",
        "        print(f\" Generated country pairs: {pairs}\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Loop through country pairs and run inference on each pair\n",
        "    for pair in pairs:\n",
        "        # Tokenize the text with overflow handling\n",
        "        inputs = tokenizer(\n",
        "            text,                # Article text\n",
        "            pair,                # Country pair\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,      # Adjust max length as needed\n",
        "            stride=128,          # Overlap for chunks\n",
        "            return_overflowing_tokens=True\n",
        "        )\n",
        "\n",
        "        # Remove overflow_to_sample_mapping from the inputs\n",
        "        if \"overflow_to_sample_mapping\" in inputs:\n",
        "            del inputs[\"overflow_to_sample_mapping\"]\n",
        "\n",
        "        # Initialize variable to accumulate sentiment scores for the chunks\n",
        "        total_score = 0\n",
        "        num_chunks = 0\n",
        "\n",
        "        # Iterate through each chunk in overflowed tokens\n",
        "        for i in range(len(inputs['input_ids'])):\n",
        "            # Get the chunk for this index\n",
        "            chunk = {key: value[i:i+1] for key, value in inputs.items()}\n",
        "\n",
        "            chunk = {k: v.to(model.device) for k, v in chunk.items()}\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                outputs = model(**chunk)\n",
        "                score = outputs[\"logits\"].item()\n",
        "                # Accumulate the score for the chunk\n",
        "                total_score += score\n",
        "                num_chunks += 1\n",
        "\n",
        "        # Compute the average score across all chunks for this country pair\n",
        "        avg_score = total_score / num_chunks if num_chunks > 0 else 0  # Avoid division by zero\n",
        "\n",
        "        results[pair] = round(avg_score, 4)  # Store the averaged score for the current pair\n",
        "\n",
        "    if debug:\n",
        "        print(f\" Predicted sentiment scores: {results}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Function to extract content from URL and then perform prediction\n",
        "def extract_and_predict_from_url(url):\n",
        "    # Extract article content from URL using trafilatura\n",
        "    content = extract_articles(url, unwanted_keywords)[\"content\"]\n",
        "\n",
        "    if not content:\n",
        "        print(\"No content extracted from URL.\")\n",
        "        return {}\n",
        "\n",
        "    # Get sentiment predictions for country pairs in the article\n",
        "    return predict_all_pairs(content)\n",
        "\n",
        "# Example usage\n",
        "url=\"https://www.channelnewsasia.com/business/trump-spares-smartphones-computers-other-electronics-his-125-china-tariffs-5062011\"\n",
        "extract_and_predict_from_url(url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFLo0cfADLPk",
        "outputId": "35290abb-4986-41ea-c8fe-b9374a6aa8ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/deberta-regression-checkpoints/checkpoint-1008 and are newly initialized: ['classifier.bias', 'classifier.weight', 'deberta.embeddings.LayerNorm.bias', 'deberta.embeddings.LayerNorm.weight', 'deberta.embeddings.word_embeddings.weight', 'deberta.encoder.LayerNorm.bias', 'deberta.encoder.LayerNorm.weight', 'deberta.encoder.layer.0.attention.output.LayerNorm.bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.weight', 'deberta.encoder.layer.0.attention.output.dense.bias', 'deberta.encoder.layer.0.attention.output.dense.weight', 'deberta.encoder.layer.0.attention.self.key_proj.bias', 'deberta.encoder.layer.0.attention.self.key_proj.weight', 'deberta.encoder.layer.0.attention.self.query_proj.bias', 'deberta.encoder.layer.0.attention.self.query_proj.weight', 'deberta.encoder.layer.0.attention.self.value_proj.bias', 'deberta.encoder.layer.0.attention.self.value_proj.weight', 'deberta.encoder.layer.0.intermediate.dense.bias', 'deberta.encoder.layer.0.intermediate.dense.weight', 'deberta.encoder.layer.0.output.LayerNorm.bias', 'deberta.encoder.layer.0.output.LayerNorm.weight', 'deberta.encoder.layer.0.output.dense.bias', 'deberta.encoder.layer.0.output.dense.weight', 'deberta.encoder.layer.1.attention.output.LayerNorm.bias', 'deberta.encoder.layer.1.attention.output.LayerNorm.weight', 'deberta.encoder.layer.1.attention.output.dense.bias', 'deberta.encoder.layer.1.attention.output.dense.weight', 'deberta.encoder.layer.1.attention.self.key_proj.bias', 'deberta.encoder.layer.1.attention.self.key_proj.weight', 'deberta.encoder.layer.1.attention.self.query_proj.bias', 'deberta.encoder.layer.1.attention.self.query_proj.weight', 'deberta.encoder.layer.1.attention.self.value_proj.bias', 'deberta.encoder.layer.1.attention.self.value_proj.weight', 'deberta.encoder.layer.1.intermediate.dense.bias', 'deberta.encoder.layer.1.intermediate.dense.weight', 'deberta.encoder.layer.1.output.LayerNorm.bias', 'deberta.encoder.layer.1.output.LayerNorm.weight', 'deberta.encoder.layer.1.output.dense.bias', 'deberta.encoder.layer.1.output.dense.weight', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.dense.bias', 'deberta.encoder.layer.10.attention.output.dense.weight', 'deberta.encoder.layer.10.attention.self.key_proj.bias', 'deberta.encoder.layer.10.attention.self.key_proj.weight', 'deberta.encoder.layer.10.attention.self.query_proj.bias', 'deberta.encoder.layer.10.attention.self.query_proj.weight', 'deberta.encoder.layer.10.attention.self.value_proj.bias', 'deberta.encoder.layer.10.attention.self.value_proj.weight', 'deberta.encoder.layer.10.intermediate.dense.bias', 'deberta.encoder.layer.10.intermediate.dense.weight', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'deberta.encoder.layer.10.output.dense.bias', 'deberta.encoder.layer.10.output.dense.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.dense.bias', 'deberta.encoder.layer.11.attention.output.dense.weight', 'deberta.encoder.layer.11.attention.self.key_proj.bias', 'deberta.encoder.layer.11.attention.self.key_proj.weight', 'deberta.encoder.layer.11.attention.self.query_proj.bias', 'deberta.encoder.layer.11.attention.self.query_proj.weight', 'deberta.encoder.layer.11.attention.self.value_proj.bias', 'deberta.encoder.layer.11.attention.self.value_proj.weight', 'deberta.encoder.layer.11.intermediate.dense.bias', 'deberta.encoder.layer.11.intermediate.dense.weight', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.dense.bias', 'deberta.encoder.layer.11.output.dense.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.bias', 'deberta.encoder.layer.2.attention.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.output.dense.bias', 'deberta.encoder.layer.2.attention.output.dense.weight', 'deberta.encoder.layer.2.attention.self.key_proj.bias', 'deberta.encoder.layer.2.attention.self.key_proj.weight', 'deberta.encoder.layer.2.attention.self.query_proj.bias', 'deberta.encoder.layer.2.attention.self.query_proj.weight', 'deberta.encoder.layer.2.attention.self.value_proj.bias', 'deberta.encoder.layer.2.attention.self.value_proj.weight', 'deberta.encoder.layer.2.intermediate.dense.bias', 'deberta.encoder.layer.2.intermediate.dense.weight', 'deberta.encoder.layer.2.output.LayerNorm.bias', 'deberta.encoder.layer.2.output.LayerNorm.weight', 'deberta.encoder.layer.2.output.dense.bias', 'deberta.encoder.layer.2.output.dense.weight', 'deberta.encoder.layer.3.attention.output.LayerNorm.bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.weight', 'deberta.encoder.layer.3.attention.output.dense.bias', 'deberta.encoder.layer.3.attention.output.dense.weight', 'deberta.encoder.layer.3.attention.self.key_proj.bias', 'deberta.encoder.layer.3.attention.self.key_proj.weight', 'deberta.encoder.layer.3.attention.self.query_proj.bias', 'deberta.encoder.layer.3.attention.self.query_proj.weight', 'deberta.encoder.layer.3.attention.self.value_proj.bias', 'deberta.encoder.layer.3.attention.self.value_proj.weight', 'deberta.encoder.layer.3.intermediate.dense.bias', 'deberta.encoder.layer.3.intermediate.dense.weight', 'deberta.encoder.layer.3.output.LayerNorm.bias', 'deberta.encoder.layer.3.output.LayerNorm.weight', 'deberta.encoder.layer.3.output.dense.bias', 'deberta.encoder.layer.3.output.dense.weight', 'deberta.encoder.layer.4.attention.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.output.LayerNorm.weight', 'deberta.encoder.layer.4.attention.output.dense.bias', 'deberta.encoder.layer.4.attention.output.dense.weight', 'deberta.encoder.layer.4.attention.self.key_proj.bias', 'deberta.encoder.layer.4.attention.self.key_proj.weight', 'deberta.encoder.layer.4.attention.self.query_proj.bias', 'deberta.encoder.layer.4.attention.self.query_proj.weight', 'deberta.encoder.layer.4.attention.self.value_proj.bias', 'deberta.encoder.layer.4.attention.self.value_proj.weight', 'deberta.encoder.layer.4.intermediate.dense.bias', 'deberta.encoder.layer.4.intermediate.dense.weight', 'deberta.encoder.layer.4.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.LayerNorm.weight', 'deberta.encoder.layer.4.output.dense.bias', 'deberta.encoder.layer.4.output.dense.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.bias', 'deberta.encoder.layer.5.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.dense.bias', 'deberta.encoder.layer.5.attention.output.dense.weight', 'deberta.encoder.layer.5.attention.self.key_proj.bias', 'deberta.encoder.layer.5.attention.self.key_proj.weight', 'deberta.encoder.layer.5.attention.self.query_proj.bias', 'deberta.encoder.layer.5.attention.self.query_proj.weight', 'deberta.encoder.layer.5.attention.self.value_proj.bias', 'deberta.encoder.layer.5.attention.self.value_proj.weight', 'deberta.encoder.layer.5.intermediate.dense.bias', 'deberta.encoder.layer.5.intermediate.dense.weight', 'deberta.encoder.layer.5.output.LayerNorm.bias', 'deberta.encoder.layer.5.output.LayerNorm.weight', 'deberta.encoder.layer.5.output.dense.bias', 'deberta.encoder.layer.5.output.dense.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.6.attention.output.dense.bias', 'deberta.encoder.layer.6.attention.output.dense.weight', 'deberta.encoder.layer.6.attention.self.key_proj.bias', 'deberta.encoder.layer.6.attention.self.key_proj.weight', 'deberta.encoder.layer.6.attention.self.query_proj.bias', 'deberta.encoder.layer.6.attention.self.query_proj.weight', 'deberta.encoder.layer.6.attention.self.value_proj.bias', 'deberta.encoder.layer.6.attention.self.value_proj.weight', 'deberta.encoder.layer.6.intermediate.dense.bias', 'deberta.encoder.layer.6.intermediate.dense.weight', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.LayerNorm.weight', 'deberta.encoder.layer.6.output.dense.bias', 'deberta.encoder.layer.6.output.dense.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.attention.output.dense.bias', 'deberta.encoder.layer.7.attention.output.dense.weight', 'deberta.encoder.layer.7.attention.self.key_proj.bias', 'deberta.encoder.layer.7.attention.self.key_proj.weight', 'deberta.encoder.layer.7.attention.self.query_proj.bias', 'deberta.encoder.layer.7.attention.self.query_proj.weight', 'deberta.encoder.layer.7.attention.self.value_proj.bias', 'deberta.encoder.layer.7.attention.self.value_proj.weight', 'deberta.encoder.layer.7.intermediate.dense.bias', 'deberta.encoder.layer.7.intermediate.dense.weight', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.dense.bias', 'deberta.encoder.layer.7.output.dense.weight', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.output.dense.bias', 'deberta.encoder.layer.8.attention.output.dense.weight', 'deberta.encoder.layer.8.attention.self.key_proj.bias', 'deberta.encoder.layer.8.attention.self.key_proj.weight', 'deberta.encoder.layer.8.attention.self.query_proj.bias', 'deberta.encoder.layer.8.attention.self.query_proj.weight', 'deberta.encoder.layer.8.attention.self.value_proj.bias', 'deberta.encoder.layer.8.attention.self.value_proj.weight', 'deberta.encoder.layer.8.intermediate.dense.bias', 'deberta.encoder.layer.8.intermediate.dense.weight', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.8.output.dense.bias', 'deberta.encoder.layer.8.output.dense.weight', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.attention.output.dense.bias', 'deberta.encoder.layer.9.attention.output.dense.weight', 'deberta.encoder.layer.9.attention.self.key_proj.bias', 'deberta.encoder.layer.9.attention.self.key_proj.weight', 'deberta.encoder.layer.9.attention.self.query_proj.bias', 'deberta.encoder.layer.9.attention.self.query_proj.weight', 'deberta.encoder.layer.9.attention.self.value_proj.bias', 'deberta.encoder.layer.9.attention.self.value_proj.weight', 'deberta.encoder.layer.9.intermediate.dense.bias', 'deberta.encoder.layer.9.intermediate.dense.weight', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.9.output.dense.bias', 'deberta.encoder.layer.9.output.dense.weight', 'deberta.encoder.rel_embeddings.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CHN-DEU': -0.0088,\n",
              " 'CHN-FRA': -0.0091,\n",
              " 'CHN-IND': -0.0091,\n",
              " 'CHN-NLD': -0.0088,\n",
              " 'CHN-SGP': -0.0088,\n",
              " 'CHN-USA': -0.0091,\n",
              " 'DEU-FRA': -0.0091,\n",
              " 'DEU-IND': -0.0091,\n",
              " 'DEU-NLD': -0.0088,\n",
              " 'DEU-SGP': -0.0088,\n",
              " 'DEU-USA': -0.009,\n",
              " 'FRA-IND': -0.0096,\n",
              " 'FRA-NLD': -0.0091,\n",
              " 'FRA-SGP': -0.0091,\n",
              " 'FRA-USA': -0.0096,\n",
              " 'IND-NLD': -0.0091,\n",
              " 'IND-SGP': -0.0091,\n",
              " 'IND-USA': -0.0096,\n",
              " 'NLD-SGP': -0.0091,\n",
              " 'NLD-USA': -0.0096,\n",
              " 'SGP-USA': -0.0096}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}